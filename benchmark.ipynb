{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "import torch.utils.benchmark as benchmark\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WARM_UP = 10\n",
    "BATCH_SIZE = 4\n",
    "NUM_TEST = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jul  3 11:50:04 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 455.45.01    Driver Version: 455.45.01    CUDA Version: 11.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  RTX A6000           On   | 00000000:09:00.0 Off |                  Off |\r\n",
      "| 30%   50C    P8    26W / 250W |     41MiB / 48682MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1256      G   /usr/lib/xorg/Xorg                 24MiB |\r\n",
      "|    0   N/A  N/A      1384      G   /usr/bin/gnome-shell               12MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>top1</th>\n",
       "      <th>top1_err</th>\n",
       "      <th>top5</th>\n",
       "      <th>top5_err</th>\n",
       "      <th>param_count</th>\n",
       "      <th>img_size</th>\n",
       "      <th>cropt_pct</th>\n",
       "      <th>interpolation</th>\n",
       "      <th>top1_diff</th>\n",
       "      <th>top5_diff</th>\n",
       "      <th>rank_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tf_efficientnet_l2_ns</td>\n",
       "      <td>90.563</td>\n",
       "      <td>9.437</td>\n",
       "      <td>98.779</td>\n",
       "      <td>1.221</td>\n",
       "      <td>480.31</td>\n",
       "      <td>800</td>\n",
       "      <td>0.960</td>\n",
       "      <td>bicubic</td>\n",
       "      <td>2.211</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tf_efficientnet_l2_ns_475</td>\n",
       "      <td>90.537</td>\n",
       "      <td>9.463</td>\n",
       "      <td>98.710</td>\n",
       "      <td>1.290</td>\n",
       "      <td>480.31</td>\n",
       "      <td>475</td>\n",
       "      <td>0.936</td>\n",
       "      <td>bicubic</td>\n",
       "      <td>2.303</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cait_m48_448</td>\n",
       "      <td>90.196</td>\n",
       "      <td>9.804</td>\n",
       "      <td>98.484</td>\n",
       "      <td>1.516</td>\n",
       "      <td>356.46</td>\n",
       "      <td>448</td>\n",
       "      <td>1.000</td>\n",
       "      <td>bicubic</td>\n",
       "      <td>3.712</td>\n",
       "      <td>0.730</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vit_large_patch16_384</td>\n",
       "      <td>90.196</td>\n",
       "      <td>9.804</td>\n",
       "      <td>98.661</td>\n",
       "      <td>1.339</td>\n",
       "      <td>304.72</td>\n",
       "      <td>384</td>\n",
       "      <td>1.000</td>\n",
       "      <td>bicubic</td>\n",
       "      <td>3.116</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tf_efficientnet_b7_ns</td>\n",
       "      <td>90.100</td>\n",
       "      <td>9.900</td>\n",
       "      <td>98.614</td>\n",
       "      <td>1.386</td>\n",
       "      <td>66.35</td>\n",
       "      <td>600</td>\n",
       "      <td>0.949</td>\n",
       "      <td>bicubic</td>\n",
       "      <td>3.260</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model    top1  top1_err    top5  top5_err  param_count  \\\n",
       "0      tf_efficientnet_l2_ns  90.563     9.437  98.779     1.221       480.31   \n",
       "1  tf_efficientnet_l2_ns_475  90.537     9.463  98.710     1.290       480.31   \n",
       "2               cait_m48_448  90.196     9.804  98.484     1.516       356.46   \n",
       "3      vit_large_patch16_384  90.196     9.804  98.661     1.339       304.72   \n",
       "4      tf_efficientnet_b7_ns  90.100     9.900  98.614     1.386        66.35   \n",
       "\n",
       "   img_size  cropt_pct interpolation  top1_diff  top5_diff  rank_diff  \n",
       "0       800      0.960       bicubic      2.211      0.129          0  \n",
       "1       475      0.936       bicubic      2.303      0.164          0  \n",
       "2       448      1.000       bicubic      3.712      0.730          3  \n",
       "3       384      1.000       bicubic      3.116      0.361          0  \n",
       "4       600      0.949       bicubic      3.260      0.520          0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_models = pd.read_csv(\"results-imagenet-real.csv\")\n",
    "# use models with img size 224\n",
    "modellist = df_models[df_models[\"img_size\"]==224][\"model\"]\n",
    "df_models.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "    def __init__(self,  length, imsize):\n",
    "        self.len = length\n",
    "        self.data = torch.randn( 3, imsize, imsize, length)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[:,:,:,index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "rand_loader = DataLoader(dataset=RandomDataset(BATCH_SIZE*(WARM_UP + NUM_TEST), 224),\n",
    "                         batch_size=BATCH_SIZE, shuffle=False,num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ryujaehun/pytorch-gpu-benchmark/blob/master/benchmark_models.py\n",
    "def inference(modelname, benchmark, half=False):\n",
    "    with torch.no_grad():\n",
    "        model = timm.create_model(modelname,)\n",
    "        model=model.to('cuda')\n",
    "        model.eval()\n",
    "        precision = \"float\"\n",
    "        durations = []\n",
    "        print(f'Benchmarking Inference {modelname} ')\n",
    "        for step,img in enumerate(rand_loader):\n",
    "            img=getattr(img,precision)()\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "            model(img.to('cuda'))\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            if step >= WARM_UP:\n",
    "                durations.append((end - start)*1000)\n",
    "        print(f'{modelname} model average inference time : {sum(durations)/len(durations)}ms')\n",
    "        \n",
    "        if half:\n",
    "            durations_half = []\n",
    "            print(f'Benchmarking Inference half precision type {modelname} ')\n",
    "            model.half()\n",
    "            precision = \"half\"\n",
    "            for step,img in enumerate(rand_loader):\n",
    "                img=getattr(img,precision)()\n",
    "                torch.cuda.synchronize()\n",
    "                start = time.time()\n",
    "                model(img.to('cuda'))\n",
    "                torch.cuda.synchronize()\n",
    "                end = time.time()\n",
    "                if step >= WARM_UP:\n",
    "                    durations_half.append((end - start)*1000)\n",
    "            print(f'{modelname} half model average inference time : {sum(durations_half)/len(durations_half)}ms')\n",
    "            \n",
    "        if half:\n",
    "            benchmark[modelname] = {\"fp32\": np.mean(durations), \"fp16\": np.mean(durations_half), \"top1\": df_models[df_models[\"model\"]==modelname][\"top1\"]}\n",
    "        else:\n",
    "            benchmark[modelname] = {\"fp32\": np.mean(durations), \"top1\": float(df_models[df_models[\"model\"]==modelname][\"top1\"])}\n",
    "    return benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "benchmark = {}\n",
    "\n",
    "# inference float precision\n",
    "for i,modelname in tqdm(enumerate((modellist))):\n",
    "    try:\n",
    "        benchmark = inference(modelname, benchmark)\n",
    "    except:\n",
    "        print(\"pass {}\".format(modelname))\n",
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(benchmark).T\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"results_fp32_224.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.lmplot(y='top1', x='fp32',  \n",
    "           data=df_results, logx=True,\n",
    "           fit_reg=False, scatter_kws={'alpha':0.8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For various image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ryujaehun/pytorch-gpu-benchmark/blob/master/benchmark_models.py\n",
    "def inference_imsize(modelname, benchmark, imsize):\n",
    "    with torch.no_grad():\n",
    "        model = timm.create_model(modelname,)\n",
    "        model=model.to('cuda')\n",
    "        model.eval()\n",
    "        precision = \"float\"\n",
    "        durations = []\n",
    "        rand_loader = DataLoader(dataset=RandomDataset(BATCH_SIZE*(WARM_UP + NUM_TEST), imsize),\n",
    "                         batch_size=BATCH_SIZE, shuffle=False,num_workers=8)\n",
    "        print(f'Benchmarking Inference {modelname} ')\n",
    "        for step,img in enumerate(rand_loader):\n",
    "            img=getattr(img,precision)()\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "            model(img.to('cuda'))\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            if step >= WARM_UP:\n",
    "                durations.append((end - start)*1000)\n",
    "        print(f'{modelname} model average inference time : {sum(durations)/len(durations)}ms')\n",
    "        \n",
    "        benchmark[modelname] = {\"fp32\": np.mean(durations), \"top1\": float(df_models[df_models[\"model\"]==modelname][\"top1\"]), \"imsize\": imsize}\n",
    "    return benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b74fc1f2054b719d8411136ca306e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Inference tf_efficientnet_l2_ns \n",
      "tf_efficientnet_l2_ns model average inference time : 681.3122296333313ms\n",
      "Benchmarking Inference tf_efficientnet_l2_ns_475 \n",
      "tf_efficientnet_l2_ns_475 model average inference time : 280.7985281944275ms\n",
      "Benchmarking Inference cait_m48_448 \n",
      "cait_m48_448 model average inference time : 556.2820100784302ms\n",
      "Benchmarking Inference vit_large_patch16_384 \n",
      "vit_large_patch16_384 model average inference time : 79.5830225944519ms\n",
      "Benchmarking Inference tf_efficientnet_b7_ns \n",
      "tf_efficientnet_b7_ns model average inference time : 103.03890943527222ms\n",
      "Benchmarking Inference cait_m36_384 \n",
      "cait_m36_384 model average inference time : 241.93207263946533ms\n",
      "Benchmarking Inference dm_nfnet_f6 \n",
      "dm_nfnet_f6 model average inference time : 509.15677785873413ms\n",
      "Benchmarking Inference swin_large_patch4_window12_384 \n",
      "swin_large_patch4_window12_384 model average inference time : 60.2254581451416ms\n",
      "Benchmarking Inference tf_efficientnetv2_l_in21ft1k \n",
      "tf_efficientnetv2_l_in21ft1k model average inference time : 93.13963651657104ms\n",
      "Benchmarking Inference swin_base_patch4_window12_384 \n",
      "swin_base_patch4_window12_384 model average inference time : 36.01898908615112ms\n",
      "Benchmarking Inference vit_base_patch16_384 \n",
      "vit_base_patch16_384 model average inference time : 27.246394157409668ms\n",
      "Benchmarking Inference cait_s36_384 \n",
      "cait_s36_384 model average inference time : 137.02792882919312ms\n",
      "Benchmarking Inference swin_large_patch4_window7_224 \n",
      "swin_large_patch4_window7_224 model average inference time : 20.064375400543213ms\n",
      "Benchmarking Inference vit_large_r50_s32_384 \n",
      "vit_large_r50_s32_384 model average inference time : 38.56330156326294ms\n",
      "Benchmarking Inference tf_efficientnet_b6_ns \n",
      "tf_efficientnet_b6_ns model average inference time : 60.31878471374512ms\n",
      "Benchmarking Inference tf_efficientnetv2_m_in21ft1k \n",
      "tf_efficientnetv2_m_in21ft1k model average inference time : 50.650856494903564ms\n",
      "Benchmarking Inference tf_efficientnet_b5_ns \n",
      "tf_efficientnet_b5_ns model average inference time : 38.72903823852539ms\n",
      "Benchmarking Inference tf_efficientnet_b8_ap \n",
      "tf_efficientnet_b8_ap model average inference time : 149.99229192733765ms\n",
      "Benchmarking Inference dm_nfnet_f4 \n",
      "dm_nfnet_f4 model average inference time : 324.5601511001587ms\n",
      "Benchmarking Inference cait_s24_384 \n",
      "cait_s24_384 model average inference time : 91.95441246032715ms\n",
      "Benchmarking Inference dm_nfnet_f3 \n",
      "dm_nfnet_f3 model average inference time : 208.75796794891357ms\n",
      "Benchmarking Inference dm_nfnet_f5 \n",
      "dm_nfnet_f5 model average inference time : 416.36311531066895ms\n",
      "Benchmarking Inference deit_base_distilled_patch16_384 \n",
      "deit_base_distilled_patch16_384 model average inference time : 27.517845630645752ms\n",
      "Benchmarking Inference tf_efficientnet_b7_ap \n",
      "tf_efficientnet_b7_ap model average inference time : 103.11571598052979ms\n",
      "Benchmarking Inference tf_efficientnetv2_l \n",
      "tf_efficientnetv2_l model average inference time : 92.77960300445557ms\n",
      "Benchmarking Inference tf_efficientnet_b8 \n",
      "tf_efficientnet_b8 model average inference time : 150.59494733810425ms\n",
      "Benchmarking Inference tf_efficientnet_b6_ap \n",
      "tf_efficientnet_b6_ap model average inference time : 60.478675365448ms\n",
      "Benchmarking Inference vit_large_patch16_224 \n",
      "vit_large_patch16_224 model average inference time : 24.927678108215332ms\n",
      "Benchmarking Inference tf_efficientnet_b4_ns \n",
      "tf_efficientnet_b4_ns model average inference time : 30.860531330108643ms\n",
      "Benchmarking Inference tf_efficientnetv2_m \n",
      "tf_efficientnetv2_m model average inference time : 50.62763690948486ms\n",
      "Benchmarking Inference swin_base_patch4_window7_224 \n",
      "swin_base_patch4_window7_224 model average inference time : 15.933451652526855ms\n",
      "Benchmarking Inference eca_nfnet_l2 \n",
      "eca_nfnet_l2 model average inference time : 73.14308166503906ms\n",
      "Benchmarking Inference cait_xs24_384 \n",
      "cait_xs24_384 model average inference time : 85.18592357635498ms\n",
      "Benchmarking Inference ig_resnext101_32x48d \n",
      "ig_resnext101_32x48d model average inference time : 362.3937916755676ms\n",
      "Benchmarking Inference ig_resnext101_32x32d \n",
      "ig_resnext101_32x32d model average inference time : 243.0540633201599ms\n",
      "Benchmarking Inference tf_efficientnet_b7 \n",
      "tf_efficientnet_b7 model average inference time : 102.82350778579712ms\n",
      "Benchmarking Inference ecaresnet269d \n",
      "ecaresnet269d model average inference time : 80.64383268356323ms\n",
      "Benchmarking Inference resmlp_big_24_224_in22ft1k \n",
      "resmlp_big_24_224_in22ft1k model average inference time : 36.44584894180298ms\n",
      "Benchmarking Inference dm_nfnet_f2 \n",
      "dm_nfnet_f2 model average inference time : 132.8338122367859ms\n",
      "Benchmarking Inference efficientnetv2_rw_m \n",
      "efficientnetv2_rw_m model average inference time : 50.98617076873779ms\n",
      "Benchmarking Inference tf_efficientnet_b5_ap \n",
      "tf_efficientnet_b5_ap model average inference time : 38.59375ms\n",
      "Benchmarking Inference dm_nfnet_f1 \n",
      "dm_nfnet_f1 model average inference time : 80.64253568649292ms\n",
      "Benchmarking Inference tf_efficientnetv2_s_in21ft1k \n",
      "tf_efficientnetv2_s_in21ft1k model average inference time : 33.555705547332764ms\n",
      "Benchmarking Inference vit_base_patch16_224 \n",
      "vit_base_patch16_224 model average inference time : 8.101544380187988ms\n",
      "Benchmarking Inference resnetrs420 \n",
      "resnetrs420 model average inference time : 165.99584341049194ms\n",
      "Benchmarking Inference ig_resnext101_32x16d \n",
      "ig_resnext101_32x16d model average inference time : 119.50673341751099ms\n",
      "Benchmarking Inference resnetrs270 \n",
      "resnetrs270 model average inference time : 102.45657920837402ms\n",
      "Benchmarking Inference vit_small_r26_s32_384 \n",
      "vit_small_r26_s32_384 model average inference time : 16.07172966003418ms\n",
      "Benchmarking Inference vit_base_r50_s16_384 \n",
      "vit_base_r50_s16_384 model average inference time : 44.60647106170654ms\n",
      "Benchmarking Inference seresnet152d \n",
      "seresnet152d model average inference time : 57.83982515335083ms\n",
      "Benchmarking Inference swsl_resnext101_32x8d \n",
      "swsl_resnext101_32x8d model average inference time : 63.34679841995239ms\n",
      "Benchmarking Inference tf_efficientnet_b6 \n",
      "tf_efficientnet_b6 model average inference time : 60.3957724571228ms\n",
      "Benchmarking Inference resnetrs350 \n",
      "resnetrs350 model average inference time : 130.88914155960083ms\n",
      "Benchmarking Inference vit_base_patch16_224_miil \n",
      "vit_base_patch16_224_miil model average inference time : 7.851285934448242ms\n",
      "Benchmarking Inference resnetv2_152x2_bitm \n",
      "resnetv2_152x2_bitm model average inference time : 164.62177753448486ms\n",
      "Benchmarking Inference regnety_160 \n",
      "regnety_160 model average inference time : 50.8770489692688ms\n",
      "Benchmarking Inference pit_b_distilled_224 \n",
      "pit_b_distilled_224 model average inference time : 23.28129529953003ms\n",
      "Benchmarking Inference vit_small_patch16_384 \n",
      "vit_small_patch16_384 model average inference time : 12.297897338867188ms\n",
      "Benchmarking Inference eca_nfnet_l1 \n",
      "eca_nfnet_l1 model average inference time : 42.21022367477417ms\n",
      "Benchmarking Inference resnetrs200 \n",
      "resnetrs200 model average inference time : 76.03182554244995ms\n",
      "Benchmarking Inference resnetv2_152x4_bitm \n",
      "resnetv2_152x4_bitm model average inference time : 610.906810760498ms\n",
      "Benchmarking Inference resnet200d \n",
      "resnet200d model average inference time : 47.637178897857666ms\n",
      "Benchmarking Inference resnest269e \n",
      "resnest269e model average inference time : 121.85580492019653ms\n",
      "Benchmarking Inference efficientnetv2_rw_s \n",
      "efficientnetv2_rw_s model average inference time : 33.643550872802734ms\n",
      "Benchmarking Inference resnetv2_101x3_bitm \n",
      "resnetv2_101x3_bitm model average inference time : 220.26455163955688ms\n",
      "Benchmarking Inference cait_s24_224 \n",
      "cait_s24_224 model average inference time : 17.53107786178589ms\n",
      "Benchmarking Inference resnetv2_50x3_bitm \n",
      "resnetv2_50x3_bitm model average inference time : 119.97088432312012ms\n",
      "Benchmarking Inference resmlp_big_24_distilled_224 \n",
      "resmlp_big_24_distilled_224 model average inference time : 36.488635540008545ms\n",
      "Benchmarking Inference resnest200e \n",
      "resnest200e model average inference time : 82.75031805038452ms\n",
      "Benchmarking Inference tf_efficientnet_b3_ns \n",
      "tf_efficientnet_b3_ns model average inference time : 24.923527240753174ms\n",
      "Benchmarking Inference vit_large_r50_s32_224 \n",
      "vit_large_r50_s32_224 model average inference time : 25.936448574066162ms\n",
      "Benchmarking Inference tf_efficientnetv2_s \n",
      "tf_efficientnetv2_s model average inference time : 33.82120132446289ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Inference efficientnet_b4 \n",
      "efficientnet_b4 model average inference time : 30.89365243911743ms\n",
      "Benchmarking Inference resnet152d \n",
      "resnet152d model average inference time : 37.48584747314453ms\n",
      "Benchmarking Inference tf_efficientnet_b4_ap \n",
      "tf_efficientnet_b4_ap model average inference time : 30.958642959594727ms\n",
      "Benchmarking Inference tf_efficientnet_b5 \n",
      "tf_efficientnet_b5 model average inference time : 38.48245143890381ms\n",
      "Benchmarking Inference resnetrs152 \n",
      "resnetrs152 model average inference time : 58.53393793106079ms\n",
      "Benchmarking Inference deit_base_distilled_patch16_224 \n",
      "deit_base_distilled_patch16_224 model average inference time : 8.232343196868896ms\n",
      "Benchmarking Inference resnetv2_152x2_bit_teacher_384 \n",
      "resnetv2_152x2_bit_teacher_384 model average inference time : 120.65580368041992ms\n",
      "Benchmarking Inference ig_resnext101_32x8d \n",
      "ig_resnext101_32x8d model average inference time : 63.340277671813965ms\n",
      "Benchmarking Inference cait_xxs36_384 \n",
      "cait_xxs36_384 model average inference time : 79.82808113098145ms\n",
      "Benchmarking Inference dm_nfnet_f0 \n",
      "dm_nfnet_f0 model average inference time : 36.23260736465454ms\n",
      "Benchmarking Inference swsl_resnext101_32x4d \n",
      "swsl_resnext101_32x4d model average inference time : 45.10894298553467ms\n",
      "Benchmarking Inference eca_nfnet_l0 \n",
      "eca_nfnet_l0 model average inference time : 21.97653293609619ms\n",
      "Benchmarking Inference nfnet_l0 \n",
      "nfnet_l0 model average inference time : 25.072789192199707ms\n",
      "Benchmarking Inference tf_efficientnet_b4 \n",
      "tf_efficientnet_b4 model average inference time : 30.840539932250977ms\n",
      "Benchmarking Inference resnet101d \n",
      "resnet101d model average inference time : 25.911705493927002ms\n",
      "Benchmarking Inference regnety_032 \n",
      "regnety_032 model average inference time : 34.19325828552246ms\n",
      "Benchmarking Inference vit_base_patch32_384 \n",
      "vit_base_patch32_384 model average inference time : 8.40101957321167ms\n",
      "Benchmarking Inference twins_svt_large \n",
      "twins_svt_large model average inference time : 17.415297031402588ms\n",
      "Benchmarking Inference twins_pcpvt_large \n",
      "twins_pcpvt_large model average inference time : 33.01650047302246ms\n",
      "Benchmarking Inference deit_base_patch16_384 \n",
      "deit_base_patch16_384 model average inference time : 27.08406686782837ms\n",
      "Benchmarking Inference tresnet_xl_448 \n",
      "pass tresnet_xl_448\n",
      "Benchmarking Inference resnetv2_50x1_bit_distilled \n",
      "resnetv2_50x1_bit_distilled model average inference time : 15.680091381072998ms\n",
      "Benchmarking Inference tresnet_m \n",
      "pass tresnet_m\n",
      "Benchmarking Inference twins_pcpvt_base \n",
      "twins_pcpvt_base model average inference time : 23.03375482559204ms\n",
      "Benchmarking Inference resnetv2_101x1_bitm \n",
      "resnetv2_101x1_bitm model average inference time : 40.998923778533936ms\n",
      "Benchmarking Inference swin_small_patch4_window7_224 \n",
      "swin_small_patch4_window7_224 model average inference time : 15.727837085723877ms\n",
      "Benchmarking Inference twins_svt_base \n",
      "twins_svt_base model average inference time : 17.704596519470215ms\n",
      "Benchmarking Inference pnasnet5large \n",
      "pnasnet5large model average inference time : 53.48133563995361ms\n",
      "Benchmarking Inference swsl_resnext101_32x16d \n",
      "swsl_resnext101_32x16d model average inference time : 118.77110242843628ms\n",
      "Benchmarking Inference swsl_resnext50_32x4d \n",
      "swsl_resnext50_32x4d model average inference time : 23.418021202087402ms\n",
      "Benchmarking Inference tf_efficientnet_b2_ns \n",
      "tf_efficientnet_b2_ns model average inference time : 22.01782464981079ms\n",
      "Benchmarking Inference levit_384 \n",
      "levit_384 model average inference time : 9.783318042755127ms\n",
      "Benchmarking Inference ecaresnet50t \n",
      "ecaresnet50t model average inference time : 16.986160278320312ms\n",
      "Benchmarking Inference resnetv2_152x2_bit_teacher \n",
      "resnetv2_152x2_bit_teacher model average inference time : 67.48608827590942ms\n",
      "Benchmarking Inference efficientnet_b3 \n",
      "efficientnet_b3 model average inference time : 24.941017627716064ms\n",
      "Benchmarking Inference cait_xxs24_384 \n",
      "cait_xxs24_384 model average inference time : 53.56301546096802ms\n",
      "Benchmarking Inference resnet51q \n",
      "resnet51q model average inference time : 15.64049243927002ms\n",
      "Benchmarking Inference coat_lite_small \n",
      "coat_lite_small model average inference time : 21.108202934265137ms\n",
      "Benchmarking Inference tresnet_l_448 \n",
      "pass tresnet_l_448\n",
      "Benchmarking Inference nasnetalarge \n",
      "nasnetalarge model average inference time : 68.66656303405762ms\n",
      "Benchmarking Inference ecaresnet101d \n",
      "ecaresnet101d model average inference time : 30.888094902038574ms\n",
      "Benchmarking Inference resnest101e \n",
      "resnest101e model average inference time : 42.74486064910889ms\n",
      "Benchmarking Inference pit_s_distilled_224 \n",
      "pit_s_distilled_224 model average inference time : 16.380975246429443ms\n",
      "Benchmarking Inference resnetrs101 \n",
      "resnetrs101 model average inference time : 39.43504810333252ms\n",
      "Benchmarking Inference mixer_b16_224_miil \n",
      "mixer_b16_224_miil model average inference time : 5.967974662780762ms\n",
      "Benchmarking Inference tresnet_xl \n",
      "pass tresnet_xl\n",
      "Benchmarking Inference convit_base \n",
      "convit_base model average inference time : 12.051944732666016ms\n",
      "Benchmarking Inference tf_efficientnet_b3_ap \n",
      "tf_efficientnet_b3_ap model average inference time : 25.10059118270874ms\n",
      "Benchmarking Inference visformer_small \n",
      "visformer_small model average inference time : 15.008840560913086ms\n",
      "Benchmarking Inference convit_small \n",
      "convit_small model average inference time : 8.650243282318115ms\n",
      "Benchmarking Inference tf_efficientnetv2_b3 \n",
      "tf_efficientnetv2_b3 model average inference time : 26.755173206329346ms\n",
      "Benchmarking Inference deit_small_distilled_patch16_224 \n",
      "deit_small_distilled_patch16_224 model average inference time : 6.368427276611328ms\n",
      "Benchmarking Inference resmlp_36_distilled_224 \n",
      "resmlp_36_distilled_224 model average inference time : 10.460522174835205ms\n",
      "Benchmarking Inference tnt_s_patch16_224 \n",
      "tnt_s_patch16_224 model average inference time : 14.255411624908447ms\n",
      "Benchmarking Inference vit_small_patch16_224 \n",
      "vit_small_patch16_224 model average inference time : 6.382865905761719ms\n",
      "Benchmarking Inference vit_small_r26_s32_224 \n",
      "vit_small_r26_s32_224 model average inference time : 14.236021041870117ms\n",
      "Benchmarking Inference ssl_resnext101_32x16d \n",
      "ssl_resnext101_32x16d model average inference time : 118.77163887023926ms\n",
      "Benchmarking Inference rexnet_200 \n",
      "rexnet_200 model average inference time : 15.597269535064697ms\n",
      "Benchmarking Inference tf_efficientnet_b3 \n",
      "tf_efficientnet_b3 model average inference time : 24.86647129058838ms\n",
      "Benchmarking Inference deit_base_patch16_224 \n",
      "deit_base_patch16_224 model average inference time : 8.074183464050293ms\n",
      "Benchmarking Inference tresnet_m_448 \n",
      "pass tresnet_m_448\n",
      "Benchmarking Inference ssl_resnext101_32x8d \n",
      "ssl_resnext101_32x8d model average inference time : 62.92789936065674ms\n",
      "Benchmarking Inference swsl_resnet50 \n",
      "swsl_resnet50 model average inference time : 13.253288269042969ms\n",
      "Benchmarking Inference tf_efficientnet_lite4 \n",
      "tf_efficientnet_lite4 model average inference time : 17.07280397415161ms\n",
      "Benchmarking Inference coat_mini \n",
      "coat_mini model average inference time : 36.04537487030029ms\n",
      "Benchmarking Inference tresnet_l \n",
      "pass tresnet_l\n",
      "Benchmarking Inference twins_svt_small \n",
      "twins_svt_small model average inference time : 13.34057092666626ms\n",
      "Benchmarking Inference levit_256 \n",
      "levit_256 model average inference time : 9.783623218536377ms\n",
      "Benchmarking Inference seresnext50_32x4d \n",
      "seresnext50_32x4d model average inference time : 30.111424922943115ms\n",
      "Benchmarking Inference pit_b_224 \n",
      "pit_b_224 model average inference time : 22.816636562347412ms\n",
      "Benchmarking Inference tf_efficientnet_b1_ns \n",
      "tf_efficientnet_b1_ns model average inference time : 22.025365829467773ms\n",
      "Benchmarking Inference swin_tiny_patch4_window7_224 \n",
      "swin_tiny_patch4_window7_224 model average inference time : 8.557002544403076ms\n",
      "Benchmarking Inference gernet_l \n",
      "gernet_l model average inference time : 12.576262950897217ms\n",
      "Benchmarking Inference wide_resnet50_2 \n",
      "wide_resnet50_2 model average inference time : 16.69198513031006ms\n",
      "Benchmarking Inference efficientnet_el \n",
      "efficientnet_el model average inference time : 14.113028049468994ms\n",
      "Benchmarking Inference resmlp_24_distilled_224 \n",
      "resmlp_24_distilled_224 model average inference time : 7.340612411499023ms\n",
      "Benchmarking Inference twins_pcpvt_small \n",
      "twins_pcpvt_small model average inference time : 13.705272674560547ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Inference nf_resnet50 \n",
      "nf_resnet50 model average inference time : 16.528451442718506ms\n",
      "Benchmarking Inference resnest50d_4s2x40d \n",
      "resnest50d_4s2x40d model average inference time : 24.056780338287354ms\n",
      "Benchmarking Inference efficientnet_b3_pruned \n",
      "efficientnet_b3_pruned model average inference time : 24.966726303100586ms\n",
      "Benchmarking Inference repvgg_b3 \n",
      "repvgg_b3 model average inference time : 28.252103328704834ms\n",
      "Benchmarking Inference ssl_resnext101_32x4d \n",
      "ssl_resnext101_32x4d model average inference time : 45.07107496261597ms\n",
      "Benchmarking Inference ecaresnet50d \n",
      "ecaresnet50d model average inference time : 16.41528844833374ms\n",
      "Benchmarking Inference gluon_resnet152_v1s \n",
      "gluon_resnet152_v1s model average inference time : 36.24826669692993ms\n",
      "Benchmarking Inference resnest50d_1s4x24d \n",
      "resnest50d_1s4x24d model average inference time : 22.795162200927734ms\n",
      "Benchmarking Inference resnetv2_50x1_bitm \n",
      "resnetv2_50x1_bitm model average inference time : 23.91517400741577ms\n",
      "Benchmarking Inference repvgg_b3g4 \n",
      "repvgg_b3g4 model average inference time : 27.49490261077881ms\n",
      "Benchmarking Inference legacy_senet154 \n",
      "legacy_senet154 model average inference time : 118.89943361282349ms\n",
      "Benchmarking Inference cait_xxs36_224 \n",
      "cait_xxs36_224 model average inference time : 25.418710708618164ms\n",
      "Benchmarking Inference gernet_m \n",
      "gernet_m model average inference time : 10.411837100982666ms\n",
      "Benchmarking Inference pit_s_224 \n",
      "pit_s_224 model average inference time : 16.34552240371704ms\n",
      "Benchmarking Inference vit_small_patch32_384 \n",
      "vit_small_patch32_384 model average inference time : 7.977423667907715ms\n",
      "Benchmarking Inference efficientnet_b2 \n",
      "efficientnet_b2 model average inference time : 21.932315826416016ms\n",
      "Benchmarking Inference gluon_senet154 \n",
      "gluon_senet154 model average inference time : 119.28701639175415ms\n",
      "Benchmarking Inference resnest50d \n",
      "resnest50d model average inference time : 22.430155277252197ms\n",
      "Benchmarking Inference ecaresnet101d_pruned \n",
      "ecaresnet101d_pruned model average inference time : 30.854415893554688ms\n",
      "Benchmarking Inference efficientnet_el_pruned \n",
      "efficientnet_el_pruned model average inference time : 14.494609832763672ms\n",
      "Benchmarking Inference cspdarknet53 \n",
      "cspdarknet53 model average inference time : 17.536776065826416ms\n",
      "Benchmarking Inference inception_v4 \n",
      "inception_v4 model average inference time : 35.37125825881958ms\n",
      "Benchmarking Inference rexnet_150 \n",
      "rexnet_150 model average inference time : 15.330533981323242ms\n",
      "Benchmarking Inference inception_resnet_v2 \n",
      "inception_resnet_v2 model average inference time : 57.305214405059814ms\n",
      "Benchmarking Inference ssl_resnext50_32x4d \n",
      "ssl_resnext50_32x4d model average inference time : 23.490662574768066ms\n",
      "Benchmarking Inference tf_efficientnet_el \n",
      "tf_efficientnet_el model average inference time : 14.638526439666748ms\n",
      "Benchmarking Inference gluon_resnet101_v1s \n",
      "gluon_resnet101_v1s model average inference time : 24.847488403320312ms\n",
      "Benchmarking Inference ecaresnetlight \n",
      "ecaresnetlight model average inference time : 16.167688369750977ms\n",
      "Benchmarking Inference gluon_seresnext101_32x4d \n",
      "gluon_seresnext101_32x4d model average inference time : 58.75911235809326ms\n",
      "Benchmarking Inference resnet50d \n",
      "resnet50d model average inference time : 13.711214065551758ms\n",
      "Benchmarking Inference ecaresnet26t \n",
      "ecaresnet26t model average inference time : 9.866526126861572ms\n",
      "Benchmarking Inference tf_efficientnet_b2_ap \n",
      "tf_efficientnet_b2_ap model average inference time : 22.07948923110962ms\n",
      "Benchmarking Inference gluon_seresnext101_64x4d \n",
      "gluon_seresnext101_64x4d model average inference time : 80.4191780090332ms\n",
      "Benchmarking Inference vit_base_patch32_224 \n",
      "vit_base_patch32_224 model average inference time : 6.389458179473877ms\n",
      "Benchmarking Inference gluon_resnet152_v1d \n",
      "gluon_resnet152_v1d model average inference time : 36.32312059402466ms\n",
      "Benchmarking Inference vit_large_patch32_384 \n",
      "vit_large_patch32_384 model average inference time : 20.15129327774048ms\n",
      "Benchmarking Inference tf_efficientnet_b2 \n",
      "tf_efficientnet_b2 model average inference time : 21.905653476715088ms\n",
      "Benchmarking Inference tf_efficientnetv2_b2 \n",
      "tf_efficientnetv2_b2 model average inference time : 22.837042808532715ms\n",
      "Benchmarking Inference seresnet50 \n",
      "seresnet50 model average inference time : 19.616737365722656ms\n",
      "Benchmarking Inference repvgg_b2g4 \n",
      "repvgg_b2g4 model average inference time : 23.08713436126709ms\n",
      "Benchmarking Inference gluon_resnet101_v1d \n",
      "gluon_resnet101_v1d model average inference time : 25.16246795654297ms\n",
      "Benchmarking Inference resnet50 \n",
      "resnet50 model average inference time : 13.282124996185303ms\n",
      "Benchmarking Inference mixnet_xl \n",
      "mixnet_xl model average inference time : 30.40233612060547ms\n",
      "Benchmarking Inference ens_adv_inception_resnet_v2 \n",
      "ens_adv_inception_resnet_v2 model average inference time : 57.010860443115234ms\n",
      "Benchmarking Inference tf_efficientnet_lite3 \n",
      "tf_efficientnet_lite3 model average inference time : 13.321521282196045ms\n",
      "Benchmarking Inference ese_vovnet39b \n",
      "ese_vovnet39b model average inference time : 14.513218402862549ms\n",
      "Benchmarking Inference gluon_resnext101_32x4d \n",
      "gluon_resnext101_32x4d model average inference time : 45.24773359298706ms\n",
      "Benchmarking Inference legacy_seresnext101_32x4d \n",
      "legacy_seresnext101_32x4d model average inference time : 58.8041090965271ms\n",
      "Benchmarking Inference cspresnext50 \n",
      "cspresnext50 model average inference time : 24.183776378631592ms\n",
      "Benchmarking Inference regnety_320 \n",
      "regnety_320 model average inference time : 60.277042388916016ms\n",
      "Benchmarking Inference cspresnet50 \n",
      "cspresnet50 model average inference time : 14.007270336151123ms\n",
      "Benchmarking Inference xception71 \n",
      "xception71 model average inference time : 23.552582263946533ms\n",
      "Benchmarking Inference resmlp_big_24_224 \n",
      "resmlp_big_24_224 model average inference time : 36.26847505569458ms\n",
      "Benchmarking Inference gluon_resnext101_64x4d \n",
      "gluon_resnext101_64x4d model average inference time : 72.80056715011597ms\n",
      "Benchmarking Inference efficientnet_em \n",
      "efficientnet_em model average inference time : 12.492294311523438ms\n",
      "Benchmarking Inference deit_small_patch16_224 \n",
      "deit_small_patch16_224 model average inference time : 6.665711402893066ms\n",
      "Benchmarking Inference pit_xs_distilled_224 \n",
      "pit_xs_distilled_224 model average inference time : 13.45930814743042ms\n",
      "Benchmarking Inference efficientnet_b2_pruned \n",
      "efficientnet_b2_pruned model average inference time : 21.822547912597656ms\n",
      "Benchmarking Inference dpn107 \n",
      "dpn107 model average inference time : 68.79962921142578ms\n",
      "Benchmarking Inference resmlp_36_224 \n",
      "resmlp_36_224 model average inference time : 10.581929683685303ms\n",
      "Benchmarking Inference levit_192 \n",
      "levit_192 model average inference time : 9.580020904541016ms\n",
      "Benchmarking Inference gluon_resnet152_v1c \n",
      "gluon_resnet152_v1c model average inference time : 36.30887985229492ms\n",
      "Benchmarking Inference ecaresnet50d_pruned \n",
      "ecaresnet50d_pruned model average inference time : 16.514463424682617ms\n",
      "Benchmarking Inference resnext50d_32x4d \n",
      "resnext50d_32x4d model average inference time : 23.969454765319824ms\n",
      "Benchmarking Inference tf_efficientnetv2_b1 \n",
      "tf_efficientnetv2_b1 model average inference time : 21.87920331954956ms\n",
      "Benchmarking Inference regnety_120 \n",
      "regnety_120 model average inference time : 34.869704246520996ms\n",
      "Benchmarking Inference regnetx_320 \n",
      "regnetx_320 model average inference time : 58.58788251876831ms\n",
      "Benchmarking Inference nf_regnet_b1 \n",
      "nf_regnet_b1 model average inference time : 46.61731481552124ms\n",
      "Benchmarking Inference dpn92 \n",
      "dpn92 model average inference time : 43.35390567779541ms\n",
      "Benchmarking Inference gluon_resnet152_v1b \n",
      "gluon_resnet152_v1b model average inference time : 35.709426403045654ms\n",
      "Benchmarking Inference rexnet_130 \n",
      "rexnet_130 model average inference time : 15.473835468292236ms\n",
      "Benchmarking Inference resnetrs50 \n",
      "resnetrs50 model average inference time : 20.467588901519775ms\n",
      "Benchmarking Inference dpn131 \n",
      "dpn131 model average inference time : 68.6958646774292ms\n",
      "Benchmarking Inference regnetx_160 \n",
      "regnetx_160 model average inference time : 37.68540143966675ms\n",
      "Benchmarking Inference dla102x2 \n",
      "dla102x2 model average inference time : 62.18226909637451ms\n",
      "Benchmarking Inference gluon_seresnext50_32x4d \n",
      "gluon_seresnext50_32x4d model average inference time : 30.013272762298584ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Inference xception65 \n",
      "xception65 model average inference time : 19.800798892974854ms\n",
      "Benchmarking Inference skresnext50_32x4d \n",
      "skresnext50_32x4d model average inference time : 35.61493158340454ms\n",
      "Benchmarking Inference dpn98 \n",
      "dpn98 model average inference time : 52.345054149627686ms\n",
      "Benchmarking Inference gluon_resnet101_v1c \n",
      "gluon_resnet101_v1c model average inference time : 24.925501346588135ms\n",
      "Benchmarking Inference dpn68b \n",
      "dpn68b model average inference time : 35.77160835266113ms\n",
      "Benchmarking Inference regnety_064 \n",
      "regnety_064 model average inference time : 34.27822828292847ms\n",
      "Benchmarking Inference resnetblur50 \n",
      "resnetblur50 model average inference time : 13.456873893737793ms\n",
      "Benchmarking Inference resmlp_24_224 \n",
      "resmlp_24_224 model average inference time : 7.339167594909668ms\n",
      "Benchmarking Inference coat_lite_mini \n",
      "coat_lite_mini model average inference time : 11.493215560913086ms\n",
      "Benchmarking Inference regnety_080 \n",
      "regnety_080 model average inference time : 30.399339199066162ms\n",
      "Benchmarking Inference cait_xxs24_224 \n",
      "cait_xxs24_224 model average inference time : 17.500200271606445ms\n",
      "Benchmarking Inference resnext50_32x4d \n",
      "resnext50_32x4d model average inference time : 23.546035289764404ms\n",
      "Benchmarking Inference resnext101_32x8d \n",
      "resnext101_32x8d model average inference time : 63.384528160095215ms\n",
      "Benchmarking Inference gluon_inception_v3 \n",
      "gluon_inception_v3 model average inference time : 22.422194480895996ms\n",
      "Benchmarking Inference hrnet_w48 \n",
      "hrnet_w48 model average inference time : 78.23988437652588ms\n",
      "Benchmarking Inference gluon_xception65 \n",
      "gluon_xception65 model average inference time : 19.773759841918945ms\n",
      "Benchmarking Inference gluon_resnet101_v1b \n",
      "gluon_resnet101_v1b model average inference time : 24.857568740844727ms\n",
      "Benchmarking Inference regnetx_120 \n",
      "regnetx_120 model average inference time : 31.976091861724854ms\n",
      "Benchmarking Inference xception \n",
      "xception model average inference time : 11.336536407470703ms\n",
      "Benchmarking Inference tf_efficientnet_b1_ap \n",
      "tf_efficientnet_b1_ap model average inference time : 21.77605628967285ms\n",
      "Benchmarking Inference hrnet_w64 \n",
      "hrnet_w64 model average inference time : 78.03293704986572ms\n",
      "Benchmarking Inference ssl_resnet50 \n",
      "ssl_resnet50 model average inference time : 13.15248966217041ms\n",
      "Benchmarking Inference res2net101_26w_4s \n",
      "res2net101_26w_4s model average inference time : 42.26217985153198ms\n",
      "Benchmarking Inference tf_efficientnet_cc_b1_8e \n",
      "tf_efficientnet_cc_b1_8e model average inference time : 24.27849054336548ms\n",
      "Benchmarking Inference res2net50_26w_8s \n",
      "res2net50_26w_8s model average inference time : 37.63259172439575ms\n",
      "Benchmarking Inference resnest26d \n",
      "resnest26d model average inference time : 12.774097919464111ms\n",
      "Benchmarking Inference gluon_resnext50_32x4d \n",
      "gluon_resnext50_32x4d model average inference time : 23.45468759536743ms\n",
      "Benchmarking Inference tf_efficientnet_b0_ns \n",
      "tf_efficientnet_b0_ns model average inference time : 15.619382858276367ms\n",
      "Benchmarking Inference coat_tiny \n",
      "coat_tiny model average inference time : 35.55366277694702ms\n",
      "Benchmarking Inference regnety_040 \n",
      "regnety_040 model average inference time : 29.794514179229736ms\n",
      "Benchmarking Inference dla169 \n",
      "dla169 model average inference time : 39.723639488220215ms\n",
      "Benchmarking Inference tf_efficientnet_b1 \n",
      "tf_efficientnet_b1 model average inference time : 21.92162036895752ms\n",
      "Benchmarking Inference legacy_seresnext50_32x4d \n",
      "legacy_seresnext50_32x4d model average inference time : 29.979007244110107ms\n",
      "Benchmarking Inference hrnet_w44 \n",
      "hrnet_w44 model average inference time : 78.14115762710571ms\n",
      "Benchmarking Inference gluon_resnet50_v1s \n",
      "gluon_resnet50_v1s model average inference time : 13.584201335906982ms\n",
      "Benchmarking Inference regnetx_080 \n",
      "regnetx_080 model average inference time : 28.918519020080566ms\n",
      "Benchmarking Inference levit_128 \n",
      "levit_128 model average inference time : 9.593560695648193ms\n",
      "Benchmarking Inference gluon_resnet50_v1d \n",
      "gluon_resnet50_v1d model average inference time : 13.726880550384521ms\n",
      "Benchmarking Inference dla60_res2next \n",
      "dla60_res2next model average inference time : 30.87926149368286ms\n",
      "Benchmarking Inference vit_tiny_patch16_384 \n",
      "vit_tiny_patch16_384 model average inference time : 7.711431980133057ms\n",
      "Benchmarking Inference mixnet_l \n",
      "mixnet_l model average inference time : 25.013890266418457ms\n",
      "Benchmarking Inference tv_resnet152 \n",
      "tv_resnet152 model average inference time : 35.722551345825195ms\n",
      "Benchmarking Inference dla60_res2net \n",
      "dla60_res2net model average inference time : 24.49625015258789ms\n",
      "Benchmarking Inference dla102x \n",
      "dla102x model average inference time : 42.59997367858887ms\n",
      "Benchmarking Inference pit_xs_224 \n",
      "pit_xs_224 model average inference time : 13.426010608673096ms\n",
      "Benchmarking Inference xception41 \n",
      "xception41 model average inference time : 13.454711437225342ms\n",
      "Benchmarking Inference regnetx_064 \n",
      "regnetx_064 model average inference time : 23.596620559692383ms\n",
      "Benchmarking Inference hrnet_w40 \n",
      "hrnet_w40 model average inference time : 78.1423830986023ms\n",
      "Benchmarking Inference res2net50_26w_6s \n",
      "res2net50_26w_6s model average inference time : 29.89434003829956ms\n",
      "Benchmarking Inference repvgg_b2 \n",
      "repvgg_b2 model average inference time : 23.297388553619385ms\n",
      "Benchmarking Inference resmlp_12_distilled_224 \n",
      "resmlp_12_distilled_224 model average inference time : 4.392945766448975ms\n",
      "Benchmarking Inference legacy_seresnet152 \n",
      "legacy_seresnet152 model average inference time : 56.50923490524292ms\n",
      "Benchmarking Inference selecsls60b \n",
      "selecsls60b model average inference time : 15.830533504486084ms\n",
      "Benchmarking Inference hrnet_w32 \n",
      "hrnet_w32 model average inference time : 78.29899787902832ms\n",
      "Benchmarking Inference tf_efficientnetv2_b0 \n",
      "tf_efficientnetv2_b0 model average inference time : 17.900583744049072ms\n",
      "Benchmarking Inference efficientnet_b1 \n",
      "efficientnet_b1 model average inference time : 21.907551288604736ms\n",
      "Benchmarking Inference regnetx_040 \n",
      "regnetx_040 model average inference time : 24.754343032836914ms\n",
      "Benchmarking Inference efficientnet_es \n",
      "efficientnet_es model average inference time : 9.546875953674316ms\n",
      "Benchmarking Inference hrnet_w30 \n",
      "hrnet_w30 model average inference time : 78.08218955993652ms\n",
      "Benchmarking Inference tf_mixnet_l \n",
      "tf_mixnet_l model average inference time : 24.884541034698486ms\n",
      "Benchmarking Inference wide_resnet101_2 \n",
      "wide_resnet101_2 model average inference time : 31.614665985107422ms\n",
      "Benchmarking Inference dla60x \n",
      "dla60x model average inference time : 25.543386936187744ms\n",
      "Benchmarking Inference legacy_seresnet101 \n",
      "legacy_seresnet101 model average inference time : 37.91924476623535ms\n",
      "Benchmarking Inference tf_efficientnet_em \n",
      "tf_efficientnet_em model average inference time : 12.597429752349854ms\n",
      "Benchmarking Inference coat_lite_tiny \n",
      "coat_lite_tiny model average inference time : 11.528496742248535ms\n",
      "Benchmarking Inference repvgg_b1 \n",
      "repvgg_b1 model average inference time : 17.975382804870605ms\n",
      "Benchmarking Inference efficientnet_b1_pruned \n",
      "efficientnet_b1_pruned model average inference time : 21.74143075942993ms\n",
      "Benchmarking Inference res2net50_26w_4s \n",
      "res2net50_26w_4s model average inference time : 21.786844730377197ms\n",
      "Benchmarking Inference hardcorenas_f \n",
      "hardcorenas_f model average inference time : 16.0121488571167ms\n",
      "Benchmarking Inference res2net50_14w_8s \n",
      "res2net50_14w_8s model average inference time : 37.401931285858154ms\n",
      "Benchmarking Inference selecsls60 \n",
      "selecsls60 model average inference time : 15.870141983032227ms\n",
      "Benchmarking Inference regnetx_032 \n",
      "regnetx_032 model average inference time : 23.303406238555908ms\n",
      "Benchmarking Inference res2next50 \n",
      "res2next50 model average inference time : 28.005220890045166ms\n",
      "Benchmarking Inference gluon_resnet50_v1c \n",
      "gluon_resnet50_v1c model average inference time : 13.912467956542969ms\n",
      "Benchmarking Inference dla102 \n",
      "dla102 model average inference time : 25.287036895751953ms\n",
      "Benchmarking Inference rexnet_100 \n",
      "rexnet_100 model average inference time : 15.513498783111572ms\n",
      "Benchmarking Inference tf_inception_v3 \n",
      "tf_inception_v3 model average inference time : 22.589099407196045ms\n",
      "Benchmarking Inference res2net50_48w_2s \n",
      "res2net50_48w_2s model average inference time : 13.783876895904541ms\n",
      "Benchmarking Inference resnet34d \n",
      "resnet34d model average inference time : 12.148823738098145ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Inference tf_efficientnet_lite2 \n",
      "tf_efficientnet_lite2 model average inference time : 11.678578853607178ms\n",
      "Benchmarking Inference efficientnet_b0 \n",
      "efficientnet_b0 model average inference time : 15.488321781158447ms\n",
      "Benchmarking Inference gmixer_24_224 \n",
      "gmixer_24_224 model average inference time : 10.4020357131958ms\n",
      "Benchmarking Inference hardcorenas_e \n",
      "hardcorenas_e model average inference time : 16.024208068847656ms\n",
      "Benchmarking Inference tf_efficientnet_cc_b0_8e \n",
      "tf_efficientnet_cc_b0_8e model average inference time : 17.45192289352417ms\n",
      "Benchmarking Inference tv_resnext50_32x4d \n",
      "tv_resnext50_32x4d model average inference time : 23.615386486053467ms\n",
      "Benchmarking Inference regnety_016 \n",
      "regnety_016 model average inference time : 37.89597749710083ms\n",
      "Benchmarking Inference gluon_resnet50_v1b \n",
      "gluon_resnet50_v1b model average inference time : 13.205344676971436ms\n",
      "Benchmarking Inference densenet161 \n",
      "densenet161 model average inference time : 39.54745054244995ms\n",
      "Benchmarking Inference adv_inception_v3 \n",
      "adv_inception_v3 model average inference time : 22.53640651702881ms\n",
      "Benchmarking Inference mobilenetv2_120d \n",
      "mobilenetv2_120d model average inference time : 13.914711475372314ms\n",
      "Benchmarking Inference seresnext26t_32x4d \n",
      "seresnext26t_32x4d model average inference time : 16.58782958984375ms\n",
      "Benchmarking Inference tv_resnet101 \n",
      "tv_resnet101 model average inference time : 24.6561336517334ms\n",
      "Benchmarking Inference inception_v3 \n",
      "inception_v3 model average inference time : 22.784764766693115ms\n",
      "Benchmarking Inference hardcorenas_d \n",
      "hardcorenas_d model average inference time : 17.058401107788086ms\n",
      "Benchmarking Inference seresnext26d_32x4d \n",
      "seresnext26d_32x4d model average inference time : 16.565802097320557ms\n",
      "Benchmarking Inference dla60 \n",
      "dla60 model average inference time : 15.73573112487793ms\n",
      "Benchmarking Inference repvgg_b1g4 \n",
      "repvgg_b1g4 model average inference time : 19.077744483947754ms\n",
      "Benchmarking Inference legacy_seresnet50 \n",
      "legacy_seresnet50 model average inference time : 19.75764513015747ms\n",
      "Benchmarking Inference tf_efficientnet_b0_ap \n",
      "tf_efficientnet_b0_ap model average inference time : 15.696699619293213ms\n",
      "Benchmarking Inference skresnet34 \n",
      "skresnet34 model average inference time : 22.068228721618652ms\n",
      "Benchmarking Inference tf_efficientnet_cc_b0_4e \n",
      "tf_efficientnet_cc_b0_4e model average inference time : 17.553105354309082ms\n",
      "Benchmarking Inference resmlp_12_224 \n",
      "resmlp_12_224 model average inference time : 4.048259258270264ms\n",
      "Benchmarking Inference densenet201 \n",
      "densenet201 model average inference time : 49.57787752151489ms\n",
      "Benchmarking Inference mobilenetv3_large_100_miil \n",
      "mobilenetv3_large_100_miil model average inference time : 11.783394813537598ms\n",
      "Benchmarking Inference gernet_s \n",
      "gernet_s model average inference time : 10.78639268875122ms\n",
      "Benchmarking Inference legacy_seresnext26_32x4d \n",
      "legacy_seresnext26_32x4d model average inference time : 15.90550422668457ms\n",
      "Benchmarking Inference mixnet_m \n",
      "mixnet_m model average inference time : 24.742238521575928ms\n",
      "Benchmarking Inference tf_efficientnet_b0 \n",
      "tf_efficientnet_b0 model average inference time : 15.570008754730225ms\n",
      "Benchmarking Inference hrnet_w18 \n",
      "hrnet_w18 model average inference time : 78.14085006713867ms\n",
      "Benchmarking Inference densenetblur121d \n",
      "densenetblur121d model average inference time : 30.081913471221924ms\n",
      "Benchmarking Inference selecsls42b \n",
      "selecsls42b model average inference time : 12.409875392913818ms\n",
      "Benchmarking Inference tf_efficientnet_lite1 \n",
      "tf_efficientnet_lite1 model average inference time : 11.537988185882568ms\n",
      "Benchmarking Inference hardcorenas_c \n",
      "hardcorenas_c model average inference time : 12.615303993225098ms\n",
      "Benchmarking Inference regnetx_016 \n",
      "regnetx_016 model average inference time : 19.429233074188232ms\n",
      "Benchmarking Inference mobilenetv2_140 \n",
      "mobilenetv2_140 model average inference time : 9.3467116355896ms\n",
      "Benchmarking Inference dpn68 \n",
      "dpn68 model average inference time : 32.70716190338135ms\n",
      "Benchmarking Inference tf_efficientnet_es \n",
      "tf_efficientnet_es model average inference time : 9.875457286834717ms\n",
      "Benchmarking Inference tf_mixnet_m \n",
      "tf_mixnet_m model average inference time : 25.16129493713379ms\n",
      "Benchmarking Inference ese_vovnet19b_dw \n",
      "ese_vovnet19b_dw model average inference time : 7.285418510437012ms\n",
      "Benchmarking Inference levit_128s \n",
      "levit_128s model average inference time : 7.949166297912598ms\n",
      "Benchmarking Inference resnet26d \n",
      "resnet26d model average inference time : 8.064417839050293ms\n",
      "Benchmarking Inference repvgg_a2 \n",
      "repvgg_a2 model average inference time : 12.027881145477295ms\n",
      "Benchmarking Inference tv_resnet50 \n",
      "tv_resnet50 model average inference time : 13.208227157592773ms\n",
      "Benchmarking Inference hardcorenas_b \n",
      "hardcorenas_b model average inference time : 11.70189619064331ms\n",
      "Benchmarking Inference densenet121 \n",
      "densenet121 model average inference time : 29.414219856262207ms\n",
      "Benchmarking Inference vit_tiny_r_s16_p8_384 \n",
      "vit_tiny_r_s16_p8_384 model average inference time : 8.360729217529297ms\n",
      "Benchmarking Inference densenet169 \n",
      "densenet169 model average inference time : 41.47421360015869ms\n",
      "Benchmarking Inference mixnet_s \n",
      "mixnet_s model average inference time : 20.370938777923584ms\n",
      "Benchmarking Inference vit_small_patch32_224 \n",
      "vit_small_patch32_224 model average inference time : 6.572155952453613ms\n",
      "Benchmarking Inference regnety_008 \n",
      "regnety_008 model average inference time : 22.462012767791748ms\n",
      "Benchmarking Inference efficientnet_lite0 \n",
      "efficientnet_lite0 model average inference time : 8.851077556610107ms\n",
      "Benchmarking Inference resnest14d \n",
      "resnest14d model average inference time : 7.5067853927612305ms\n",
      "Benchmarking Inference hardcorenas_a \n",
      "hardcorenas_a model average inference time : 9.79759693145752ms\n",
      "Benchmarking Inference efficientnet_es_pruned \n",
      "efficientnet_es_pruned model average inference time : 9.677238464355469ms\n",
      "Benchmarking Inference mobilenetv3_rw \n",
      "mobilenetv3_rw model average inference time : 12.055420875549316ms\n",
      "Benchmarking Inference semnasnet_100 \n",
      "semnasnet_100 model average inference time : 12.051868438720703ms\n",
      "Benchmarking Inference mobilenetv3_large_100 \n",
      "mobilenetv3_large_100 model average inference time : 11.939566135406494ms\n",
      "Benchmarking Inference resnet34 \n",
      "resnet34 model average inference time : 11.482102870941162ms\n",
      "Benchmarking Inference mobilenetv2_110d \n",
      "mobilenetv2_110d model average inference time : 11.972315311431885ms\n",
      "Benchmarking Inference vit_tiny_patch16_224 \n",
      "vit_tiny_patch16_224 model average inference time : 6.647017002105713ms\n",
      "Benchmarking Inference tf_mixnet_s \n",
      "tf_mixnet_s model average inference time : 20.821356773376465ms\n",
      "Benchmarking Inference repvgg_b0 \n",
      "repvgg_b0 model average inference time : 14.782853126525879ms\n",
      "Benchmarking Inference deit_tiny_distilled_patch16_224 \n",
      "deit_tiny_distilled_patch16_224 model average inference time : 6.631865501403809ms\n",
      "Benchmarking Inference mixer_b16_224 \n",
      "mixer_b16_224 model average inference time : 5.857203006744385ms\n",
      "Benchmarking Inference pit_ti_distilled_224 \n",
      "pit_ti_distilled_224 model average inference time : 11.387217044830322ms\n",
      "Benchmarking Inference hrnet_w18_small_v2 \n",
      "hrnet_w18_small_v2 model average inference time : 40.0214958190918ms\n",
      "Benchmarking Inference tf_efficientnet_lite0 \n",
      "tf_efficientnet_lite0 model average inference time : 9.183862209320068ms\n",
      "Benchmarking Inference resnet26 \n",
      "resnet26 model average inference time : 7.71432638168335ms\n",
      "Benchmarking Inference tf_mobilenetv3_large_100 \n",
      "tf_mobilenetv3_large_100 model average inference time : 12.049391269683838ms\n",
      "Benchmarking Inference tv_densenet121 \n",
      "tv_densenet121 model average inference time : 29.645564556121826ms\n",
      "Benchmarking Inference regnety_006 \n",
      "regnety_006 model average inference time : 23.649604320526123ms\n",
      "Benchmarking Inference dla34 \n",
      "dla34 model average inference time : 10.816187858581543ms\n",
      "Benchmarking Inference fbnetc_100 \n",
      "fbnetc_100 model average inference time : 11.311442852020264ms\n",
      "Benchmarking Inference legacy_seresnet34 \n",
      "legacy_seresnet34 model average inference time : 16.80896759033203ms\n",
      "Benchmarking Inference gluon_resnet34_v1b \n",
      "gluon_resnet34_v1b model average inference time : 11.373028755187988ms\n",
      "Benchmarking Inference regnetx_008 \n",
      "regnetx_008 model average inference time : 19.651708602905273ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Inference mnasnet_100 \n",
      "mnasnet_100 model average inference time : 9.475789070129395ms\n",
      "Benchmarking Inference vgg19_bn \n",
      "vgg19_bn model average inference time : 21.40399694442749ms\n",
      "Benchmarking Inference convit_tiny \n",
      "convit_tiny model average inference time : 8.962047100067139ms\n",
      "Benchmarking Inference spnasnet_100 \n",
      "spnasnet_100 model average inference time : 11.406304836273193ms\n",
      "Benchmarking Inference ghostnet_100 \n",
      "ghostnet_100 model average inference time : 15.808005332946777ms\n",
      "Benchmarking Inference regnety_004 \n",
      "regnety_004 model average inference time : 29.50500249862671ms\n",
      "Benchmarking Inference skresnet18 \n",
      "skresnet18 model average inference time : 12.039923667907715ms\n",
      "Benchmarking Inference regnetx_006 \n",
      "regnetx_006 model average inference time : 16.601219177246094ms\n",
      "Benchmarking Inference pit_ti_224 \n",
      "pit_ti_224 model average inference time : 11.367049217224121ms\n",
      "Benchmarking Inference swsl_resnet18 \n",
      "swsl_resnet18 model average inference time : 6.388416290283203ms\n",
      "Benchmarking Inference vgg16_bn \n",
      "vgg16_bn model average inference time : 18.277535438537598ms\n",
      "Benchmarking Inference tv_resnet34 \n",
      "tv_resnet34 model average inference time : 11.475884914398193ms\n",
      "Benchmarking Inference resnet18d \n",
      "resnet18d model average inference time : 6.916868686676025ms\n",
      "Benchmarking Inference mobilenetv2_100 \n",
      "mobilenetv2_100 model average inference time : 9.230058193206787ms\n",
      "Benchmarking Inference ssl_resnet18 \n",
      "ssl_resnet18 model average inference time : 6.415367126464844ms\n",
      "Benchmarking Inference tf_mobilenetv3_large_075 \n",
      "tf_mobilenetv3_large_075 model average inference time : 11.915311813354492ms\n",
      "Benchmarking Inference deit_tiny_patch16_224 \n",
      "deit_tiny_patch16_224 model average inference time : 6.59926176071167ms\n",
      "Benchmarking Inference hrnet_w18_small \n",
      "hrnet_w18_small model average inference time : 22.430191040039062ms\n",
      "Benchmarking Inference vgg19 \n",
      "vgg19 model average inference time : 20.628080368041992ms\n",
      "Benchmarking Inference regnetx_004 \n",
      "regnetx_004 model average inference time : 23.761541843414307ms\n",
      "Benchmarking Inference tf_mobilenetv3_large_minimal_100 \n",
      "tf_mobilenetv3_large_minimal_100 model average inference time : 8.6152982711792ms\n",
      "Benchmarking Inference legacy_seresnet18 \n",
      "legacy_seresnet18 model average inference time : 9.321844577789307ms\n",
      "Benchmarking Inference vgg16 \n",
      "vgg16 model average inference time : 17.57899045944214ms\n",
      "Benchmarking Inference vgg13_bn \n",
      "vgg13_bn model average inference time : 15.178887844085693ms\n",
      "Benchmarking Inference vit_tiny_r_s16_p8_224 \n",
      "vit_tiny_r_s16_p8_224 model average inference time : 6.904339790344238ms\n",
      "Benchmarking Inference gluon_resnet18_v1b \n",
      "gluon_resnet18_v1b model average inference time : 6.424798965454102ms\n",
      "Benchmarking Inference vgg11_bn \n",
      "vgg11_bn model average inference time : 11.495509147644043ms\n",
      "Benchmarking Inference regnety_002 \n",
      "regnety_002 model average inference time : 23.84559392929077ms\n",
      "Benchmarking Inference mixer_l16_224 \n",
      "mixer_l16_224 model average inference time : 18.659563064575195ms\n",
      "Benchmarking Inference resnet18 \n",
      "resnet18 model average inference time : 6.515593528747559ms\n",
      "Benchmarking Inference vgg13 \n",
      "vgg13 model average inference time : 14.501135349273682ms\n",
      "Benchmarking Inference vgg11 \n",
      "vgg11 model average inference time : 11.102778911590576ms\n",
      "Benchmarking Inference regnetx_002 \n",
      "regnetx_002 model average inference time : 18.230838775634766ms\n",
      "Benchmarking Inference dla60x_c \n",
      "dla60x_c model average inference time : 24.40481662750244ms\n",
      "Benchmarking Inference tf_mobilenetv3_small_100 \n",
      "tf_mobilenetv3_small_100 model average inference time : 10.441474914550781ms\n",
      "Benchmarking Inference dla46x_c \n",
      "dla46x_c model average inference time : 18.950676918029785ms\n",
      "Benchmarking Inference tf_mobilenetv3_small_075 \n",
      "tf_mobilenetv3_small_075 model average inference time : 10.450973510742188ms\n",
      "Benchmarking Inference dla46_c \n",
      "dla46_c model average inference time : 11.729588508605957ms\n",
      "Benchmarking Inference tf_mobilenetv3_small_minimal_100 \n",
      "tf_mobilenetv3_small_minimal_100 model average inference time : 6.815752983093262ms\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tf_efficientnet_l2_ns': {'fp32': 681.3122296333313,\n",
       "  'top1': 90.56299999999999,\n",
       "  'imsize': 800},\n",
       " 'tf_efficientnet_l2_ns_475': {'fp32': 280.7985281944275,\n",
       "  'top1': 90.537,\n",
       "  'imsize': 475},\n",
       " 'cait_m48_448': {'fp32': 556.2820100784302, 'top1': 90.196, 'imsize': 448},\n",
       " 'vit_large_patch16_384': {'fp32': 79.5830225944519,\n",
       "  'top1': 90.196,\n",
       "  'imsize': 384},\n",
       " 'tf_efficientnet_b7_ns': {'fp32': 103.03890943527222,\n",
       "  'top1': 90.1,\n",
       "  'imsize': 600},\n",
       " 'cait_m36_384': {'fp32': 241.93207263946533, 'top1': 90.046, 'imsize': 384},\n",
       " 'dm_nfnet_f6': {'fp32': 509.15677785873413, 'top1': 90.046, 'imsize': 576},\n",
       " 'swin_large_patch4_window12_384': {'fp32': 60.2254581451416,\n",
       "  'top1': 90.027,\n",
       "  'imsize': 384},\n",
       " 'tf_efficientnetv2_l_in21ft1k': {'fp32': 93.13963651657104,\n",
       "  'top1': 90.008,\n",
       "  'imsize': 480},\n",
       " 'swin_base_patch4_window12_384': {'fp32': 36.01898908615112,\n",
       "  'top1': 89.995,\n",
       "  'imsize': 384},\n",
       " 'vit_base_patch16_384': {'fp32': 27.246394157409668,\n",
       "  'top1': 89.98899999999999,\n",
       "  'imsize': 384},\n",
       " 'cait_s36_384': {'fp32': 137.02792882919312, 'top1': 89.844, 'imsize': 384},\n",
       " 'swin_large_patch4_window7_224': {'fp32': 20.064375400543213,\n",
       "  'top1': 89.796,\n",
       "  'imsize': 224},\n",
       " 'vit_large_r50_s32_384': {'fp32': 38.56330156326294,\n",
       "  'top1': 89.794,\n",
       "  'imsize': 384},\n",
       " 'tf_efficientnet_b6_ns': {'fp32': 60.31878471374512,\n",
       "  'top1': 89.78200000000001,\n",
       "  'imsize': 528},\n",
       " 'tf_efficientnetv2_m_in21ft1k': {'fp32': 50.650856494903564,\n",
       "  'top1': 89.775,\n",
       "  'imsize': 480},\n",
       " 'tf_efficientnet_b5_ns': {'fp32': 38.72903823852539,\n",
       "  'top1': 89.65100000000001,\n",
       "  'imsize': 456},\n",
       " 'tf_efficientnet_b8_ap': {'fp32': 149.99229192733765,\n",
       "  'top1': 89.581,\n",
       "  'imsize': 672},\n",
       " 'dm_nfnet_f4': {'fp32': 324.5601511001587, 'top1': 89.557, 'imsize': 512},\n",
       " 'cait_s24_384': {'fp32': 91.95441246032715,\n",
       "  'top1': 89.50200000000001,\n",
       "  'imsize': 384},\n",
       " 'dm_nfnet_f3': {'fp32': 208.75796794891357, 'top1': 89.485, 'imsize': 416},\n",
       " 'dm_nfnet_f5': {'fp32': 416.36311531066895, 'top1': 89.461, 'imsize': 544},\n",
       " 'deit_base_distilled_patch16_384': {'fp32': 27.517845630645752,\n",
       "  'top1': 89.429,\n",
       "  'imsize': 384},\n",
       " 'tf_efficientnet_b7_ap': {'fp32': 103.11571598052979,\n",
       "  'top1': 89.429,\n",
       "  'imsize': 600},\n",
       " 'tf_efficientnetv2_l': {'fp32': 92.77960300445557,\n",
       "  'top1': 89.367,\n",
       "  'imsize': 480},\n",
       " 'tf_efficientnet_b8': {'fp32': 150.59494733810425,\n",
       "  'top1': 89.355,\n",
       "  'imsize': 672},\n",
       " 'tf_efficientnet_b6_ap': {'fp32': 60.478675365448,\n",
       "  'top1': 89.34200000000001,\n",
       "  'imsize': 528},\n",
       " 'vit_large_patch16_224': {'fp32': 24.927678108215332,\n",
       "  'top1': 89.314,\n",
       "  'imsize': 224},\n",
       " 'tf_efficientnet_b4_ns': {'fp32': 30.860531330108643,\n",
       "  'top1': 89.305,\n",
       "  'imsize': 380},\n",
       " 'tf_efficientnetv2_m': {'fp32': 50.62763690948486,\n",
       "  'top1': 89.28399999999999,\n",
       "  'imsize': 480},\n",
       " 'swin_base_patch4_window7_224': {'fp32': 15.933451652526855,\n",
       "  'top1': 89.145,\n",
       "  'imsize': 224},\n",
       " 'eca_nfnet_l2': {'fp32': 73.14308166503906, 'top1': 89.141, 'imsize': 384},\n",
       " 'cait_xs24_384': {'fp32': 85.18592357635498, 'top1': 89.139, 'imsize': 384},\n",
       " 'ig_resnext101_32x48d': {'fp32': 362.3937916755676,\n",
       "  'top1': 89.12,\n",
       "  'imsize': 224},\n",
       " 'ig_resnext101_32x32d': {'fp32': 243.0540633201599,\n",
       "  'top1': 89.111,\n",
       "  'imsize': 224},\n",
       " 'tf_efficientnet_b7': {'fp32': 102.82350778579712,\n",
       "  'top1': 89.086,\n",
       "  'imsize': 600},\n",
       " 'ecaresnet269d': {'fp32': 80.64383268356323, 'top1': 89.069, 'imsize': 352},\n",
       " 'resmlp_big_24_224_in22ft1k': {'fp32': 36.44584894180298,\n",
       "  'top1': 89.01100000000001,\n",
       "  'imsize': 224},\n",
       " 'dm_nfnet_f2': {'fp32': 132.8338122367859, 'top1': 89.009, 'imsize': 352},\n",
       " 'efficientnetv2_rw_m': {'fp32': 50.98617076873779,\n",
       "  'top1': 88.98700000000001,\n",
       "  'imsize': 416},\n",
       " 'tf_efficientnet_b5_ap': {'fp32': 38.59375,\n",
       "  'top1': 88.93799999999999,\n",
       "  'imsize': 456},\n",
       " 'dm_nfnet_f1': {'fp32': 80.64253568649292, 'top1': 88.925, 'imsize': 320},\n",
       " 'tf_efficientnetv2_s_in21ft1k': {'fp32': 33.555705547332764,\n",
       "  'top1': 88.904,\n",
       "  'imsize': 384},\n",
       " 'vit_base_patch16_224': {'fp32': 8.101544380187988,\n",
       "  'top1': 88.866,\n",
       "  'imsize': 224},\n",
       " 'resnetrs420': {'fp32': 165.99584341049194, 'top1': 88.84, 'imsize': 416},\n",
       " 'ig_resnext101_32x16d': {'fp32': 119.50673341751099,\n",
       "  'top1': 88.834,\n",
       "  'imsize': 224},\n",
       " 'resnetrs270': {'fp32': 102.45657920837402, 'top1': 88.834, 'imsize': 352},\n",
       " 'vit_small_r26_s32_384': {'fp32': 16.07172966003418,\n",
       "  'top1': 88.819,\n",
       "  'imsize': 384},\n",
       " 'vit_base_r50_s16_384': {'fp32': 44.60647106170654,\n",
       "  'top1': 88.80799999999999,\n",
       "  'imsize': 384},\n",
       " 'seresnet152d': {'fp32': 57.83982515335083, 'top1': 88.795, 'imsize': 320},\n",
       " 'swsl_resnext101_32x8d': {'fp32': 63.34679841995239,\n",
       "  'top1': 88.77,\n",
       "  'imsize': 224},\n",
       " 'tf_efficientnet_b6': {'fp32': 60.3957724571228,\n",
       "  'top1': 88.76100000000001,\n",
       "  'imsize': 528},\n",
       " 'resnetrs350': {'fp32': 130.88914155960083, 'top1': 88.759, 'imsize': 384},\n",
       " 'vit_base_patch16_224_miil': {'fp32': 7.851285934448242,\n",
       "  'top1': 88.73700000000001,\n",
       "  'imsize': 224},\n",
       " 'resnetv2_152x2_bitm': {'fp32': 164.62177753448486,\n",
       "  'top1': 88.725,\n",
       "  'imsize': 448},\n",
       " 'regnety_160': {'fp32': 50.8770489692688, 'top1': 88.697, 'imsize': 288},\n",
       " 'pit_b_distilled_224': {'fp32': 23.28129529953003,\n",
       "  'top1': 88.676,\n",
       "  'imsize': 224},\n",
       " 'vit_small_patch16_384': {'fp32': 12.297897338867188,\n",
       "  'top1': 88.652,\n",
       "  'imsize': 384},\n",
       " 'eca_nfnet_l1': {'fp32': 42.21022367477417, 'top1': 88.624, 'imsize': 320},\n",
       " 'resnetrs200': {'fp32': 76.03182554244995, 'top1': 88.605, 'imsize': 320},\n",
       " 'resnetv2_152x4_bitm': {'fp32': 610.906810760498,\n",
       "  'top1': 88.545,\n",
       "  'imsize': 480},\n",
       " 'resnet200d': {'fp32': 47.637178897857666,\n",
       "  'top1': 88.54299999999999,\n",
       "  'imsize': 320},\n",
       " 'resnest269e': {'fp32': 121.85580492019653, 'top1': 88.522, 'imsize': 416},\n",
       " 'efficientnetv2_rw_s': {'fp32': 33.643550872802734,\n",
       "  'top1': 88.473,\n",
       "  'imsize': 384},\n",
       " 'resnetv2_101x3_bitm': {'fp32': 220.26455163955688,\n",
       "  'top1': 88.464,\n",
       "  'imsize': 448},\n",
       " 'cait_s24_224': {'fp32': 17.53107786178589, 'top1': 88.447, 'imsize': 224},\n",
       " 'resnetv2_50x3_bitm': {'fp32': 119.97088432312012,\n",
       "  'top1': 88.443,\n",
       "  'imsize': 448},\n",
       " 'resmlp_big_24_distilled_224': {'fp32': 36.488635540008545,\n",
       "  'top1': 88.443,\n",
       "  'imsize': 224},\n",
       " 'resnest200e': {'fp32': 82.75031805038452, 'top1': 88.432, 'imsize': 320},\n",
       " 'tf_efficientnet_b3_ns': {'fp32': 24.923527240753174,\n",
       "  'top1': 88.426,\n",
       "  'imsize': 300},\n",
       " 'vit_large_r50_s32_224': {'fp32': 25.936448574066162,\n",
       "  'top1': 88.426,\n",
       "  'imsize': 224},\n",
       " 'tf_efficientnetv2_s': {'fp32': 33.82120132446289,\n",
       "  'top1': 88.402,\n",
       "  'imsize': 384},\n",
       " 'efficientnet_b4': {'fp32': 30.89365243911743,\n",
       "  'top1': 88.37200000000001,\n",
       "  'imsize': 384},\n",
       " 'resnet152d': {'fp32': 37.48584747314453, 'top1': 88.355, 'imsize': 320},\n",
       " 'tf_efficientnet_b4_ap': {'fp32': 30.958642959594727,\n",
       "  'top1': 88.34899999999999,\n",
       "  'imsize': 380},\n",
       " 'tf_efficientnet_b5': {'fp32': 38.48245143890381,\n",
       "  'top1': 88.321,\n",
       "  'imsize': 456},\n",
       " 'resnetrs152': {'fp32': 58.53393793106079, 'top1': 88.251, 'imsize': 320},\n",
       " 'deit_base_distilled_patch16_224': {'fp32': 8.232343196868896,\n",
       "  'top1': 88.214,\n",
       "  'imsize': 224},\n",
       " 'resnetv2_152x2_bit_teacher_384': {'fp32': 120.65580368041992,\n",
       "  'top1': 88.15,\n",
       "  'imsize': 384},\n",
       " 'ig_resnext101_32x8d': {'fp32': 63.340277671813965,\n",
       "  'top1': 88.146,\n",
       "  'imsize': 224},\n",
       " 'cait_xxs36_384': {'fp32': 79.82808113098145, 'top1': 88.14, 'imsize': 384},\n",
       " 'dm_nfnet_f0': {'fp32': 36.23260736465454, 'top1': 88.125, 'imsize': 256},\n",
       " 'swsl_resnext101_32x4d': {'fp32': 45.10894298553467,\n",
       "  'top1': 88.09899999999999,\n",
       "  'imsize': 224},\n",
       " 'eca_nfnet_l0': {'fp32': 21.97653293609619, 'top1': 87.98, 'imsize': 288},\n",
       " 'nfnet_l0': {'fp32': 25.072789192199707,\n",
       "  'top1': 87.96700000000001,\n",
       "  'imsize': 288},\n",
       " 'tf_efficientnet_b4': {'fp32': 30.840539932250977,\n",
       "  'top1': 87.963,\n",
       "  'imsize': 380},\n",
       " 'resnet101d': {'fp32': 25.911705493927002, 'top1': 87.941, 'imsize': 320},\n",
       " 'regnety_032': {'fp32': 34.19325828552246,\n",
       "  'top1': 87.93700000000001,\n",
       "  'imsize': 288},\n",
       " 'vit_base_patch32_384': {'fp32': 8.40101957321167,\n",
       "  'top1': 87.90899999999999,\n",
       "  'imsize': 384},\n",
       " 'twins_svt_large': {'fp32': 17.415297031402588,\n",
       "  'top1': 87.90100000000001,\n",
       "  'imsize': 224},\n",
       " 'twins_pcpvt_large': {'fp32': 33.01650047302246,\n",
       "  'top1': 87.87700000000001,\n",
       "  'imsize': 224},\n",
       " 'deit_base_patch16_384': {'fp32': 27.08406686782837,\n",
       "  'top1': 87.845,\n",
       "  'imsize': 384},\n",
       " 'resnetv2_50x1_bit_distilled': {'fp32': 15.680091381072998,\n",
       "  'top1': 87.787,\n",
       "  'imsize': 224},\n",
       " 'twins_pcpvt_base': {'fp32': 23.03375482559204,\n",
       "  'top1': 87.736,\n",
       "  'imsize': 224},\n",
       " 'resnetv2_101x1_bitm': {'fp32': 40.998923778533936,\n",
       "  'top1': 87.681,\n",
       "  'imsize': 448},\n",
       " 'swin_small_patch4_window7_224': {'fp32': 15.727837085723877,\n",
       "  'top1': 87.664,\n",
       "  'imsize': 224},\n",
       " 'twins_svt_base': {'fp32': 17.704596519470215,\n",
       "  'top1': 87.63799999999999,\n",
       "  'imsize': 224},\n",
       " 'pnasnet5large': {'fp32': 53.48133563995361,\n",
       "  'top1': 87.63600000000001,\n",
       "  'imsize': 331},\n",
       " 'swsl_resnext101_32x16d': {'fp32': 118.77110242843628,\n",
       "  'top1': 87.615,\n",
       "  'imsize': 224},\n",
       " 'swsl_resnext50_32x4d': {'fp32': 23.418021202087402,\n",
       "  'top1': 87.6,\n",
       "  'imsize': 224},\n",
       " 'tf_efficientnet_b2_ns': {'fp32': 22.01782464981079,\n",
       "  'top1': 87.557,\n",
       "  'imsize': 260},\n",
       " 'levit_384': {'fp32': 9.783318042755127, 'top1': 87.553, 'imsize': 224},\n",
       " 'ecaresnet50t': {'fp32': 16.986160278320312, 'top1': 87.538, 'imsize': 320},\n",
       " 'resnetv2_152x2_bit_teacher': {'fp32': 67.48608827590942,\n",
       "  'top1': 87.493,\n",
       "  'imsize': 224},\n",
       " 'efficientnet_b3': {'fp32': 24.941017627716064,\n",
       "  'top1': 87.435,\n",
       "  'imsize': 320},\n",
       " 'cait_xxs24_384': {'fp32': 53.56301546096802, 'top1': 87.416, 'imsize': 384},\n",
       " 'resnet51q': {'fp32': 15.64049243927002, 'top1': 87.395, 'imsize': 288},\n",
       " 'coat_lite_small': {'fp32': 21.108202934265137, 'top1': 87.38, 'imsize': 224},\n",
       " 'nasnetalarge': {'fp32': 68.66656303405762, 'top1': 87.35, 'imsize': 331},\n",
       " 'ecaresnet101d': {'fp32': 30.888094902038574, 'top1': 87.288, 'imsize': 224},\n",
       " 'resnest101e': {'fp32': 42.74486064910889,\n",
       "  'top1': 87.28399999999999,\n",
       "  'imsize': 256},\n",
       " 'pit_s_distilled_224': {'fp32': 16.380975246429443,\n",
       "  'top1': 87.277,\n",
       "  'imsize': 224},\n",
       " 'resnetrs101': {'fp32': 39.43504810333252,\n",
       "  'top1': 87.24700000000001,\n",
       "  'imsize': 288},\n",
       " 'mixer_b16_224_miil': {'fp32': 5.967974662780762,\n",
       "  'top1': 87.226,\n",
       "  'imsize': 224},\n",
       " 'convit_base': {'fp32': 12.051944732666016, 'top1': 87.2, 'imsize': 224},\n",
       " 'tf_efficientnet_b3_ap': {'fp32': 25.10059118270874,\n",
       "  'top1': 87.19200000000001,\n",
       "  'imsize': 300},\n",
       " 'visformer_small': {'fp32': 15.008840560913086,\n",
       "  'top1': 87.181,\n",
       "  'imsize': 224},\n",
       " 'convit_small': {'fp32': 8.650243282318115, 'top1': 87.053, 'imsize': 224},\n",
       " 'tf_efficientnetv2_b3': {'fp32': 26.755173206329346,\n",
       "  'top1': 87.03200000000001,\n",
       "  'imsize': 300},\n",
       " 'deit_small_distilled_patch16_224': {'fp32': 6.368427276611328,\n",
       "  'top1': 86.993,\n",
       "  'imsize': 224},\n",
       " 'resmlp_36_distilled_224': {'fp32': 10.460522174835205,\n",
       "  'top1': 86.993,\n",
       "  'imsize': 224},\n",
       " 'tnt_s_patch16_224': {'fp32': 14.255411624908447,\n",
       "  'top1': 86.90299999999999,\n",
       "  'imsize': 224},\n",
       " 'vit_small_patch16_224': {'fp32': 6.382865905761719,\n",
       "  'top1': 86.869,\n",
       "  'imsize': 224},\n",
       " 'vit_small_r26_s32_224': {'fp32': 14.236021041870117,\n",
       "  'top1': 86.863,\n",
       "  'imsize': 224},\n",
       " 'ssl_resnext101_32x16d': {'fp32': 118.77163887023926,\n",
       "  'top1': 86.85600000000001,\n",
       "  'imsize': 224},\n",
       " 'rexnet_200': {'fp32': 15.597269535064697, 'top1': 86.846, 'imsize': 224},\n",
       " 'tf_efficientnet_b3': {'fp32': 24.86647129058838,\n",
       "  'top1': 86.835,\n",
       "  'imsize': 300},\n",
       " 'deit_base_patch16_224': {'fp32': 8.074183464050293,\n",
       "  'top1': 86.829,\n",
       "  'imsize': 224},\n",
       " 'ssl_resnext101_32x8d': {'fp32': 62.92789936065674,\n",
       "  'top1': 86.807,\n",
       "  'imsize': 224},\n",
       " 'swsl_resnet50': {'fp32': 13.253288269042969, 'top1': 86.807, 'imsize': 224},\n",
       " 'tf_efficientnet_lite4': {'fp32': 17.07280397415161,\n",
       "  'top1': 86.803,\n",
       "  'imsize': 380},\n",
       " 'coat_mini': {'fp32': 36.04537487030029,\n",
       "  'top1': 86.79299999999999,\n",
       "  'imsize': 224},\n",
       " 'twins_svt_small': {'fp32': 13.34057092666626, 'top1': 86.756, 'imsize': 224},\n",
       " 'levit_256': {'fp32': 9.783623218536377, 'top1': 86.728, 'imsize': 224},\n",
       " 'seresnext50_32x4d': {'fp32': 30.111424922943115,\n",
       "  'top1': 86.699,\n",
       "  'imsize': 224},\n",
       " 'pit_b_224': {'fp32': 22.816636562347412, 'top1': 86.686, 'imsize': 224},\n",
       " 'tf_efficientnet_b1_ns': {'fp32': 22.025365829467773,\n",
       "  'top1': 86.669,\n",
       "  'imsize': 240},\n",
       " 'swin_tiny_patch4_window7_224': {'fp32': 8.557002544403076,\n",
       "  'top1': 86.664,\n",
       "  'imsize': 224},\n",
       " 'gernet_l': {'fp32': 12.576262950897217, 'top1': 86.654, 'imsize': 256},\n",
       " 'wide_resnet50_2': {'fp32': 16.69198513031006, 'top1': 86.647, 'imsize': 224},\n",
       " 'efficientnet_el': {'fp32': 14.113028049468994,\n",
       "  'top1': 86.635,\n",
       "  'imsize': 300},\n",
       " 'resmlp_24_distilled_224': {'fp32': 7.340612411499023,\n",
       "  'top1': 86.62200000000001,\n",
       "  'imsize': 224},\n",
       " 'twins_pcpvt_small': {'fp32': 13.705272674560547,\n",
       "  'top1': 86.62,\n",
       "  'imsize': 224},\n",
       " 'nf_resnet50': {'fp32': 16.528451442718506, 'top1': 86.609, 'imsize': 288},\n",
       " 'resnest50d_4s2x40d': {'fp32': 24.056780338287354,\n",
       "  'top1': 86.59200000000001,\n",
       "  'imsize': 224},\n",
       " 'efficientnet_b3_pruned': {'fp32': 24.966726303100586,\n",
       "  'top1': 86.581,\n",
       "  'imsize': 300},\n",
       " 'repvgg_b3': {'fp32': 28.252103328704834, 'top1': 86.566, 'imsize': 224},\n",
       " 'ssl_resnext101_32x4d': {'fp32': 45.07107496261597,\n",
       "  'top1': 86.479,\n",
       "  'imsize': 224},\n",
       " 'ecaresnet50d': {'fp32': 16.41528844833374, 'top1': 86.47, 'imsize': 224},\n",
       " 'gluon_resnet152_v1s': {'fp32': 36.24826669692993,\n",
       "  'top1': 86.46799999999999,\n",
       "  'imsize': 224},\n",
       " 'resnest50d_1s4x24d': {'fp32': 22.795162200927734,\n",
       "  'top1': 86.447,\n",
       "  'imsize': 224},\n",
       " 'resnetv2_50x1_bitm': {'fp32': 23.91517400741577,\n",
       "  'top1': 86.436,\n",
       "  'imsize': 448},\n",
       " 'repvgg_b3g4': {'fp32': 27.49490261077881, 'top1': 86.361, 'imsize': 224},\n",
       " 'legacy_senet154': {'fp32': 118.89943361282349,\n",
       "  'top1': 86.34200000000001,\n",
       "  'imsize': 224},\n",
       " 'cait_xxs36_224': {'fp32': 25.418710708618164, 'top1': 86.34, 'imsize': 224},\n",
       " 'gernet_m': {'fp32': 10.411837100982666, 'top1': 86.319, 'imsize': 224},\n",
       " 'pit_s_224': {'fp32': 16.34552240371704, 'top1': 86.316, 'imsize': 224},\n",
       " 'vit_small_patch32_384': {'fp32': 7.977423667907715,\n",
       "  'top1': 86.31200000000001,\n",
       "  'imsize': 384},\n",
       " 'efficientnet_b2': {'fp32': 21.932315826416016,\n",
       "  'top1': 86.304,\n",
       "  'imsize': 288},\n",
       " 'gluon_senet154': {'fp32': 119.28701639175415,\n",
       "  'top1': 86.27799999999999,\n",
       "  'imsize': 224},\n",
       " 'resnest50d': {'fp32': 22.430155277252197, 'top1': 86.24, 'imsize': 224},\n",
       " 'ecaresnet101d_pruned': {'fp32': 30.854415893554688,\n",
       "  'top1': 86.21,\n",
       "  'imsize': 224},\n",
       " 'efficientnet_el_pruned': {'fp32': 14.494609832763672,\n",
       "  'top1': 86.19200000000001,\n",
       "  'imsize': 300},\n",
       " 'cspdarknet53': {'fp32': 17.536776065826416, 'top1': 86.182, 'imsize': 256},\n",
       " 'inception_v4': {'fp32': 35.37125825881958, 'top1': 86.169, 'imsize': 299},\n",
       " 'rexnet_150': {'fp32': 15.330533981323242, 'top1': 86.154, 'imsize': 224},\n",
       " 'inception_resnet_v2': {'fp32': 57.305214405059814,\n",
       "  'top1': 86.133,\n",
       "  'imsize': 299},\n",
       " 'ssl_resnext50_32x4d': {'fp32': 23.490662574768066,\n",
       "  'top1': 86.086,\n",
       "  'imsize': 224},\n",
       " 'tf_efficientnet_el': {'fp32': 14.638526439666748,\n",
       "  'top1': 86.084,\n",
       "  'imsize': 300},\n",
       " 'gluon_resnet101_v1s': {'fp32': 24.847488403320312,\n",
       "  'top1': 86.054,\n",
       "  'imsize': 224},\n",
       " 'ecaresnetlight': {'fp32': 16.167688369750977, 'top1': 86.052, 'imsize': 224},\n",
       " 'gluon_seresnext101_32x4d': {'fp32': 58.75911235809326,\n",
       "  'top1': 86.03200000000001,\n",
       "  'imsize': 224},\n",
       " 'resnet50d': {'fp32': 13.711214065551758, 'top1': 86.009, 'imsize': 224},\n",
       " 'ecaresnet26t': {'fp32': 9.866526126861572,\n",
       "  'top1': 85.98299999999999,\n",
       "  'imsize': 320},\n",
       " 'tf_efficientnet_b2_ap': {'fp32': 22.07948923110962,\n",
       "  'top1': 85.975,\n",
       "  'imsize': 260},\n",
       " 'gluon_seresnext101_64x4d': {'fp32': 80.4191780090332,\n",
       "  'top1': 85.96,\n",
       "  'imsize': 224},\n",
       " 'vit_base_patch32_224': {'fp32': 6.389458179473877,\n",
       "  'top1': 85.956,\n",
       "  'imsize': 224},\n",
       " 'gluon_resnet152_v1d': {'fp32': 36.32312059402466,\n",
       "  'top1': 85.917,\n",
       "  'imsize': 224},\n",
       " 'vit_large_patch32_384': {'fp32': 20.15129327774048,\n",
       "  'top1': 85.90899999999999,\n",
       "  'imsize': 384},\n",
       " 'tf_efficientnet_b2': {'fp32': 21.905653476715088,\n",
       "  'top1': 85.902,\n",
       "  'imsize': 260},\n",
       " 'tf_efficientnetv2_b2': {'fp32': 22.837042808532715,\n",
       "  'top1': 85.9,\n",
       "  'imsize': 260},\n",
       " 'seresnet50': {'fp32': 19.616737365722656,\n",
       "  'top1': 85.85700000000001,\n",
       "  'imsize': 224},\n",
       " 'repvgg_b2g4': {'fp32': 23.08713436126709, 'top1': 85.855, 'imsize': 224},\n",
       " 'gluon_resnet101_v1d': {'fp32': 25.16246795654297,\n",
       "  'top1': 85.84899999999999,\n",
       "  'imsize': 224},\n",
       " 'resnet50': {'fp32': 13.282124996185303, 'top1': 85.804, 'imsize': 224},\n",
       " 'mixnet_xl': {'fp32': 30.40233612060547,\n",
       "  'top1': 85.79799999999999,\n",
       "  'imsize': 224},\n",
       " 'ens_adv_inception_resnet_v2': {'fp32': 57.010860443115234,\n",
       "  'top1': 85.781,\n",
       "  'imsize': 299},\n",
       " 'tf_efficientnet_lite3': {'fp32': 13.321521282196045,\n",
       "  'top1': 85.755,\n",
       "  'imsize': 300},\n",
       " 'ese_vovnet39b': {'fp32': 14.513218402862549, 'top1': 85.751, 'imsize': 224},\n",
       " 'gluon_resnext101_32x4d': {'fp32': 45.24773359298706,\n",
       "  'top1': 85.74600000000001,\n",
       "  'imsize': 224},\n",
       " 'legacy_seresnext101_32x4d': {'fp32': 58.8041090965271,\n",
       "  'top1': 85.74600000000001,\n",
       "  'imsize': 224},\n",
       " 'cspresnext50': {'fp32': 24.183776378631592, 'top1': 85.74, 'imsize': 224},\n",
       " 'regnety_320': {'fp32': 60.277042388916016, 'top1': 85.727, 'imsize': 224},\n",
       " 'cspresnet50': {'fp32': 14.007270336151123, 'top1': 85.721, 'imsize': 256},\n",
       " 'xception71': {'fp32': 23.552582263946533, 'top1': 85.697, 'imsize': 299},\n",
       " 'resmlp_big_24_224': {'fp32': 36.26847505569458,\n",
       "  'top1': 85.695,\n",
       "  'imsize': 224},\n",
       " 'gluon_resnext101_64x4d': {'fp32': 72.80056715011597,\n",
       "  'top1': 85.693,\n",
       "  'imsize': 224},\n",
       " 'efficientnet_em': {'fp32': 12.492294311523438,\n",
       "  'top1': 85.684,\n",
       "  'imsize': 240},\n",
       " 'deit_small_patch16_224': {'fp32': 6.665711402893066,\n",
       "  'top1': 85.678,\n",
       "  'imsize': 224},\n",
       " 'pit_xs_distilled_224': {'fp32': 13.45930814743042,\n",
       "  'top1': 85.65700000000001,\n",
       "  'imsize': 224},\n",
       " 'efficientnet_b2_pruned': {'fp32': 21.822547912597656,\n",
       "  'top1': 85.64200000000001,\n",
       "  'imsize': 260},\n",
       " 'dpn107': {'fp32': 68.79962921142578, 'top1': 85.64, 'imsize': 224},\n",
       " 'resmlp_36_224': {'fp32': 10.581929683685303, 'top1': 85.62, 'imsize': 224},\n",
       " 'levit_192': {'fp32': 9.580020904541016, 'top1': 85.58, 'imsize': 224},\n",
       " 'gluon_resnet152_v1c': {'fp32': 36.30887985229492,\n",
       "  'top1': 85.58,\n",
       "  'imsize': 224},\n",
       " 'ecaresnet50d_pruned': {'fp32': 16.514463424682617,\n",
       "  'top1': 85.58,\n",
       "  'imsize': 224},\n",
       " 'resnext50d_32x4d': {'fp32': 23.969454765319824,\n",
       "  'top1': 85.569,\n",
       "  'imsize': 224},\n",
       " 'tf_efficientnetv2_b1': {'fp32': 21.87920331954956,\n",
       "  'top1': 85.561,\n",
       "  'imsize': 240},\n",
       " 'regnety_120': {'fp32': 34.869704246520996,\n",
       "  'top1': 85.54299999999999,\n",
       "  'imsize': 224},\n",
       " 'regnetx_320': {'fp32': 58.58788251876831, 'top1': 85.524, 'imsize': 224},\n",
       " 'nf_regnet_b1': {'fp32': 46.61731481552124, 'top1': 85.505, 'imsize': 288},\n",
       " 'dpn92': {'fp32': 43.35390567779541, 'top1': 85.494, 'imsize': 224},\n",
       " 'gluon_resnet152_v1b': {'fp32': 35.709426403045654,\n",
       "  'top1': 85.475,\n",
       "  'imsize': 224},\n",
       " 'rexnet_130': {'fp32': 15.473835468292236, 'top1': 85.473, 'imsize': 224},\n",
       " 'resnetrs50': {'fp32': 20.467588901519775, 'top1': 85.462, 'imsize': 224},\n",
       " 'dpn131': {'fp32': 68.6958646774292, 'top1': 85.398, 'imsize': 224},\n",
       " 'regnetx_160': {'fp32': 37.68540143966675, 'top1': 85.39, 'imsize': 224},\n",
       " 'dla102x2': {'fp32': 62.18226909637451, 'top1': 85.366, 'imsize': 224},\n",
       " 'gluon_seresnext50_32x4d': {'fp32': 30.013272762298584,\n",
       "  'top1': 85.336,\n",
       "  'imsize': 224},\n",
       " 'xception65': {'fp32': 19.800798892974854, 'top1': 85.315, 'imsize': 299},\n",
       " 'skresnext50_32x4d': {'fp32': 35.61493158340454,\n",
       "  'top1': 85.31299999999999,\n",
       "  'imsize': 224},\n",
       " 'dpn98': {'fp32': 52.345054149627686, 'top1': 85.311, 'imsize': 224},\n",
       " 'gluon_resnet101_v1c': {'fp32': 24.925501346588135,\n",
       "  'top1': 85.304,\n",
       "  'imsize': 224},\n",
       " 'dpn68b': {'fp32': 35.77160835266113, 'top1': 85.291, 'imsize': 224},\n",
       " 'regnety_064': {'fp32': 34.27822828292847,\n",
       "  'top1': 85.28299999999999,\n",
       "  'imsize': 224},\n",
       " 'resnetblur50': {'fp32': 13.456873893737793,\n",
       "  'top1': 85.28299999999999,\n",
       "  'imsize': 224},\n",
       " 'resmlp_24_224': {'fp32': 7.339167594909668,\n",
       "  'top1': 85.26799999999999,\n",
       "  'imsize': 224},\n",
       " 'coat_lite_mini': {'fp32': 11.493215560913086, 'top1': 85.251, 'imsize': 224},\n",
       " 'regnety_080': {'fp32': 30.399339199066162, 'top1': 85.245, 'imsize': 224},\n",
       " 'cait_xxs24_224': {'fp32': 17.500200271606445, 'top1': 85.228, 'imsize': 224},\n",
       " 'resnext50_32x4d': {'fp32': 23.546035289764404,\n",
       "  'top1': 85.221,\n",
       "  'imsize': 224},\n",
       " 'resnext101_32x8d': {'fp32': 63.384528160095215,\n",
       "  'top1': 85.18700000000001,\n",
       "  'imsize': 224},\n",
       " 'gluon_inception_v3': {'fp32': 22.422194480895996,\n",
       "  'top1': 85.18299999999999,\n",
       "  'imsize': 299},\n",
       " 'hrnet_w48': {'fp32': 78.23988437652588,\n",
       "  'top1': 85.15100000000001,\n",
       "  'imsize': 224},\n",
       " 'gluon_xception65': {'fp32': 19.773759841918945,\n",
       "  'top1': 85.148,\n",
       "  'imsize': 299},\n",
       " 'gluon_resnet101_v1b': {'fp32': 24.857568740844727,\n",
       "  'top1': 85.14200000000001,\n",
       "  'imsize': 224},\n",
       " 'regnetx_120': {'fp32': 31.976091861724854, 'top1': 85.131, 'imsize': 224},\n",
       " 'xception': {'fp32': 11.336536407470703,\n",
       "  'top1': 85.12899999999999,\n",
       "  'imsize': 299},\n",
       " 'tf_efficientnet_b1_ap': {'fp32': 21.77605628967285,\n",
       "  'top1': 85.12700000000001,\n",
       "  'imsize': 240},\n",
       " 'hrnet_w64': {'fp32': 78.03293704986572, 'top1': 85.119, 'imsize': 224},\n",
       " 'ssl_resnet50': {'fp32': 13.15248966217041,\n",
       "  'top1': 85.09700000000001,\n",
       "  'imsize': 224},\n",
       " 'res2net101_26w_4s': {'fp32': 42.26217985153198,\n",
       "  'top1': 85.09299999999999,\n",
       "  'imsize': 224},\n",
       " 'tf_efficientnet_cc_b1_8e': {'fp32': 24.27849054336548,\n",
       "  'top1': 85.06299999999999,\n",
       "  'imsize': 240},\n",
       " 'res2net50_26w_8s': {'fp32': 37.63259172439575,\n",
       "  'top1': 85.029,\n",
       "  'imsize': 224},\n",
       " 'resnest26d': {'fp32': 12.774097919464111, 'top1': 85.008, 'imsize': 224},\n",
       " 'gluon_resnext50_32x4d': {'fp32': 23.45468759536743,\n",
       "  'top1': 84.995,\n",
       "  'imsize': 224},\n",
       " 'tf_efficientnet_b0_ns': {'fp32': 15.619382858276367,\n",
       "  'top1': 84.984,\n",
       "  'imsize': 224},\n",
       " 'coat_tiny': {'fp32': 35.55366277694702, 'top1': 84.976, 'imsize': 224},\n",
       " 'regnety_040': {'fp32': 29.794514179229736, 'top1': 84.948, 'imsize': 224},\n",
       " 'dla169': {'fp32': 39.723639488220215, 'top1': 84.92, 'imsize': 224},\n",
       " 'tf_efficientnet_b1': {'fp32': 21.92162036895752,\n",
       "  'top1': 84.91799999999999,\n",
       "  'imsize': 240},\n",
       " 'legacy_seresnext50_32x4d': {'fp32': 29.979007244110107,\n",
       "  'top1': 84.90100000000001,\n",
       "  'imsize': 224},\n",
       " 'hrnet_w44': {'fp32': 78.14115762710571, 'top1': 84.884, 'imsize': 224},\n",
       " 'gluon_resnet50_v1s': {'fp32': 13.584201335906982,\n",
       "  'top1': 84.86200000000001,\n",
       "  'imsize': 224},\n",
       " 'regnetx_080': {'fp32': 28.918519020080566,\n",
       "  'top1': 84.86200000000001,\n",
       "  'imsize': 224},\n",
       " 'levit_128': {'fp32': 9.593560695648193,\n",
       "  'top1': 84.84299999999999,\n",
       "  'imsize': 224},\n",
       " 'gluon_resnet50_v1d': {'fp32': 13.726880550384521,\n",
       "  'top1': 84.83200000000001,\n",
       "  'imsize': 224},\n",
       " 'dla60_res2next': {'fp32': 30.87926149368286, 'top1': 84.83, 'imsize': 224},\n",
       " 'vit_tiny_patch16_384': {'fp32': 7.711431980133057,\n",
       "  'top1': 84.82799999999999,\n",
       "  'imsize': 384},\n",
       " 'mixnet_l': {'fp32': 25.013890266418457, 'top1': 84.822, 'imsize': 224},\n",
       " 'tv_resnet152': {'fp32': 35.722551345825195, 'top1': 84.815, 'imsize': 224},\n",
       " 'dla60_res2net': {'fp32': 24.49625015258789,\n",
       "  'top1': 84.81299999999999,\n",
       "  'imsize': 224},\n",
       " 'dla102x': {'fp32': 42.59997367858887,\n",
       "  'top1': 84.81299999999999,\n",
       "  'imsize': 224},\n",
       " 'pit_xs_224': {'fp32': 13.426010608673096, 'top1': 84.792, 'imsize': 224},\n",
       " 'xception41': {'fp32': 13.454711437225342, 'top1': 84.792, 'imsize': 299},\n",
       " 'regnetx_064': {'fp32': 23.596620559692383, 'top1': 84.781, 'imsize': 224},\n",
       " 'hrnet_w40': {'fp32': 78.1423830986023, 'top1': 84.743, 'imsize': 224},\n",
       " 'res2net50_26w_6s': {'fp32': 29.89434003829956,\n",
       "  'top1': 84.726,\n",
       "  'imsize': 224},\n",
       " 'repvgg_b2': {'fp32': 23.297388553619385,\n",
       "  'top1': 84.72399999999999,\n",
       "  'imsize': 224},\n",
       " 'resmlp_12_distilled_224': {'fp32': 4.392945766448975,\n",
       "  'top1': 84.713,\n",
       "  'imsize': 224},\n",
       " 'legacy_seresnet152': {'fp32': 56.50923490524292,\n",
       "  'top1': 84.704,\n",
       "  'imsize': 224},\n",
       " 'selecsls60b': {'fp32': 15.830533504486084,\n",
       "  'top1': 84.65700000000001,\n",
       "  'imsize': 224},\n",
       " 'hrnet_w32': {'fp32': 78.29899787902832,\n",
       "  'top1': 84.65100000000001,\n",
       "  'imsize': 224},\n",
       " 'tf_efficientnetv2_b0': {'fp32': 17.900583744049072,\n",
       "  'top1': 84.625,\n",
       "  'imsize': 224},\n",
       " 'efficientnet_b1': {'fp32': 21.907551288604736,\n",
       "  'top1': 84.60799999999999,\n",
       "  'imsize': 256},\n",
       " 'regnetx_040': {'fp32': 24.754343032836914, 'top1': 84.6, 'imsize': 224},\n",
       " 'efficientnet_es': {'fp32': 9.546875953674316,\n",
       "  'top1': 84.59100000000001,\n",
       "  'imsize': 224},\n",
       " 'hrnet_w30': {'fp32': 78.08218955993652, 'top1': 84.572, 'imsize': 224},\n",
       " 'tf_mixnet_l': {'fp32': 24.884541034698486, 'top1': 84.564, 'imsize': 224},\n",
       " 'wide_resnet101_2': {'fp32': 31.614665985107422,\n",
       "  'top1': 84.557,\n",
       "  'imsize': 224},\n",
       " 'dla60x': {'fp32': 25.543386936187744, 'top1': 84.523, 'imsize': 224},\n",
       " 'legacy_seresnet101': {'fp32': 37.91924476623535,\n",
       "  'top1': 84.50399999999999,\n",
       "  'imsize': 224},\n",
       " 'tf_efficientnet_em': {'fp32': 12.597429752349854,\n",
       "  'top1': 84.45,\n",
       "  'imsize': 240},\n",
       " 'coat_lite_tiny': {'fp32': 11.528496742248535, 'top1': 84.45, 'imsize': 224},\n",
       " 'repvgg_b1': {'fp32': 17.975382804870605, 'top1': 84.416, 'imsize': 224},\n",
       " 'efficientnet_b1_pruned': {'fp32': 21.74143075942993,\n",
       "  'top1': 84.39299999999999,\n",
       "  'imsize': 240},\n",
       " 'res2net50_26w_4s': {'fp32': 21.786844730377197,\n",
       "  'top1': 84.365,\n",
       "  'imsize': 224},\n",
       " 'hardcorenas_f': {'fp32': 16.0121488571167,\n",
       "  'top1': 84.32600000000001,\n",
       "  'imsize': 224},\n",
       " 'res2net50_14w_8s': {'fp32': 37.401931285858154,\n",
       "  'top1': 84.309,\n",
       "  'imsize': 224},\n",
       " 'selecsls60': {'fp32': 15.870141983032227, 'top1': 84.288, 'imsize': 224},\n",
       " 'regnetx_032': {'fp32': 23.303406238555908,\n",
       "  'top1': 84.23700000000001,\n",
       "  'imsize': 224},\n",
       " 'res2next50': {'fp32': 28.005220890045166, 'top1': 84.226, 'imsize': 224},\n",
       " 'gluon_resnet50_v1c': {'fp32': 13.912467956542969,\n",
       "  'top1': 84.20700000000001,\n",
       "  'imsize': 224},\n",
       " 'dla102': {'fp32': 25.287036895751953, 'top1': 84.19, 'imsize': 224},\n",
       " 'rexnet_100': {'fp32': 15.513498783111572, 'top1': 84.162, 'imsize': 224},\n",
       " 'tf_inception_v3': {'fp32': 22.589099407196045,\n",
       "  'top1': 84.132,\n",
       "  'imsize': 299},\n",
       " 'res2net50_48w_2s': {'fp32': 13.783876895904541,\n",
       "  'top1': 84.126,\n",
       "  'imsize': 224},\n",
       " 'resnet34d': {'fp32': 12.148823738098145, 'top1': 84.098, 'imsize': 224},\n",
       " 'tf_efficientnet_lite2': {'fp32': 11.678578853607178,\n",
       "  'top1': 84.094,\n",
       "  'imsize': 260},\n",
       " 'efficientnet_b0': {'fp32': 15.488321781158447,\n",
       "  'top1': 84.038,\n",
       "  'imsize': 224},\n",
       " 'gmixer_24_224': {'fp32': 10.4020357131958,\n",
       "  'top1': 83.96799999999999,\n",
       "  'imsize': 224},\n",
       " 'hardcorenas_e': {'fp32': 16.024208068847656,\n",
       "  'top1': 83.96799999999999,\n",
       "  'imsize': 224},\n",
       " 'tf_efficientnet_cc_b0_8e': {'fp32': 17.45192289352417,\n",
       "  'top1': 83.96600000000001,\n",
       "  'imsize': 224},\n",
       " 'tv_resnext50_32x4d': {'fp32': 23.615386486053467,\n",
       "  'top1': 83.959,\n",
       "  'imsize': 224},\n",
       " 'regnety_016': {'fp32': 37.89597749710083, 'top1': 83.955, 'imsize': 224},\n",
       " 'gluon_resnet50_v1b': {'fp32': 13.205344676971436,\n",
       "  'top1': 83.94,\n",
       "  'imsize': 224},\n",
       " 'densenet161': {'fp32': 39.54745054244995, 'top1': 83.906, 'imsize': 224},\n",
       " 'adv_inception_v3': {'fp32': 22.53640651702881,\n",
       "  'top1': 83.902,\n",
       "  'imsize': 299},\n",
       " 'mobilenetv2_120d': {'fp32': 13.914711475372314,\n",
       "  'top1': 83.89299999999999,\n",
       "  'imsize': 224},\n",
       " 'seresnext26t_32x4d': {'fp32': 16.58782958984375,\n",
       "  'top1': 83.87799999999999,\n",
       "  'imsize': 224},\n",
       " 'tv_resnet101': {'fp32': 24.6561336517334, 'top1': 83.848, 'imsize': 224},\n",
       " 'inception_v3': {'fp32': 22.784764766693115,\n",
       "  'top1': 83.76100000000001,\n",
       "  'imsize': 299},\n",
       " 'hardcorenas_d': {'fp32': 17.058401107788086, 'top1': 83.759, 'imsize': 224},\n",
       " 'seresnext26d_32x4d': {'fp32': 16.565802097320557,\n",
       "  'top1': 83.75399999999999,\n",
       "  'imsize': 224},\n",
       " 'dla60': {'fp32': 15.73573112487793, 'top1': 83.729, 'imsize': 224},\n",
       " 'repvgg_b1g4': {'fp32': 19.077744483947754, 'top1': 83.699, 'imsize': 224},\n",
       " 'legacy_seresnet50': {'fp32': 19.75764513015747,\n",
       "  'top1': 83.662,\n",
       "  'imsize': 224},\n",
       " 'tf_efficientnet_b0_ap': {'fp32': 15.696699619293213,\n",
       "  'top1': 83.65,\n",
       "  'imsize': 224},\n",
       " 'skresnet34': {'fp32': 22.068228721618652, 'top1': 83.641, 'imsize': 224},\n",
       " 'tf_efficientnet_cc_b0_4e': {'fp32': 17.553105354309082,\n",
       "  'top1': 83.639,\n",
       "  'imsize': 224},\n",
       " 'resmlp_12_224': {'fp32': 4.048259258270264, 'top1': 83.571, 'imsize': 224},\n",
       " 'densenet201': {'fp32': 49.57787752151489, 'top1': 83.556, 'imsize': 224},\n",
       " 'mobilenetv3_large_100_miil': {'fp32': 11.783394813537598,\n",
       "  'top1': 83.556,\n",
       "  'imsize': 224},\n",
       " 'gernet_s': {'fp32': 10.78639268875122, 'top1': 83.522, 'imsize': 224},\n",
       " 'legacy_seresnext26_32x4d': {'fp32': 15.90550422668457,\n",
       "  'top1': 83.51700000000001,\n",
       "  'imsize': 224},\n",
       " 'mixnet_m': {'fp32': 24.742238521575928, 'top1': 83.515, 'imsize': 224},\n",
       " 'tf_efficientnet_b0': {'fp32': 15.570008754730225,\n",
       "  'top1': 83.515,\n",
       "  'imsize': 224},\n",
       " 'hrnet_w18': {'fp32': 78.14085006713867, 'top1': 83.5, 'imsize': 224},\n",
       " 'densenetblur121d': {'fp32': 30.081913471221924,\n",
       "  'top1': 83.47200000000001,\n",
       "  'imsize': 224},\n",
       " 'selecsls42b': {'fp32': 12.409875392913818,\n",
       "  'top1': 83.45700000000001,\n",
       "  'imsize': 224},\n",
       " 'tf_efficientnet_lite1': {'fp32': 11.537988185882568,\n",
       "  'top1': 83.344,\n",
       "  'imsize': 240},\n",
       " 'hardcorenas_c': {'fp32': 12.615303993225098,\n",
       "  'top1': 83.34200000000001,\n",
       "  'imsize': 224},\n",
       " 'regnetx_016': {'fp32': 19.429233074188232, 'top1': 83.195, 'imsize': 224},\n",
       " 'mobilenetv2_140': {'fp32': 9.3467116355896, 'top1': 83.182, 'imsize': 224},\n",
       " 'dpn68': {'fp32': 32.70716190338135, 'top1': 83.178, 'imsize': 224},\n",
       " 'tf_efficientnet_es': {'fp32': 9.875457286834717,\n",
       "  'top1': 83.178,\n",
       "  'imsize': 224},\n",
       " 'tf_mixnet_m': {'fp32': 25.16129493713379, 'top1': 83.176, 'imsize': 224},\n",
       " 'ese_vovnet19b_dw': {'fp32': 7.285418510437012,\n",
       "  'top1': 83.109,\n",
       "  'imsize': 224},\n",
       " 'levit_128s': {'fp32': 7.949166297912598, 'top1': 83.069, 'imsize': 224},\n",
       " 'resnet26d': {'fp32': 8.064417839050293, 'top1': 83.05, 'imsize': 224},\n",
       " 'repvgg_a2': {'fp32': 12.027881145477295, 'top1': 83.001, 'imsize': 224},\n",
       " 'tv_resnet50': {'fp32': 13.208227157592773, 'top1': 82.958, 'imsize': 224},\n",
       " 'hardcorenas_b': {'fp32': 11.70189619064331,\n",
       "  'top1': 82.87299999999999,\n",
       "  'imsize': 224},\n",
       " 'densenet121': {'fp32': 29.414219856262207, 'top1': 82.823, 'imsize': 224},\n",
       " 'vit_tiny_r_s16_p8_384': {'fp32': 8.360729217529297,\n",
       "  'top1': 82.691,\n",
       "  'imsize': 384},\n",
       " 'densenet169': {'fp32': 41.47421360015869,\n",
       "  'top1': 82.68299999999999,\n",
       "  'imsize': 224},\n",
       " 'mixnet_s': {'fp32': 20.370938777923584, 'top1': 82.525, 'imsize': 224},\n",
       " 'vit_small_patch32_224': {'fp32': 6.572155952453613,\n",
       "  'top1': 82.514,\n",
       "  'imsize': 224},\n",
       " 'regnety_008': {'fp32': 22.462012767791748, 'top1': 82.493, 'imsize': 224},\n",
       " 'efficientnet_lite0': {'fp32': 8.851077556610107,\n",
       "  'top1': 82.382,\n",
       "  'imsize': 224},\n",
       " 'resnest14d': {'fp32': 7.5067853927612305,\n",
       "  'top1': 82.34899999999999,\n",
       "  'imsize': 224},\n",
       " 'hardcorenas_a': {'fp32': 9.79759693145752,\n",
       "  'top1': 82.31299999999999,\n",
       "  'imsize': 224},\n",
       " 'efficientnet_es_pruned': {'fp32': 9.677238464355469,\n",
       "  'top1': 82.296,\n",
       "  'imsize': 224},\n",
       " 'mobilenetv3_rw': {'fp32': 12.055420875549316, 'top1': 82.275, 'imsize': 224},\n",
       " 'semnasnet_100': {'fp32': 12.051868438720703, 'top1': 82.251, 'imsize': 224},\n",
       " 'mobilenetv3_large_100': {'fp32': 11.939566135406494,\n",
       "  'top1': 82.177,\n",
       "  'imsize': 224},\n",
       " 'resnet34': {'fp32': 11.482102870941162,\n",
       "  'top1': 82.13799999999999,\n",
       "  'imsize': 224},\n",
       " 'mobilenetv2_110d': {'fp32': 11.972315311431885,\n",
       "  'top1': 82.07,\n",
       "  'imsize': 224},\n",
       " 'vit_tiny_patch16_224': {'fp32': 6.647017002105713,\n",
       "  'top1': 82.066,\n",
       "  'imsize': 224},\n",
       " 'tf_mixnet_s': {'fp32': 20.821356773376465, 'top1': 82.038, 'imsize': 224},\n",
       " 'repvgg_b0': {'fp32': 14.782853126525879, 'top1': 82.001, 'imsize': 224},\n",
       " 'deit_tiny_distilled_patch16_224': {'fp32': 6.631865501403809,\n",
       "  'top1': 81.99700000000001,\n",
       "  'imsize': 224},\n",
       " 'mixer_b16_224': {'fp32': 5.857203006744385, 'top1': 81.978, 'imsize': 224},\n",
       " 'pit_ti_distilled_224': {'fp32': 11.387217044830322,\n",
       "  'top1': 81.96700000000001,\n",
       "  'imsize': 224},\n",
       " 'hrnet_w18_small_v2': {'fp32': 40.0214958190918,\n",
       "  'top1': 81.961,\n",
       "  'imsize': 224},\n",
       " 'tf_efficientnet_lite0': {'fp32': 9.183862209320068,\n",
       "  'top1': 81.95200000000001,\n",
       "  'imsize': 224},\n",
       " 'resnet26': {'fp32': 7.71432638168335, 'top1': 81.944, 'imsize': 224},\n",
       " 'tf_mobilenetv3_large_100': {'fp32': 12.049391269683838,\n",
       "  'top1': 81.848,\n",
       "  'imsize': 224},\n",
       " 'tv_densenet121': {'fp32': 29.645564556121826, 'top1': 81.726, 'imsize': 224},\n",
       " 'regnety_006': {'fp32': 23.649604320526123, 'top1': 81.7, 'imsize': 224},\n",
       " 'dla34': {'fp32': 10.816187858581543, 'top1': 81.658, 'imsize': 224},\n",
       " 'fbnetc_100': {'fp32': 11.311442852020264, 'top1': 81.559, 'imsize': 224},\n",
       " 'legacy_seresnet34': {'fp32': 16.80896759033203,\n",
       "  'top1': 81.53399999999999,\n",
       "  'imsize': 224},\n",
       " 'gluon_resnet34_v1b': {'fp32': 11.373028755187988,\n",
       "  'top1': 81.5,\n",
       "  'imsize': 224},\n",
       " 'regnetx_008': {'fp32': 19.651708602905273, 'top1': 81.485, 'imsize': 224},\n",
       " 'mnasnet_100': {'fp32': 9.475789070129395, 'top1': 81.459, 'imsize': 224},\n",
       " 'vgg19_bn': {'fp32': 21.40399694442749, 'top1': 81.444, 'imsize': 224},\n",
       " 'convit_tiny': {'fp32': 8.962047100067139, 'top1': 81.126, 'imsize': 224},\n",
       " 'spnasnet_100': {'fp32': 11.406304836273193, 'top1': 80.878, 'imsize': 224},\n",
       " 'ghostnet_100': {'fp32': 15.808005332946777, 'top1': 80.699, 'imsize': 224},\n",
       " 'regnety_004': {'fp32': 29.50500249862671,\n",
       "  'top1': 80.65899999999999,\n",
       "  'imsize': 224},\n",
       " 'skresnet18': {'fp32': 12.039923667907715, 'top1': 80.637, 'imsize': 224},\n",
       " 'regnetx_006': {'fp32': 16.601219177246094,\n",
       "  'top1': 80.62899999999999,\n",
       "  'imsize': 224},\n",
       " 'pit_ti_224': {'fp32': 11.367049217224121, 'top1': 80.605, 'imsize': 224},\n",
       " 'swsl_resnet18': {'fp32': 6.388416290283203, 'top1': 80.575, 'imsize': 224},\n",
       " 'vgg16_bn': {'fp32': 18.277535438537598, 'top1': 80.556, 'imsize': 224},\n",
       " 'tv_resnet34': {'fp32': 11.475884914398193, 'top1': 80.389, 'imsize': 224},\n",
       " 'resnet18d': {'fp32': 6.916868686676025, 'top1': 80.387, 'imsize': 224},\n",
       " 'mobilenetv2_100': {'fp32': 9.230058193206787, 'top1': 80.257, 'imsize': 224},\n",
       " 'ssl_resnet18': {'fp32': 6.415367126464844, 'top1': 80.101, 'imsize': 224},\n",
       " 'tf_mobilenetv3_large_075': {'fp32': 11.915311813354492,\n",
       "  'top1': 80.093,\n",
       "  'imsize': 224},\n",
       " 'deit_tiny_patch16_224': {'fp32': 6.59926176071167,\n",
       "  'top1': 80.018,\n",
       "  'imsize': 224},\n",
       " 'hrnet_w18_small': {'fp32': 22.430191040039062,\n",
       "  'top1': 79.557,\n",
       "  'imsize': 224},\n",
       " 'vgg19': {'fp32': 20.628080368041992, 'top1': 79.48, 'imsize': 224},\n",
       " 'regnetx_004': {'fp32': 23.761541843414307, 'top1': 79.435, 'imsize': 224},\n",
       " 'tf_mobilenetv3_large_minimal_100': {'fp32': 8.6152982711792,\n",
       "  'top1': 79.222,\n",
       "  'imsize': 224},\n",
       " 'legacy_seresnet18': {'fp32': 9.321844577789307,\n",
       "  'top1': 79.153,\n",
       "  'imsize': 224},\n",
       " 'vgg16': {'fp32': 17.57899045944214, 'top1': 79.038, 'imsize': 224},\n",
       " 'vgg13_bn': {'fp32': 15.178887844085693, 'top1': 79.006, 'imsize': 224},\n",
       " 'vit_tiny_r_s16_p8_224': {'fp32': 6.904339790344238,\n",
       "  'top1': 78.991,\n",
       "  'imsize': 224},\n",
       " 'gluon_resnet18_v1b': {'fp32': 6.424798965454102,\n",
       "  'top1': 78.372,\n",
       "  'imsize': 224},\n",
       " 'vgg11_bn': {'fp32': 11.495509147644043, 'top1': 77.926, 'imsize': 224},\n",
       " 'regnety_002': {'fp32': 23.84559392929077, 'top1': 77.405, 'imsize': 224},\n",
       " 'mixer_l16_224': {'fp32': 18.659563064575195, 'top1': 77.285, 'imsize': 224},\n",
       " 'resnet18': {'fp32': 6.515593528747559,\n",
       "  'top1': 77.27600000000001,\n",
       "  'imsize': 224},\n",
       " 'vgg13': {'fp32': 14.501135349273682, 'top1': 77.23, 'imsize': 224},\n",
       " 'vgg11': {'fp32': 11.102778911590576, 'top1': 76.384, 'imsize': 224},\n",
       " 'regnetx_002': {'fp32': 18.230838775634766, 'top1': 76.124, 'imsize': 224},\n",
       " 'dla60x_c': {'fp32': 24.40481662750244, 'top1': 75.637, 'imsize': 224},\n",
       " 'tf_mobilenetv3_small_100': {'fp32': 10.441474914550781,\n",
       "  'top1': 74.717,\n",
       "  'imsize': 224},\n",
       " 'dla46x_c': {'fp32': 18.950676918029785,\n",
       "  'top1': 73.64699999999999,\n",
       "  'imsize': 224},\n",
       " 'tf_mobilenetv3_small_075': {'fp32': 10.450973510742188,\n",
       "  'top1': 72.812,\n",
       "  'imsize': 224},\n",
       " 'dla46_c': {'fp32': 11.729588508605957, 'top1': 72.601, 'imsize': 224},\n",
       " 'tf_mobilenetv3_small_minimal_100': {'fp32': 6.815752983093262,\n",
       "  'top1': 70.111,\n",
       "  'imsize': 224}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modellist = df_models[\"model\"]\n",
    "benchmark = {}\n",
    "\n",
    "# inference float precision\n",
    "for i,modelname in tqdm(enumerate((modellist))):\n",
    "    imsize = int(df_models[df_models[\"model\"]==modelname][\"img_size\"])\n",
    "    try:\n",
    "        benchmark = inference_imsize(modelname, benchmark, imsize)\n",
    "    except:\n",
    "        print(\"pass {}\".format(modelname))\n",
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(benchmark).T\n",
    "df_results\n",
    "df_results.to_csv(\"results_fp32_imsizeall.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
