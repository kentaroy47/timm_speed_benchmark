{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "#import torch.utils.benchmark as benchmark\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.3dev0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "WARM_UP = 10\n",
    "BATCH_SIZE = 4\n",
    "NUM_TEST = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 31 09:25:06 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:09:00.0 Off |                  Off |\r\n",
      "| 31%   59C    P2    81W / 300W |   7386MiB / 48682MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1786      G   /usr/lib/xorg/Xorg                 23MiB |\r\n",
      "|    0   N/A  N/A      2123      G   /usr/bin/gnome-shell                8MiB |\r\n",
      "|    0   N/A  N/A     19016      C   .../pytorch_p38_2/bin/python     7351MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>top1</th>\n",
       "      <th>top1_err</th>\n",
       "      <th>top5</th>\n",
       "      <th>top5_err</th>\n",
       "      <th>param_count</th>\n",
       "      <th>img_size</th>\n",
       "      <th>crop_pct</th>\n",
       "      <th>interpolation</th>\n",
       "      <th>top1_diff</th>\n",
       "      <th>top5_diff</th>\n",
       "      <th>rank_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eva02_large_patch14_448.mim_m38m_ft_in22k_in1k</td>\n",
       "      <td>91.129</td>\n",
       "      <td>8.871</td>\n",
       "      <td>98.713</td>\n",
       "      <td>1.287</td>\n",
       "      <td>305.08</td>\n",
       "      <td>448</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bicubic</td>\n",
       "      <td>1.077</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eva_giant_patch14_336.clip_ft_in1k</td>\n",
       "      <td>91.058</td>\n",
       "      <td>8.942</td>\n",
       "      <td>98.602</td>\n",
       "      <td>1.399</td>\n",
       "      <td>1,013.01</td>\n",
       "      <td>336</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bicubic</td>\n",
       "      <td>1.592</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eva02_large_patch14_448.mim_in22k_ft_in22k_in1k</td>\n",
       "      <td>91.020</td>\n",
       "      <td>8.980</td>\n",
       "      <td>98.685</td>\n",
       "      <td>1.315</td>\n",
       "      <td>305.08</td>\n",
       "      <td>448</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bicubic</td>\n",
       "      <td>1.054</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eva_giant_patch14_560.m30m_ft_in22k_in1k</td>\n",
       "      <td>90.969</td>\n",
       "      <td>9.031</td>\n",
       "      <td>98.672</td>\n",
       "      <td>1.328</td>\n",
       "      <td>1,014.45</td>\n",
       "      <td>560</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bicubic</td>\n",
       "      <td>1.183</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eva02_large_patch14_448.mim_in22k_ft_in1k</td>\n",
       "      <td>90.922</td>\n",
       "      <td>9.078</td>\n",
       "      <td>98.683</td>\n",
       "      <td>1.317</td>\n",
       "      <td>305.08</td>\n",
       "      <td>448</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bicubic</td>\n",
       "      <td>1.298</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             model    top1  top1_err    top5  \\\n",
       "0   eva02_large_patch14_448.mim_m38m_ft_in22k_in1k  91.129     8.871  98.713   \n",
       "1               eva_giant_patch14_336.clip_ft_in1k  91.058     8.942  98.602   \n",
       "2  eva02_large_patch14_448.mim_in22k_ft_in22k_in1k  91.020     8.980  98.685   \n",
       "3         eva_giant_patch14_560.m30m_ft_in22k_in1k  90.969     9.031  98.672   \n",
       "4        eva02_large_patch14_448.mim_in22k_ft_in1k  90.922     9.078  98.683   \n",
       "\n",
       "   top5_err param_count  img_size  crop_pct interpolation  top1_diff  \\\n",
       "0     1.287      305.08       448       1.0       bicubic      1.077   \n",
       "1     1.399    1,013.01       336       1.0       bicubic      1.592   \n",
       "2     1.315      305.08       448       1.0       bicubic      1.054   \n",
       "3     1.328    1,014.45       560       1.0       bicubic      1.183   \n",
       "4     1.317      305.08       448       1.0       bicubic      1.298   \n",
       "\n",
       "   top5_diff  rank_diff  \n",
       "0     -0.335        0.0  \n",
       "1     -0.224        5.0  \n",
       "2     -0.327       -1.0  \n",
       "3     -0.320       -1.0  \n",
       "4     -0.267       -1.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_models = pd.read_csv(\"results-imagenet-real.csv\")\n",
    "# use models with img size 224\n",
    "modellist = df_models[df_models[\"img_size\"]==224][\"model\"]\n",
    "df_models.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "    def __init__(self,  length, imsize):\n",
    "        self.len = length\n",
    "        self.data = torch.randn( 3, imsize, imsize, length)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[:,:,:,index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "rand_loader = DataLoader(dataset=RandomDataset(BATCH_SIZE*(WARM_UP + NUM_TEST), 224),\n",
    "                         batch_size=BATCH_SIZE, shuffle=False,num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ryujaehun/pytorch-gpu-benchmark/blob/master/benchmark_models.py\n",
    "def inference(modelname, benchmark, half=False):\n",
    "    with torch.no_grad():\n",
    "        model = timm.create_model(modelname,)\n",
    "        model=model.to('cuda')\n",
    "        model.eval()\n",
    "        precision = \"float\"\n",
    "        durations = []\n",
    "        print(f'Benchmarking Inference {modelname} ')\n",
    "        for step,img in enumerate(rand_loader):\n",
    "            img=getattr(img,precision)()\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "            model(img.to('cuda'))\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            if step >= WARM_UP:\n",
    "                durations.append((end - start)*1000)\n",
    "        print(f'{modelname} model average inference time : {sum(durations)/len(durations)}ms')\n",
    "        \n",
    "        if half:\n",
    "            durations_half = []\n",
    "            print(f'Benchmarking Inference half precision type {modelname} ')\n",
    "            model.half()\n",
    "            precision = \"half\"\n",
    "            for step,img in enumerate(rand_loader):\n",
    "                img=getattr(img,precision)()\n",
    "                torch.cuda.synchronize()\n",
    "                start = time.time()\n",
    "                model(img.to('cuda'))\n",
    "                torch.cuda.synchronize()\n",
    "                end = time.time()\n",
    "                if step >= WARM_UP:\n",
    "                    durations_half.append((end - start)*1000)\n",
    "            print(f'{modelname} half model average inference time : {sum(durations_half)/len(durations_half)}ms')\n",
    "            \n",
    "        if half:\n",
    "            benchmark[modelname] = {\"fp32\": np.mean(durations), \"fp16\": np.mean(durations_half), \"top1\": df_models[df_models[\"model\"]==modelname][\"top1\"]}\n",
    "        else:\n",
    "            benchmark[modelname] = {\"fp32\": np.mean(durations), \"top1\": float(df_models[df_models[\"model\"]==modelname][\"top1\"])}\n",
    "    return benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58777dac746405ea5eefbc55cc50bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Inference eva_giant_patch14_224 \n",
      "eva_giant_patch14_224 model average inference time : 75.5289077758789ms\n",
      "pass eva_giant_patch14_224.clip_ft_in1k\n",
      "Benchmarking Inference caformer_b36 \n",
      "caformer_b36 model average inference time : 23.005459308624268ms\n",
      "pass caformer_b36.sail_in22k_ft_in1k\n",
      "Benchmarking Inference vit_huge_patch14_clip_224 \n",
      "vit_huge_patch14_clip_224 model average inference time : 52.99990177154541ms\n",
      "pass vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k\n",
      "Benchmarking Inference vit_large_patch14_clip_224 \n",
      "vit_large_patch14_clip_224 model average inference time : 26.639528274536133ms\n",
      "pass vit_large_patch14_clip_224.openai_ft_in12k_in1k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/envs/pytorch_p38_2/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Inference beitv2_large_patch16_224 \n",
      "beitv2_large_patch16_224 model average inference time : 22.335903644561768ms\n",
      "pass beitv2_large_patch16_224.in1k_ft_in22k_in1k\n",
      "Benchmarking Inference vit_large_patch14_clip_224 \n",
      "vit_large_patch14_clip_224 model average inference time : 26.682329177856445ms\n",
      "pass vit_large_patch14_clip_224.laion2b_ft_in12k_in1k\n",
      "Benchmarking Inference vit_large_patch14_clip_224 \n",
      "vit_large_patch14_clip_224 model average inference time : 26.756203174591064ms\n",
      "pass vit_large_patch14_clip_224.openai_ft_in1k\n",
      "Benchmarking Inference deit3_huge_patch14_224 \n",
      "deit3_huge_patch14_224 model average inference time : 54.3524956703186ms\n",
      "pass deit3_huge_patch14_224.fb_in22k_ft_in1k\n",
      "Benchmarking Inference convformer_b36 \n",
      "convformer_b36 model average inference time : 26.950178146362305ms\n",
      "pass convformer_b36.sail_in22k_ft_in1k\n",
      "Benchmarking Inference vit_huge_patch14_clip_224 \n",
      "vit_huge_patch14_clip_224 model average inference time : 53.39716672897339ms\n",
      "pass vit_huge_patch14_clip_224.laion2b_ft_in1k\n",
      "Benchmarking Inference beit_large_patch16_224 \n",
      "beit_large_patch16_224 model average inference time : 22.400729656219482ms\n",
      "pass beit_large_patch16_224.in22k_ft_in22k_in1k\n",
      "Benchmarking Inference beitv2_large_patch16_224 \n",
      "beitv2_large_patch16_224 model average inference time : 22.425596714019775ms\n",
      "pass beitv2_large_patch16_224.in1k_ft_in1k\n",
      "Benchmarking Inference vit_large_patch14_clip_224 \n",
      "vit_large_patch14_clip_224 model average inference time : 26.781320571899414ms\n",
      "pass vit_large_patch14_clip_224.laion2b_ft_in1k\n",
      "Benchmarking Inference deit3_large_patch16_224 \n",
      "deit3_large_patch16_224 model average inference time : 20.9485125541687ms\n",
      "pass deit3_large_patch16_224.fb_in22k_ft_in1k\n",
      "Benchmarking Inference maxvit_rmlp_base_rw_224 \n",
      "maxvit_rmlp_base_rw_224 model average inference time : 38.56623411178589ms\n",
      "pass maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k\n",
      "Benchmarking Inference volo_d5_224 \n",
      "volo_d5_224 model average inference time : 36.36343002319336ms\n",
      "pass volo_d5_224.sail_in1k\n",
      "Benchmarking Inference caformer_m36 \n",
      "caformer_m36 model average inference time : 17.392747402191162ms\n",
      "pass caformer_m36.sail_in22k_ft_in1k\n",
      "Benchmarking Inference coatnet_2_rw_224 \n",
      "coatnet_2_rw_224 model average inference time : 18.312788009643555ms\n",
      "pass coatnet_2_rw_224.sw_in12k_ft_in1k\n",
      "Benchmarking Inference convformer_m36 \n",
      "convformer_m36 model average inference time : 19.68834400177002ms\n",
      "pass convformer_m36.sail_in22k_ft_in1k\n",
      "Benchmarking Inference volo_d4_224 \n",
      "volo_d4_224 model average inference time : 24.817750453948975ms\n",
      "pass volo_d4_224.sail_in1k\n",
      "Benchmarking Inference swin_large_patch4_window7_224 \n",
      "swin_large_patch4_window7_224 model average inference time : 18.62790822982788ms\n",
      "pass swin_large_patch4_window7_224.ms_in22k_ft_in1k\n",
      "Benchmarking Inference coatnet_rmlp_2_rw_224 \n",
      "coatnet_rmlp_2_rw_224 model average inference time : 18.837239742279053ms\n",
      "pass coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k\n",
      "Benchmarking Inference maxxvitv2_rmlp_base_rw_224 \n",
      "maxxvitv2_rmlp_base_rw_224 model average inference time : 79.65704917907715ms\n",
      "pass maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k\n",
      "Benchmarking Inference beitv2_base_patch16_224 \n",
      "beitv2_base_patch16_224 model average inference time : 7.879650592803955ms\n",
      "pass beitv2_base_patch16_224.in1k_ft_in22k_in1k\n",
      "Benchmarking Inference vit_base_patch8_224 \n",
      "vit_base_patch8_224 model average inference time : 35.27453899383545ms\n",
      "pass vit_base_patch8_224.augreg2_in21k_ft_in1k\n",
      "Benchmarking Inference caformer_s36 \n",
      "caformer_s36 model average inference time : 16.104445457458496ms\n",
      "pass caformer_s36.sail_in22k_ft_in1k\n",
      "Benchmarking Inference vit_base_patch16_clip_224 \n",
      "vit_base_patch16_clip_224 model average inference time : 7.246983051300049ms\n",
      "pass vit_base_patch16_clip_224.laion2b_ft_in12k_in1k\n",
      "Benchmarking Inference volo_d3_224 \n",
      "volo_d3_224 model average inference time : 17.554616928100586ms\n",
      "pass volo_d3_224.sail_in1k\n",
      "Benchmarking Inference xcit_large_24_p8_224 \n",
      "xcit_large_24_p8_224 model average inference time : 69.85740423202515ms\n",
      "pass xcit_large_24_p8_224.fb_dist_in1k\n",
      "Benchmarking Inference convformer_s36 \n",
      "convformer_s36 model average inference time : 17.657172679901123ms\n",
      "pass convformer_s36.sail_in22k_ft_in1k\n",
      "Benchmarking Inference deit3_base_patch16_224 \n",
      "deit3_base_patch16_224 model average inference time : 7.428841590881348ms\n",
      "pass deit3_base_patch16_224.fb_in22k_ft_in1k\n",
      "Benchmarking Inference vit_base_patch16_224 \n",
      "vit_base_patch16_224 model average inference time : 7.2476959228515625ms\n",
      "pass vit_base_patch16_224.augreg2_in21k_ft_in1k\n",
      "Benchmarking Inference vit_base_patch8_224 \n",
      "vit_base_patch8_224 model average inference time : 35.30386924743652ms\n",
      "pass vit_base_patch8_224.augreg_in21k_ft_in1k\n",
      "Benchmarking Inference vit_base_patch16_clip_224 \n",
      "vit_base_patch16_clip_224 model average inference time : 7.257294654846191ms\n",
      "pass vit_base_patch16_clip_224.laion2b_ft_in1k\n",
      "Benchmarking Inference caformer_b36 \n",
      "caformer_b36 model average inference time : 23.26225996017456ms\n",
      "pass caformer_b36.sail_in1k\n",
      "Benchmarking Inference vit_base_patch16_clip_224 \n",
      "vit_base_patch16_clip_224 model average inference time : 7.238278388977051ms\n",
      "pass vit_base_patch16_clip_224.openai_ft_in12k_in1k\n",
      "Benchmarking Inference beit_base_patch16_224 \n",
      "beit_base_patch16_224 model average inference time : 7.894272804260254ms\n",
      "pass beit_base_patch16_224.in22k_ft_in22k_in1k\n",
      "Benchmarking Inference volo_d2_224 \n",
      "volo_d2_224 model average inference time : 12.490208148956299ms\n",
      "pass volo_d2_224.sail_in1k\n",
      "Benchmarking Inference vit_large_patch16_224 \n",
      "vit_large_patch16_224 model average inference time : 20.69392204284668ms\n",
      "pass vit_large_patch16_224.augreg_in21k_ft_in1k\n",
      "Benchmarking Inference xcit_medium_24_p8_224 \n",
      "xcit_medium_24_p8_224 model average inference time : 43.413941860198975ms\n",
      "pass xcit_medium_24_p8_224.fb_dist_in1k\n",
      "Benchmarking Inference beitv2_base_patch16_224 \n",
      "beitv2_base_patch16_224 model average inference time : 7.853231430053711ms\n",
      "pass beitv2_base_patch16_224.in1k_ft_in1k\n",
      "Benchmarking Inference deit3_huge_patch14_224 \n",
      "deit3_huge_patch14_224 model average inference time : 54.840030670166016ms\n",
      "pass deit3_huge_patch14_224.fb_in1k\n",
      "Benchmarking Inference xcit_small_24_p8_224 \n",
      "xcit_small_24_p8_224 model average inference time : 29.240095615386963ms\n",
      "pass xcit_small_24_p8_224.fb_dist_in1k\n",
      "Benchmarking Inference swin_base_patch4_window7_224 \n",
      "swin_base_patch4_window7_224 model average inference time : 12.823069095611572ms\n",
      "pass swin_base_patch4_window7_224.ms_in22k_ft_in1k\n",
      "Benchmarking Inference vit_base_patch16_clip_224 \n",
      "vit_base_patch16_clip_224 model average inference time : 7.285869121551514ms\n",
      "pass vit_base_patch16_clip_224.openai_ft_in1k\n",
      "Benchmarking Inference resnext101_32x32d \n",
      "resnext101_32x32d model average inference time : 67.7442717552185ms\n",
      "pass resnext101_32x32d.fb_wsl_ig1b_ft_in1k\n",
      "Benchmarking Inference xcit_large_24_p16_224 \n",
      "xcit_large_24_p16_224 model average inference time : 18.485641479492188ms\n",
      "pass xcit_large_24_p16_224.fb_dist_in1k\n",
      "Benchmarking Inference resmlp_big_24_224 \n",
      "resmlp_big_24_224 model average inference time : 34.099247455596924ms\n",
      "pass resmlp_big_24_224.fb_in22k_ft_in1k\n",
      "Benchmarking Inference xcit_small_12_p8_224 \n",
      "xcit_small_12_p8_224 model average inference time : 15.71432113647461ms\n",
      "pass xcit_small_12_p8_224.fb_dist_in1k\n",
      "Benchmarking Inference caformer_m36 \n",
      "caformer_m36 model average inference time : 17.337098121643066ms\n",
      "pass caformer_m36.sail_in1k\n",
      "Benchmarking Inference regnety_1280 \n",
      "regnety_1280 model average inference time : 70.88788986206055ms\n",
      "pass regnety_1280.swag_lc_in1k\n",
      "Benchmarking Inference mvitv2_large \n",
      "mvitv2_large model average inference time : 49.10564184188843ms\n",
      "pass mvitv2_large.fb_in1k\n",
      "Benchmarking Inference caformer_s18 \n",
      "caformer_s18 model average inference time : 8.434779644012451ms\n",
      "pass caformer_s18.sail_in22k_ft_in1k\n",
      "Benchmarking Inference deit3_medium_patch16_224 \n",
      "deit3_medium_patch16_224 model average inference time : 5.237462520599365ms\n",
      "pass deit3_medium_patch16_224.fb_in22k_ft_in1k\n",
      "Benchmarking Inference volo_d1_224 \n",
      "volo_d1_224 model average inference time : 8.763432502746582ms\n",
      "pass volo_d1_224.sail_in1k\n",
      "Benchmarking Inference vit_base_patch16_224 \n",
      "vit_base_patch16_224 model average inference time : 7.268095016479492ms\n",
      "pass vit_base_patch16_224.augreg_in21k_ft_in1k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Inference convformer_b36 \n",
      "convformer_b36 model average inference time : 27.112410068511963ms\n",
      "pass convformer_b36.sail_in1k\n",
      "Benchmarking Inference coatnet_rmlp_1_rw2_224 \n",
      "coatnet_rmlp_1_rw2_224 model average inference time : 15.502762794494629ms\n",
      "pass coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k\n",
      "Benchmarking Inference resnext101_32x16d \n",
      "resnext101_32x16d model average inference time : 30.904955863952637ms\n",
      "pass resnext101_32x16d.fb_wsl_ig1b_ft_in1k\n",
      "Benchmarking Inference swin_small_patch4_window7_224 \n",
      "swin_small_patch4_window7_224 model average inference time : 12.081964015960693ms\n",
      "pass swin_small_patch4_window7_224.ms_in22k_ft_in1k\n",
      "Benchmarking Inference xcit_medium_24_p16_224 \n",
      "xcit_medium_24_p16_224 model average inference time : 18.137314319610596ms\n",
      "pass xcit_medium_24_p16_224.fb_dist_in1k\n",
      "Benchmarking Inference convformer_m36 \n",
      "convformer_m36 model average inference time : 19.74210023880005ms\n",
      "pass convformer_m36.sail_in1k\n",
      "Benchmarking Inference resnext101_32x8d \n",
      "resnext101_32x8d model average inference time : 17.60477066040039ms\n",
      "pass resnext101_32x8d.fb_swsl_ig1b_ft_in1k\n",
      "Benchmarking Inference deit3_large_patch16_224 \n",
      "deit3_large_patch16_224 model average inference time : 21.115763187408447ms\n",
      "pass deit3_large_patch16_224.fb_in1k\n",
      "Benchmarking Inference caformer_s36 \n",
      "caformer_s36 model average inference time : 16.09084129333496ms\n",
      "pass caformer_s36.sail_in1k\n",
      "Benchmarking Inference vit_base_patch16_224_miil \n",
      "vit_base_patch16_224_miil model average inference time : 7.062649726867676ms\n",
      "pass vit_base_patch16_224_miil.in21k_ft_in1k\n",
      "Benchmarking Inference convformer_s18 \n",
      "convformer_s18 model average inference time : 9.551084041595459ms\n",
      "pass convformer_s18.sail_in22k_ft_in1k\n",
      "Benchmarking Inference davit_base \n",
      "davit_base model average inference time : 13.785758018493652ms\n",
      "pass davit_base.msft_in1k\n",
      "Benchmarking Inference mvitv2_base \n",
      "mvitv2_base model average inference time : 21.718857288360596ms\n",
      "pass mvitv2_base.fb_in1k\n",
      "Benchmarking Inference davit_small \n",
      "davit_small model average inference time : 13.619251251220703ms\n",
      "pass davit_small.msft_in1k\n",
      "Benchmarking Inference maxvit_base_tf_224 \n",
      "pass maxvit_base_tf_224.in1k\n",
      "Benchmarking Inference xcit_small_24_p16_224 \n",
      "xcit_small_24_p16_224 model average inference time : 17.683749198913574ms\n",
      "pass xcit_small_24_p16_224.fb_dist_in1k\n",
      "Benchmarking Inference maxvit_rmlp_small_rw_224 \n",
      "maxvit_rmlp_small_rw_224 model average inference time : 19.074690341949463ms\n",
      "pass maxvit_rmlp_small_rw_224.sw_in1k\n",
      "Benchmarking Inference coatnet_rmlp_2_rw_224 \n",
      "coatnet_rmlp_2_rw_224 model average inference time : 18.84667158126831ms\n",
      "pass coatnet_rmlp_2_rw_224.sw_in1k\n",
      "Benchmarking Inference efficientformerv2_l \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/envs/pytorch_p38_2/lib/python3.8/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientformerv2_l model average inference time : 19.756417274475098ms\n",
      "pass efficientformerv2_l.snap_dist_in1k\n",
      "Benchmarking Inference gcvit_base \n",
      "gcvit_base model average inference time : 18.81230592727661ms\n",
      "pass gcvit_base.in1k\n",
      "Benchmarking Inference convformer_s36 \n",
      "convformer_s36 model average inference time : 17.91667938232422ms\n",
      "pass convformer_s36.sail_in1k\n",
      "Benchmarking Inference maxvit_large_tf_224 \n",
      "pass maxvit_large_tf_224.in1k\n",
      "Benchmarking Inference maxvit_small_tf_224 \n",
      "pass maxvit_small_tf_224.in1k\n",
      "Benchmarking Inference cait_s24_224 \n",
      "cait_s24_224 model average inference time : 13.369269371032715ms\n",
      "pass cait_s24_224.fb_dist_in1k\n",
      "Benchmarking Inference resmlp_big_24_224 \n",
      "resmlp_big_24_224 model average inference time : 33.98709297180176ms\n",
      "pass resmlp_big_24_224.fb_distilled_in1k\n",
      "Benchmarking Inference vit_large_r50_s32_224 \n",
      "vit_large_r50_s32_224 model average inference time : 17.629268169403076ms\n",
      "pass vit_large_r50_s32_224.augreg_in21k_ft_in1k\n",
      "Benchmarking Inference regnety_320 \n",
      "regnety_320 model average inference time : 19.871926307678223ms\n",
      "pass regnety_320.swag_lc_in1k\n",
      "Benchmarking Inference tresnet_v2_l \n",
      "tresnet_v2_l model average inference time : 17.092092037200928ms\n",
      "pass tresnet_v2_l.miil_in21k_ft_in1k\n",
      "Benchmarking Inference deit3_small_patch16_224 \n",
      "deit3_small_patch16_224 model average inference time : 5.474185943603516ms\n",
      "pass deit3_small_patch16_224.fb_in22k_ft_in1k\n",
      "Benchmarking Inference pvt_v2_b4 \n",
      "pvt_v2_b4 model average inference time : 24.008197784423828ms\n",
      "pass pvt_v2_b4.in1k\n",
      "Benchmarking Inference efficientformer_l7 \n",
      "efficientformer_l7 model average inference time : 12.688689231872559ms\n",
      "pass efficientformer_l7.snap_dist_in1k\n",
      "Benchmarking Inference mvitv2_small \n",
      "mvitv2_small model average inference time : 14.743402004241943ms\n",
      "pass mvitv2_small.fb_in1k\n",
      "Benchmarking Inference deit3_base_patch16_224 \n",
      "deit3_base_patch16_224 model average inference time : 7.412879467010498ms\n",
      "pass deit3_base_patch16_224.fb_in1k\n",
      "Benchmarking Inference xcit_small_12_p16_224 \n",
      "xcit_small_12_p16_224 model average inference time : 10.355689525604248ms\n",
      "pass xcit_small_12_p16_224.fb_dist_in1k\n",
      "Benchmarking Inference gcvit_small \n",
      "gcvit_small model average inference time : 17.853262424468994ms\n",
      "pass gcvit_small.in1k\n",
      "Benchmarking Inference deit_base_distilled_patch16_224 \n",
      "deit_base_distilled_patch16_224 model average inference time : 7.204897403717041ms\n",
      "pass deit_base_distilled_patch16_224.fb_in1k\n",
      "Benchmarking Inference caformer_s18 \n",
      "caformer_s18 model average inference time : 8.994543552398682ms\n",
      "pass caformer_s18.sail_in1k\n",
      "Benchmarking Inference xcit_large_24_p8_224 \n",
      "xcit_large_24_p8_224 model average inference time : 69.63407754898071ms\n",
      "pass xcit_large_24_p8_224.fb_in1k\n",
      "Benchmarking Inference coat_lite_medium \n",
      "coat_lite_medium model average inference time : 38.2371187210083ms\n",
      "pass coat_lite_medium.in1k\n",
      "Benchmarking Inference resnext101_32x8d \n",
      "resnext101_32x8d model average inference time : 17.51880407333374ms\n",
      "pass resnext101_32x8d.fb_wsl_ig1b_ft_in1k\n",
      "Benchmarking Inference resnext101_32x4d \n",
      "resnext101_32x4d model average inference time : 12.882463932037354ms\n",
      "pass resnext101_32x4d.fb_swsl_ig1b_ft_in1k\n",
      "Benchmarking Inference pit_b_distilled_224 \n",
      "pit_b_distilled_224 model average inference time : 53.457884788513184ms\n",
      "pass pit_b_distilled_224.in1k\n",
      "Benchmarking Inference pvt_v2_b5 \n",
      "pvt_v2_b5 model average inference time : 29.73137617111206ms\n",
      "pass pvt_v2_b5.in1k\n",
      "Benchmarking Inference pvt_v2_b3 \n",
      "pvt_v2_b3 model average inference time : 16.032121181488037ms\n",
      "pass pvt_v2_b3.in1k\n",
      "Benchmarking Inference swin_s3_base_224 \n",
      "swin_s3_base_224 model average inference time : 16.459932327270508ms\n",
      "pass swin_s3_base_224.ms_in1k\n",
      "Benchmarking Inference xcit_tiny_24_p8_224 \n",
      "xcit_tiny_24_p8_224 model average inference time : 17.766826152801514ms\n",
      "pass xcit_tiny_24_p8_224.fb_dist_in1k\n",
      "Benchmarking Inference focalnet_base_srf \n",
      "focalnet_base_srf model average inference time : 18.190464973449707ms\n",
      "pass focalnet_base_srf.ms_in1k\n",
      "Benchmarking Inference regnety_160 \n",
      "regnety_160 model average inference time : 15.468273162841797ms\n",
      "pass regnety_160.swag_lc_in1k\n",
      "Benchmarking Inference maxvit_tiny_tf_224 \n",
      "pass maxvit_tiny_tf_224.in1k\n",
      "Benchmarking Inference focalnet_base_lrf \n",
      "focalnet_base_lrf model average inference time : 18.875608444213867ms\n",
      "pass focalnet_base_lrf.ms_in1k\n",
      "Benchmarking Inference gcvit_tiny \n",
      "gcvit_tiny model average inference time : 20.563273429870605ms\n",
      "pass gcvit_tiny.in1k\n",
      "Benchmarking Inference efficientformer_l3 \n",
      "efficientformer_l3 model average inference time : 9.176568984985352ms\n",
      "pass efficientformer_l3.snap_dist_in1k\n",
      "Benchmarking Inference xcit_small_24_p8_224 \n",
      "xcit_small_24_p8_224 model average inference time : 29.20827865600586ms\n",
      "pass xcit_small_24_p8_224.fb_in1k\n",
      "Benchmarking Inference coatnet_1_rw_224 \n",
      "coatnet_1_rw_224 model average inference time : 13.607902526855469ms\n",
      "pass coatnet_1_rw_224.sw_in1k\n",
      "Benchmarking Inference swinv2_cr_small_ns_224 \n",
      "swinv2_cr_small_ns_224 model average inference time : 15.484654903411865ms\n",
      "pass swinv2_cr_small_ns_224.sw_in1k\n",
      "Benchmarking Inference focalnet_small_lrf \n",
      "focalnet_small_lrf model average inference time : 17.656211853027344ms\n",
      "pass focalnet_small_lrf.ms_in1k\n",
      "Benchmarking Inference sequencer2d_l \n",
      "sequencer2d_l model average inference time : 38.274033069610596ms\n",
      "pass sequencer2d_l.in1k\n",
      "Benchmarking Inference twins_svt_large \n",
      "twins_svt_large model average inference time : 12.903759479522705ms\n",
      "pass twins_svt_large.in1k\n",
      "Benchmarking Inference coatnet_rmlp_1_rw_224 \n",
      "coatnet_rmlp_1_rw_224 model average inference time : 14.98382568359375ms\n",
      "pass coatnet_rmlp_1_rw_224.sw_in1k\n",
      "Benchmarking Inference swin_base_patch4_window7_224 \n",
      "swin_base_patch4_window7_224 model average inference time : 12.758164405822754ms\n",
      "pass swin_base_patch4_window7_224.ms_in1k\n",
      "Benchmarking Inference twins_pcpvt_large \n",
      "twins_pcpvt_large model average inference time : 21.229512691497803ms\n",
      "pass twins_pcpvt_large.in1k\n",
      "Benchmarking Inference maxvit_tiny_rw_224 \n",
      "maxvit_tiny_rw_224 model average inference time : 16.473543643951416ms\n",
      "pass maxvit_tiny_rw_224.sw_in1k\n",
      "Benchmarking Inference swin_s3_small_224 \n",
      "swin_s3_small_224 model average inference time : 11.70731782913208ms\n",
      "pass swin_s3_small_224.ms_in1k\n",
      "Benchmarking Inference convformer_s18 \n",
      "convformer_s18 model average inference time : 9.453809261322021ms\n",
      "pass convformer_s18.sail_in1k\n",
      "Benchmarking Inference xcit_small_12_p8_224 \n",
      "xcit_small_12_p8_224 model average inference time : 15.684394836425781ms\n",
      "pass xcit_small_12_p8_224.fb_in1k\n",
      "Benchmarking Inference vit_base_patch32_clip_224 \n",
      "vit_base_patch32_clip_224 model average inference time : 5.006778240203857ms\n",
      "pass vit_base_patch32_clip_224.laion2b_ft_in12k_in1k\n",
      "Benchmarking Inference focalnet_small_srf \n",
      "focalnet_small_srf model average inference time : 16.895194053649902ms\n",
      "pass focalnet_small_srf.ms_in1k\n",
      "Benchmarking Inference deit3_medium_patch16_224 \n",
      "deit3_medium_patch16_224 model average inference time : 5.520665645599365ms\n",
      "pass deit3_medium_patch16_224.fb_in1k\n",
      "Benchmarking Inference resnetv2_50x1_bit \n",
      "resnetv2_50x1_bit model average inference time : 8.855171203613281ms\n",
      "pass resnetv2_50x1_bit.goog_distilled_in1k\n",
      "Benchmarking Inference regnety_320 \n",
      "regnety_320 model average inference time : 19.84015464782715ms\n",
      "pass regnety_320.tv2_in1k\n",
      "Benchmarking Inference resnext101_64x4d \n",
      "resnext101_64x4d model average inference time : 17.731235027313232ms\n",
      "pass resnext101_64x4d.tv_in1k\n",
      "Benchmarking Inference twins_pcpvt_base \n",
      "twins_pcpvt_base model average inference time : 15.155246257781982ms\n",
      "pass twins_pcpvt_base.in1k\n",
      "Benchmarking Inference tresnet_m \n",
      "tresnet_m model average inference time : 10.109901428222656ms\n",
      "pass tresnet_m.miil_in21k_ft_in1k\n",
      "Benchmarking Inference mvitv2_tiny \n",
      "mvitv2_tiny model average inference time : 9.927849769592285ms\n",
      "pass mvitv2_tiny.fb_in1k\n",
      "Benchmarking Inference rexnet_300 \n",
      "rexnet_300 model average inference time : 8.381495475769043ms\n",
      "pass rexnet_300.nav_in1k\n",
      "Benchmarking Inference swin_small_patch4_window7_224 \n",
      "swin_small_patch4_window7_224 model average inference time : 12.17200756072998ms\n",
      "pass swin_small_patch4_window7_224.ms_in1k\n",
      "Benchmarking Inference twins_svt_base \n",
      "twins_svt_base model average inference time : 12.216055393218994ms\n",
      "pass twins_svt_base.in1k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Inference coat_small \n",
      "coat_small model average inference time : 20.749013423919678ms\n",
      "pass coat_small.in1k\n",
      "Benchmarking Inference xcit_medium_24_p8_224 \n",
      "xcit_medium_24_p8_224 model average inference time : 43.43877553939819ms\n",
      "pass xcit_medium_24_p8_224.fb_in1k\n",
      "Benchmarking Inference resnext101_32x16d \n",
      "resnext101_32x16d model average inference time : 30.86660385131836ms\n",
      "pass resnext101_32x16d.fb_swsl_ig1b_ft_in1k\n",
      "Benchmarking Inference resnext50_32x4d \n",
      "resnext50_32x4d model average inference time : 7.076573371887207ms\n",
      "pass resnext50_32x4d.fb_swsl_ig1b_ft_in1k\n",
      "Benchmarking Inference nest_base_jx \n",
      "nest_base_jx model average inference time : 13.149569034576416ms\n",
      "pass nest_base_jx.goog_in1k\n",
      "Benchmarking Inference davit_tiny \n",
      "davit_tiny model average inference time : 7.576360702514648ms\n",
      "pass davit_tiny.msft_in1k\n",
      "Benchmarking Inference levit_384 \n",
      "levit_384 model average inference time : 8.327574729919434ms\n",
      "pass levit_384.fb_dist_in1k\n",
      "Benchmarking Inference levit_conv_384 \n",
      "levit_conv_384 model average inference time : 7.637343406677246ms\n",
      "pass levit_conv_384.fb_dist_in1k\n",
      "Benchmarking Inference sequencer2d_m \n",
      "sequencer2d_m model average inference time : 25.729563236236572ms\n",
      "pass sequencer2d_m.in1k\n",
      "Benchmarking Inference regnetx_320 \n",
      "regnetx_320 model average inference time : 21.893250942230225ms\n",
      "pass regnetx_320.tv2_in1k\n",
      "Benchmarking Inference vit_base_patch32_clip_224 \n",
      "vit_base_patch32_clip_224 model average inference time : 5.296993255615234ms\n",
      "pass vit_base_patch32_clip_224.laion2b_ft_in1k\n",
      "Benchmarking Inference efficientformerv2_s2 \n",
      "efficientformerv2_s2 model average inference time : 15.283076763153076ms\n",
      "pass efficientformerv2_s2.snap_dist_in1k\n",
      "Benchmarking Inference coatnet_bn_0_rw_224 \n",
      "coatnet_bn_0_rw_224 model average inference time : 7.800500392913818ms\n",
      "pass coatnet_bn_0_rw_224.sw_in1k\n",
      "Benchmarking Inference vit_base_patch16_rpn_224 \n",
      "vit_base_patch16_rpn_224 model average inference time : 7.174005508422852ms\n",
      "pass vit_base_patch16_rpn_224.sw_in1k\n",
      "Benchmarking Inference pvt_v2_b2_li \n",
      "pvt_v2_b2_li model average inference time : 10.441102981567383ms\n",
      "pass pvt_v2_b2_li.in1k\n",
      "Benchmarking Inference resnetv2_152x2_bit \n",
      "resnetv2_152x2_bit model average inference time : 35.93396186828613ms\n",
      "pass resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k\n",
      "Benchmarking Inference nest_small_jx \n",
      "nest_small_jx model average inference time : 10.091023445129395ms\n",
      "pass nest_small_jx.goog_in1k\n",
      "Benchmarking Inference vit_relpos_base_patch16_clsgap_224 \n",
      "vit_relpos_base_patch16_clsgap_224 model average inference time : 7.9549241065979ms\n",
      "pass vit_relpos_base_patch16_clsgap_224.sw_in1k\n",
      "Benchmarking Inference vit_relpos_base_patch16_224 \n",
      "vit_relpos_base_patch16_224 model average inference time : 8.274726867675781ms\n",
      "pass vit_relpos_base_patch16_224.sw_in1k\n",
      "Benchmarking Inference regnety_080_tv \n",
      "regnety_080_tv model average inference time : 11.463110446929932ms\n",
      "pass regnety_080_tv.tv2_in1k\n",
      "Benchmarking Inference poolformerv2_m48 \n",
      "poolformerv2_m48 model average inference time : 24.71092462539673ms\n",
      "pass poolformerv2_m48.sail_in1k\n",
      "Benchmarking Inference regnetx_160 \n",
      "regnetx_160 model average inference time : 13.65922212600708ms\n",
      "pass regnetx_160.tv2_in1k\n",
      "Benchmarking Inference coat_lite_small \n",
      "coat_lite_small model average inference time : 12.236225605010986ms\n",
      "pass coat_lite_small.in1k\n",
      "Benchmarking Inference pvt_v2_b2 \n",
      "pvt_v2_b2 model average inference time : 9.887495040893555ms\n",
      "pass pvt_v2_b2.in1k\n",
      "Benchmarking Inference sequencer2d_s \n",
      "sequencer2d_s model average inference time : 19.986073970794678ms\n",
      "pass sequencer2d_s.in1k\n",
      "Benchmarking Inference swinv2_cr_small_224 \n",
      "swinv2_cr_small_224 model average inference time : 15.803987979888916ms\n",
      "pass swinv2_cr_small_224.sw_in1k\n",
      "Benchmarking Inference xcit_tiny_24_p8_224 \n",
      "xcit_tiny_24_p8_224 model average inference time : 17.82430410385132ms\n",
      "pass xcit_tiny_24_p8_224.fb_in1k\n",
      "Benchmarking Inference vit_relpos_medium_patch16_cls_224 \n",
      "vit_relpos_medium_patch16_cls_224 model average inference time : 7.076127529144287ms\n",
      "pass vit_relpos_medium_patch16_cls_224.sw_in1k\n",
      "Benchmarking Inference wide_resnet101_2 \n",
      "wide_resnet101_2 model average inference time : 12.8123140335083ms\n",
      "pass wide_resnet101_2.tv2_in1k\n",
      "Benchmarking Inference resnext101_32x8d \n",
      "resnext101_32x8d model average inference time : 17.50807285308838ms\n",
      "pass resnext101_32x8d.tv2_in1k\n",
      "Benchmarking Inference focalnet_tiny_srf \n",
      "focalnet_tiny_srf model average inference time : 9.40150260925293ms\n",
      "pass focalnet_tiny_srf.ms_in1k\n",
      "Benchmarking Inference gcvit_xtiny \n",
      "gcvit_xtiny model average inference time : 16.150410175323486ms\n",
      "pass gcvit_xtiny.in1k\n",
      "Benchmarking Inference coatnet_rmlp_nano_rw_224 \n",
      "coatnet_rmlp_nano_rw_224 model average inference time : 10.27590036392212ms\n",
      "pass coatnet_rmlp_nano_rw_224.sw_in1k\n",
      "Benchmarking Inference vit_relpos_medium_patch16_rpn_224 \n",
      "vit_relpos_medium_patch16_rpn_224 model average inference time : 7.072196006774902ms\n",
      "pass vit_relpos_medium_patch16_rpn_224.sw_in1k\n",
      "Benchmarking Inference swin_tiny_patch4_window7_224 \n",
      "swin_tiny_patch4_window7_224 model average inference time : 6.534159183502197ms\n",
      "pass swin_tiny_patch4_window7_224.ms_in22k_ft_in1k\n",
      "Benchmarking Inference coatnext_nano_rw_224 \n",
      "coatnext_nano_rw_224 model average inference time : 10.479648113250732ms\n",
      "pass coatnext_nano_rw_224.sw_in1k\n",
      "Benchmarking Inference poolformer_m48 \n",
      "poolformer_m48 model average inference time : 21.13490581512451ms\n",
      "pass poolformer_m48.sail_in1k\n",
      "Benchmarking Inference regnety_160 \n",
      "regnety_160 model average inference time : 15.460586547851562ms\n",
      "pass regnety_160.tv2_in1k\n",
      "Benchmarking Inference tresnet_xl \n",
      "tresnet_xl model average inference time : 17.683322429656982ms\n",
      "pass tresnet_xl.miil_in1k\n",
      "Benchmarking Inference mixer_b16_224 \n",
      "mixer_b16_224 model average inference time : 5.351617336273193ms\n",
      "pass mixer_b16_224.miil_in21k_ft_in1k\n",
      "Benchmarking Inference xcit_tiny_12_p8_224 \n",
      "xcit_tiny_12_p8_224 model average inference time : 10.174167156219482ms\n",
      "pass xcit_tiny_12_p8_224.fb_dist_in1k\n",
      "Benchmarking Inference convit_base \n",
      "convit_base model average inference time : 10.650010108947754ms\n",
      "pass convit_base.fb_in1k\n",
      "Benchmarking Inference resnet152 \n",
      "resnet152 model average inference time : 15.69202184677124ms\n",
      "pass resnet152.tv2_in1k\n",
      "Benchmarking Inference visformer_small \n",
      "visformer_small model average inference time : 7.366905212402344ms\n",
      "pass visformer_small.in1k\n",
      "Benchmarking Inference vit_base_patch32_clip_224 \n",
      "vit_base_patch32_clip_224 model average inference time : 5.253496170043945ms\n",
      "pass vit_base_patch32_clip_224.openai_ft_in1k\n",
      "Benchmarking Inference vit_srelpos_medium_patch16_224 \n",
      "vit_srelpos_medium_patch16_224 model average inference time : 5.926766395568848ms\n",
      "pass vit_srelpos_medium_patch16_224.sw_in1k\n",
      "Benchmarking Inference focalnet_tiny_lrf \n",
      "focalnet_tiny_lrf model average inference time : 9.339396953582764ms\n",
      "pass focalnet_tiny_lrf.ms_in1k\n",
      "Benchmarking Inference vit_relpos_medium_patch16_224 \n",
      "vit_relpos_medium_patch16_224 model average inference time : 7.230205535888672ms\n",
      "pass vit_relpos_medium_patch16_224.sw_in1k\n",
      "Benchmarking Inference swin_s3_tiny_224 \n",
      "swin_s3_tiny_224 model average inference time : 6.478888988494873ms\n",
      "pass swin_s3_tiny_224.ms_in1k\n",
      "Benchmarking Inference coatnet_0_rw_224 \n",
      "coatnet_0_rw_224 model average inference time : 8.961167335510254ms\n",
      "pass coatnet_0_rw_224.sw_in1k\n",
      "Benchmarking Inference xcit_small_24_p16_224 \n",
      "xcit_small_24_p16_224 model average inference time : 17.98541784286499ms\n",
      "pass xcit_small_24_p16_224.fb_in1k\n",
      "Benchmarking Inference pit_s_distilled_224 \n",
      "pit_s_distilled_224 model average inference time : 29.112670421600342ms\n",
      "pass pit_s_distilled_224.in1k\n",
      "Benchmarking Inference convit_small \n",
      "convit_small model average inference time : 6.918885707855225ms\n",
      "pass convit_small.fb_in1k\n",
      "Benchmarking Inference xcit_small_12_p16_224 \n",
      "xcit_small_12_p16_224 model average inference time : 10.460526943206787ms\n",
      "pass xcit_small_12_p16_224.fb_in1k\n",
      "Benchmarking Inference poolformerv2_m36 \n",
      "poolformerv2_m36 model average inference time : 18.81582736968994ms\n",
      "pass poolformerv2_m36.sail_in1k\n",
      "Benchmarking Inference nest_tiny_jx \n",
      "nest_tiny_jx model average inference time : 6.503782272338867ms\n",
      "pass nest_tiny_jx.goog_in1k\n",
      "Benchmarking Inference deit_small_distilled_patch16_224 \n",
      "deit_small_distilled_patch16_224 model average inference time : 5.361599922180176ms\n",
      "pass deit_small_distilled_patch16_224.fb_in1k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Inference deit3_small_patch16_224 \n",
      "deit3_small_patch16_224 model average inference time : 5.5252838134765625ms\n",
      "pass deit3_small_patch16_224.fb_in1k\n",
      "Benchmarking Inference swinv2_cr_tiny_ns_224 \n",
      "swinv2_cr_tiny_ns_224 model average inference time : 8.393325805664062ms\n",
      "pass swinv2_cr_tiny_ns_224.sw_in1k\n",
      "Benchmarking Inference coatnet_nano_rw_224 \n",
      "coatnet_nano_rw_224 model average inference time : 9.576084613800049ms\n",
      "pass coatnet_nano_rw_224.sw_in1k\n",
      "Benchmarking Inference resmlp_36_224 \n",
      "resmlp_36_224 model average inference time : 7.661442756652832ms\n",
      "pass resmlp_36_224.fb_distilled_in1k\n",
      "Benchmarking Inference poolformer_m36 \n",
      "poolformer_m36 model average inference time : 16.079864501953125ms\n",
      "pass poolformer_m36.sail_in1k\n",
      "Benchmarking Inference regnety_032 \n",
      "regnety_032 model average inference time : 12.137482166290283ms\n",
      "pass regnety_032.tv2_in1k\n",
      "Benchmarking Inference xcit_large_24_p16_224 \n",
      "xcit_large_24_p16_224 model average inference time : 18.75328540802002ms\n",
      "pass xcit_large_24_p16_224.fb_in1k\n",
      "Benchmarking Inference resnet101 \n",
      "resnet101 model average inference time : 10.618577003479004ms\n",
      "pass resnet101.tv2_in1k\n",
      "Benchmarking Inference xcit_medium_24_p16_224 \n",
      "xcit_medium_24_p16_224 model average inference time : 18.296871185302734ms\n",
      "pass xcit_medium_24_p16_224.fb_in1k\n",
      "Benchmarking Inference poolformerv2_s36 \n",
      "poolformerv2_s36 model average inference time : 13.616032600402832ms\n",
      "pass poolformerv2_s36.sail_in1k\n",
      "Benchmarking Inference tnt_s_patch16_224 \n",
      "tnt_s_patch16_224 model average inference time : 11.72980785369873ms\n",
      "Benchmarking Inference vit_relpos_small_patch16_224 \n",
      "vit_relpos_small_patch16_224 model average inference time : 7.23797082901001ms\n",
      "pass vit_relpos_small_patch16_224.sw_in1k\n",
      "Benchmarking Inference vit_small_patch16_224 \n",
      "vit_small_patch16_224 model average inference time : 5.2858710289001465ms\n",
      "pass vit_small_patch16_224.augreg_in21k_ft_in1k\n",
      "Benchmarking Inference vit_small_r26_s32_224 \n",
      "vit_small_r26_s32_224 model average inference time : 10.11115312576294ms\n",
      "pass vit_small_r26_s32_224.augreg_in21k_ft_in1k\n",
      "Benchmarking Inference resnext101_32x16d \n",
      "resnext101_32x16d model average inference time : 30.800271034240723ms\n",
      "pass resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k\n",
      "Benchmarking Inference rexnet_200 \n",
      "rexnet_200 model average inference time : 7.72705078125ms\n",
      "pass rexnet_200.nav_in1k\n",
      "Benchmarking Inference convmixer_1536_20 \n",
      "convmixer_1536_20 model average inference time : 41.25211715698242ms\n",
      "pass convmixer_1536_20.in1k\n",
      "Benchmarking Inference resnet50 \n",
      "resnet50 model average inference time : 5.685088634490967ms\n",
      "pass resnet50.fb_swsl_ig1b_ft_in1k\n",
      "Benchmarking Inference deit_base_patch16_224 \n",
      "deit_base_patch16_224 model average inference time : 7.189781665802002ms\n",
      "pass deit_base_patch16_224.fb_in1k\n",
      "Benchmarking Inference coat_mini \n",
      "coat_mini model average inference time : 93.39963912963867ms\n",
      "pass coat_mini.in1k\n",
      "Benchmarking Inference resnext101_32x8d \n",
      "resnext101_32x8d model average inference time : 17.52119779586792ms\n",
      "pass resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k\n",
      "Benchmarking Inference vit_base_patch16_224 \n",
      "vit_base_patch16_224 model average inference time : 7.283620834350586ms\n",
      "pass vit_base_patch16_224.orig_in21k_ft_in1k\n",
      "Benchmarking Inference regnetx_080 \n",
      "regnetx_080 model average inference time : 9.239521026611328ms\n",
      "pass regnetx_080.tv2_in1k\n",
      "Benchmarking Inference tresnet_l \n",
      "tresnet_l model average inference time : 14.439842700958252ms\n",
      "pass tresnet_l.miil_in1k\n",
      "Benchmarking Inference twins_svt_small \n",
      "twins_svt_small model average inference time : 9.280552864074707ms\n",
      "pass twins_svt_small.in1k\n",
      "Benchmarking Inference levit_256 \n",
      "levit_256 model average inference time : 7.897243499755859ms\n",
      "pass levit_256.fb_dist_in1k\n",
      "Benchmarking Inference levit_conv_256 \n",
      "levit_conv_256 model average inference time : 7.4030232429504395ms\n",
      "pass levit_conv_256.fb_dist_in1k\n",
      "Benchmarking Inference vit_srelpos_small_patch16_224 \n",
      "vit_srelpos_small_patch16_224 model average inference time : 5.89277982711792ms\n",
      "pass vit_srelpos_small_patch16_224.sw_in1k\n",
      "Benchmarking Inference pit_b_224 \n",
      "pit_b_224 model average inference time : 53.40660095214844ms\n",
      "pass pit_b_224.in1k\n",
      "Benchmarking Inference swin_tiny_patch4_window7_224 \n",
      "swin_tiny_patch4_window7_224 model average inference time : 6.675682067871094ms\n",
      "pass swin_tiny_patch4_window7_224.ms_in1k\n",
      "Benchmarking Inference poolformer_s36 \n",
      "poolformer_s36 model average inference time : 12.31935977935791ms\n",
      "pass poolformer_s36.sail_in1k\n",
      "Benchmarking Inference twins_pcpvt_small \n",
      "twins_pcpvt_small model average inference time : 9.221460819244385ms\n",
      "pass twins_pcpvt_small.in1k\n",
      "Benchmarking Inference resmlp_24_224 \n",
      "resmlp_24_224 model average inference time : 5.608100891113281ms\n",
      "pass resmlp_24_224.fb_distilled_in1k\n",
      "Benchmarking Inference resnest50d_4s2x40d \n",
      "resnest50d_4s2x40d model average inference time : 11.548285484313965ms\n",
      "pass resnest50d_4s2x40d.in1k\n",
      "Benchmarking Inference repvgg_b3 \n",
      "repvgg_b3 model average inference time : 12.174179553985596ms\n",
      "pass repvgg_b3.rvgg_in1k\n",
      "Benchmarking Inference wide_resnet50_2 \n",
      "wide_resnet50_2 model average inference time : 7.457034587860107ms\n",
      "pass wide_resnet50_2.tv2_in1k\n",
      "Benchmarking Inference xcit_tiny_24_p16_224 \n",
      "xcit_tiny_24_p16_224 model average inference time : 17.780892848968506ms\n",
      "pass xcit_tiny_24_p16_224.fb_dist_in1k\n",
      "Benchmarking Inference resnext101_32x4d \n",
      "resnext101_32x4d model average inference time : 13.077490329742432ms\n",
      "pass resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k\n",
      "Benchmarking Inference resnet152s \n",
      "resnet152s model average inference time : 15.719201564788818ms\n",
      "pass resnet152s.gluon_in1k\n",
      "Benchmarking Inference haloregnetz_b \n",
      "haloregnetz_b model average inference time : 13.17495346069336ms\n",
      "pass haloregnetz_b.ra3_in1k\n",
      "Benchmarking Inference resnet50 \n",
      "resnet50 model average inference time : 5.845239162445068ms\n",
      "pass resnet50.tv2_in1k\n",
      "Benchmarking Inference resnest50d_1s4x24d \n",
      "resnest50d_1s4x24d model average inference time : 9.84506607055664ms\n",
      "pass resnest50d_1s4x24d.in1k\n",
      "Benchmarking Inference poolformerv2_s24 \n",
      "poolformerv2_s24 model average inference time : 9.55894947052002ms\n",
      "pass poolformerv2_s24.sail_in1k\n",
      "Benchmarking Inference repvgg_b3g4 \n",
      "repvgg_b3g4 model average inference time : 10.38271427154541ms\n",
      "pass repvgg_b3g4.rvgg_in1k\n",
      "Benchmarking Inference regnety_016 \n",
      "regnety_016 model average inference time : 18.511033058166504ms\n",
      "pass regnety_016.tv2_in1k\n",
      "Benchmarking Inference efficientformer_l1 \n",
      "efficientformer_l1 model average inference time : 5.120739936828613ms\n",
      "pass efficientformer_l1.snap_dist_in1k\n",
      "Benchmarking Inference legacy_senet154 \n",
      "legacy_senet154 model average inference time : 30.156776905059814ms\n",
      "pass legacy_senet154.in1k\n",
      "Benchmarking Inference resnet50 \n",
      "resnet50 model average inference time : 6.247498989105225ms\n",
      "pass resnet50.a1h_in1k\n",
      "Benchmarking Inference cait_xxs36_224 \n",
      "cait_xxs36_224 model average inference time : 18.95817756652832ms\n",
      "pass cait_xxs36_224.fb_dist_in1k\n",
      "Benchmarking Inference gernet_m \n",
      "gernet_m model average inference time : 4.997880458831787ms\n",
      "pass gernet_m.idstcv_in1k\n",
      "Benchmarking Inference pit_s_224 \n",
      "pit_s_224 model average inference time : 29.069793224334717ms\n",
      "pass pit_s_224.in1k\n",
      "Benchmarking Inference senet154 \n",
      "senet154 model average inference time : 30.89747190475464ms\n",
      "pass senet154.gluon_in1k\n",
      "Benchmarking Inference resnext50_32x4d \n",
      "resnext50_32x4d model average inference time : 6.79152250289917ms\n",
      "pass resnext50_32x4d.tv2_in1k\n",
      "Benchmarking Inference resnest50d \n",
      "resnest50d model average inference time : 10.097014904022217ms\n",
      "pass resnest50d.in1k\n",
      "Benchmarking Inference gcvit_xxtiny \n",
      "gcvit_xxtiny model average inference time : 14.373500347137451ms\n",
      "pass gcvit_xxtiny.in1k\n",
      "Benchmarking Inference regnetx_032 \n",
      "regnetx_032 model average inference time : 10.313894748687744ms\n",
      "pass regnetx_032.tv2_in1k\n",
      "Benchmarking Inference convmixer_768_32 \n",
      "convmixer_768_32 model average inference time : 23.733277320861816ms\n",
      "pass convmixer_768_32.in1k\n",
      "Benchmarking Inference tresnet_m \n",
      "tresnet_m model average inference time : 10.088496208190918ms\n",
      "pass tresnet_m.miil_in1k\n",
      "Benchmarking Inference rexnet_150 \n",
      "rexnet_150 model average inference time : 7.941465377807617ms\n",
      "pass rexnet_150.nav_in1k\n",
      "Benchmarking Inference xcit_tiny_12_p8_224 \n",
      "xcit_tiny_12_p8_224 model average inference time : 10.348331928253174ms\n",
      "pass xcit_tiny_12_p8_224.fb_in1k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Inference resnext50_32x4d \n",
      "resnext50_32x4d model average inference time : 6.948068141937256ms\n",
      "pass resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k\n",
      "Benchmarking Inference res2net101d \n",
      "res2net101d model average inference time : 18.28681468963623ms\n",
      "pass res2net101d.in1k\n",
      "Benchmarking Inference resnet101s \n",
      "resnet101s model average inference time : 10.966188907623291ms\n",
      "pass resnet101s.gluon_in1k\n",
      "Benchmarking Inference seresnext101_32x4d \n",
      "seresnext101_32x4d model average inference time : 17.70906686782837ms\n",
      "pass seresnext101_32x4d.gluon_in1k\n",
      "Benchmarking Inference poolformer_s24 \n",
      "poolformer_s24 model average inference time : 8.454699516296387ms\n",
      "pass poolformer_s24.sail_in1k\n",
      "Benchmarking Inference resnet152 \n",
      "resnet152 model average inference time : 15.603017807006836ms\n",
      "pass resnet152.a3_in1k\n",
      "Benchmarking Inference seresnext101_64x4d \n",
      "seresnext101_64x4d model average inference time : 20.45177459716797ms\n",
      "pass seresnext101_64x4d.gluon_in1k\n",
      "Benchmarking Inference efficientformerv2_s1 \n",
      "efficientformerv2_s1 model average inference time : 10.421888828277588ms\n",
      "pass efficientformerv2_s1.snap_dist_in1k\n",
      "Benchmarking Inference vit_base_patch32_224 \n",
      "vit_base_patch32_224 model average inference time : 5.417788028717041ms\n",
      "pass vit_base_patch32_224.augreg_in21k_ft_in1k\n",
      "Benchmarking Inference resnet152d \n",
      "resnet152d model average inference time : 15.345664024353027ms\n",
      "pass resnet152d.gluon_in1k\n",
      "Benchmarking Inference vit_base_patch16_224 \n",
      "vit_base_patch16_224 model average inference time : 7.241761684417725ms\n",
      "pass vit_base_patch16_224.sam_in1k\n",
      "Benchmarking Inference resnet101d \n",
      "resnet101d model average inference time : 11.082377433776855ms\n",
      "pass resnet101d.gluon_in1k\n",
      "Benchmarking Inference repvgg_b2g4 \n",
      "repvgg_b2g4 model average inference time : 8.3320951461792ms\n",
      "pass repvgg_b2g4.rvgg_in1k\n",
      "Benchmarking Inference mixnet_xl \n",
      "mixnet_xl model average inference time : 13.919422626495361ms\n",
      "pass mixnet_xl.ra_in1k\n",
      "Benchmarking Inference resnext101_32x4d \n",
      "resnext101_32x4d model average inference time : 12.81735897064209ms\n",
      "pass resnext101_32x4d.gluon_in1k\n",
      "Benchmarking Inference xcit_tiny_24_p16_224 \n",
      "xcit_tiny_24_p16_224 model average inference time : 17.78379201889038ms\n",
      "pass xcit_tiny_24_p16_224.fb_in1k\n",
      "Benchmarking Inference legacy_seresnext101_32x4d \n",
      "legacy_seresnext101_32x4d model average inference time : 17.701869010925293ms\n",
      "pass legacy_seresnext101_32x4d.in1k\n",
      "Benchmarking Inference resnet101 \n",
      "resnet101 model average inference time : 10.670807361602783ms\n",
      "pass resnet101.a3_in1k\n",
      "Benchmarking Inference res2net50d \n",
      "res2net50d model average inference time : 10.12866497039795ms\n",
      "pass res2net50d.in1k\n",
      "Benchmarking Inference regnety_320 \n",
      "regnety_320 model average inference time : 19.837071895599365ms\n",
      "pass regnety_320.pycls_in1k\n",
      "Benchmarking Inference resmlp_big_24_224 \n",
      "resmlp_big_24_224 model average inference time : 33.98540258407593ms\n",
      "pass resmlp_big_24_224.fb_in1k\n",
      "Benchmarking Inference resnext101_64x4d \n",
      "resnext101_64x4d model average inference time : 17.769670486450195ms\n",
      "pass resnext101_64x4d.gluon_in1k\n",
      "Benchmarking Inference ecaresnet50t \n",
      "ecaresnet50t model average inference time : 7.215790748596191ms\n",
      "pass ecaresnet50t.a3_in1k\n",
      "Benchmarking Inference deit_small_patch16_224 \n",
      "deit_small_patch16_224 model average inference time : 5.264217853546143ms\n",
      "pass deit_small_patch16_224.fb_in1k\n",
      "Benchmarking Inference dpn107 \n",
      "dpn107 model average inference time : 22.535781860351562ms\n",
      "pass dpn107.mx_in1k\n",
      "Benchmarking Inference resmlp_36_224 \n",
      "resmlp_36_224 model average inference time : 7.6830315589904785ms\n",
      "pass resmlp_36_224.fb_in1k\n",
      "Benchmarking Inference levit_conv_192 \n",
      "levit_conv_192 model average inference time : 7.77536153793335ms\n",
      "pass levit_conv_192.fb_dist_in1k\n",
      "Benchmarking Inference levit_192 \n",
      "levit_192 model average inference time : 7.8952765464782715ms\n",
      "pass levit_192.fb_dist_in1k\n",
      "Benchmarking Inference resnet152c \n",
      "resnet152c model average inference time : 15.790762901306152ms\n",
      "pass resnet152c.gluon_in1k\n",
      "Benchmarking Inference pit_xs_distilled_224 \n",
      "pit_xs_distilled_224 model average inference time : 20.11218786239624ms\n",
      "pass pit_xs_distilled_224.in1k\n",
      "Benchmarking Inference regnety_120 \n",
      "regnety_120 model average inference time : 12.92738676071167ms\n",
      "pass regnety_120.pycls_in1k\n",
      "Benchmarking Inference regnetx_320 \n",
      "regnetx_320 model average inference time : 21.861732006072998ms\n",
      "pass regnetx_320.pycls_in1k\n",
      "Benchmarking Inference dpn92 \n",
      "dpn92 model average inference time : 12.972438335418701ms\n",
      "pass dpn92.mx_in1k\n",
      "Benchmarking Inference regnety_160 \n",
      "regnety_160 model average inference time : 15.4417085647583ms\n",
      "pass regnety_160.pycls_in1k\n",
      "Benchmarking Inference resnet152 \n",
      "resnet152 model average inference time : 15.692298412322998ms\n",
      "pass resnet152.gluon_in1k\n",
      "Benchmarking Inference resnetrs50 \n",
      "resnetrs50 model average inference time : 8.860902786254883ms\n",
      "pass resnetrs50.tf_in1k\n",
      "Benchmarking Inference rexnet_130 \n",
      "rexnet_130 model average inference time : 8.065366744995117ms\n",
      "pass rexnet_130.nav_in1k\n",
      "Benchmarking Inference dpn131 \n",
      "dpn131 model average inference time : 21.30138874053955ms\n",
      "pass dpn131.mx_in1k\n",
      "Benchmarking Inference regnetx_160 \n",
      "regnetx_160 model average inference time : 13.654701709747314ms\n",
      "pass regnetx_160.pycls_in1k\n",
      "Benchmarking Inference dla102x2 \n",
      "dla102x2 model average inference time : 15.565629005432129ms\n",
      "pass dla102x2.in1k\n",
      "Benchmarking Inference dpn98 \n",
      "dpn98 model average inference time : 15.733788013458252ms\n",
      "pass dpn98.mx_in1k\n",
      "Benchmarking Inference regnetx_016 \n",
      "regnetx_016 model average inference time : 6.7902326583862305ms\n",
      "pass regnetx_016.tv2_in1k\n",
      "Benchmarking Inference gmlp_s16_224 \n",
      "gmlp_s16_224 model average inference time : 7.412364482879639ms\n",
      "pass gmlp_s16_224.ra3_in1k\n",
      "Benchmarking Inference seresnext50_32x4d \n",
      "seresnext50_32x4d model average inference time : 9.298350811004639ms\n",
      "pass seresnext50_32x4d.gluon_in1k\n",
      "Benchmarking Inference skresnext50_32x4d \n",
      "skresnext50_32x4d model average inference time : 13.348493576049805ms\n",
      "pass skresnext50_32x4d.ra_in1k\n",
      "Benchmarking Inference resnet101c \n",
      "resnet101c model average inference time : 10.984094142913818ms\n",
      "pass resnet101c.gluon_in1k\n",
      "Benchmarking Inference resnext50_32x4d \n",
      "resnext50_32x4d model average inference time : 6.9536566734313965ms\n",
      "pass resnext50_32x4d.a3_in1k\n",
      "Benchmarking Inference regnety_064 \n",
      "regnety_064 model average inference time : 15.278916358947754ms\n",
      "pass regnety_064.pycls_in1k\n",
      "Benchmarking Inference coat_lite_mini \n",
      "coat_lite_mini model average inference time : 7.205154895782471ms\n",
      "pass coat_lite_mini.in1k\n",
      "Benchmarking Inference resmlp_24_224 \n",
      "resmlp_24_224 model average inference time : 5.784389972686768ms\n",
      "pass resmlp_24_224.fb_in1k\n",
      "Benchmarking Inference cait_xxs24_224 \n",
      "cait_xxs24_224 model average inference time : 13.12044382095337ms\n",
      "pass cait_xxs24_224.fb_dist_in1k\n",
      "Benchmarking Inference regnety_080 \n",
      "regnety_080 model average inference time : 12.163395881652832ms\n",
      "pass regnety_080.pycls_in1k\n",
      "Benchmarking Inference pvt_v2_b1 \n",
      "pvt_v2_b1 model average inference time : 5.698051452636719ms\n",
      "pass pvt_v2_b1.in1k\n",
      "Benchmarking Inference xcit_tiny_12_p16_224 \n",
      "xcit_tiny_12_p16_224 model average inference time : 10.466067790985107ms\n",
      "pass xcit_tiny_12_p16_224.fb_dist_in1k\n",
      "Benchmarking Inference resnext101_32x8d \n",
      "resnext101_32x8d model average inference time : 17.519526481628418ms\n",
      "pass resnext101_32x8d.tv_in1k\n",
      "Benchmarking Inference resnet101 \n",
      "resnet101 model average inference time : 10.487277507781982ms\n",
      "pass resnet101.gluon_in1k\n",
      "Benchmarking Inference hrnet_w48 \n",
      "hrnet_w48 model average inference time : 31.634809970855713ms\n",
      "pass hrnet_w48.ms_in1k\n",
      "Benchmarking Inference regnetx_120 \n",
      "regnetx_120 model average inference time : 11.22267484664917ms\n",
      "pass regnetx_120.pycls_in1k\n",
      "Benchmarking Inference resnet50 \n",
      "resnet50 model average inference time : 5.946216583251953ms\n",
      "pass resnet50.am_in1k\n",
      "Benchmarking Inference hrnet_w64 \n",
      "hrnet_w64 model average inference time : 32.469847202301025ms\n",
      "pass hrnet_w64.ms_in1k\n",
      "Benchmarking Inference resnet50 \n",
      "resnet50 model average inference time : 6.029167175292969ms\n",
      "pass resnet50.fb_ssl_yfcc100m_ft_in1k\n",
      "Benchmarking Inference res2net101_26w_4s \n",
      "res2net101_26w_4s model average inference time : 18.10013771057129ms\n",
      "pass res2net101_26w_4s.in1k\n",
      "Benchmarking Inference resnest26d \n",
      "resnest26d model average inference time : 5.912353992462158ms\n",
      "pass resnest26d.gluon_in1k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Inference resnext50_32x4d \n",
      "resnext50_32x4d model average inference time : 6.8968915939331055ms\n",
      "pass resnext50_32x4d.gluon_in1k\n",
      "Benchmarking Inference tf_efficientnet_b0 \n",
      "tf_efficientnet_b0 model average inference time : 7.124233245849609ms\n",
      "pass tf_efficientnet_b0.ns_jft_in1k\n",
      "Benchmarking Inference coat_tiny \n",
      "coat_tiny model average inference time : 68.7386417388916ms\n",
      "pass coat_tiny.in1k\n",
      "Benchmarking Inference regnety_040 \n",
      "regnety_040 model average inference time : 12.902524471282959ms\n",
      "pass regnety_040.pycls_in1k\n",
      "Benchmarking Inference dla169 \n",
      "dla169 model average inference time : 16.4713191986084ms\n",
      "pass dla169.in1k\n",
      "Benchmarking Inference resnet50d \n",
      "resnet50d model average inference time : 6.223800182342529ms\n",
      "pass resnet50d.a3_in1k\n",
      "Benchmarking Inference legacy_seresnext50_32x4d \n",
      "legacy_seresnext50_32x4d model average inference time : 9.208266735076904ms\n",
      "pass legacy_seresnext50_32x4d.in1k\n",
      "Benchmarking Inference hrnet_w44 \n",
      "hrnet_w44 model average inference time : 32.029478549957275ms\n",
      "pass hrnet_w44.ms_in1k\n",
      "Benchmarking Inference regnety_008_tv \n",
      "regnety_008_tv model average inference time : 7.919328212738037ms\n",
      "pass regnety_008_tv.tv2_in1k\n",
      "Benchmarking Inference resnet50s \n",
      "resnet50s model average inference time : 6.310715675354004ms\n",
      "pass resnet50s.gluon_in1k\n",
      "Benchmarking Inference regnetx_080 \n",
      "regnetx_080 model average inference time : 9.132483005523682ms\n",
      "pass regnetx_080.pycls_in1k\n",
      "Benchmarking Inference visformer_tiny \n",
      "visformer_tiny model average inference time : 6.129879951477051ms\n",
      "pass visformer_tiny.in1k\n",
      "Benchmarking Inference levit_conv_128 \n",
      "levit_conv_128 model average inference time : 7.479846477508545ms\n",
      "pass levit_conv_128.fb_dist_in1k\n",
      "Benchmarking Inference levit_128 \n",
      "levit_128 model average inference time : 7.822682857513428ms\n",
      "pass levit_128.fb_dist_in1k\n",
      "Benchmarking Inference resnet50d \n",
      "resnet50d model average inference time : 6.313881874084473ms\n",
      "pass resnet50d.gluon_in1k\n",
      "Benchmarking Inference res2net50_26w_8s \n",
      "res2net50_26w_8s model average inference time : 15.992953777313232ms\n",
      "pass res2net50_26w_8s.in1k\n",
      "Benchmarking Inference dla60_res2next \n",
      "dla60_res2next model average inference time : 11.43082857131958ms\n",
      "pass dla60_res2next.in1k\n",
      "Benchmarking Inference resnet152 \n",
      "resnet152 model average inference time : 15.578434467315674ms\n",
      "pass resnet152.tv_in1k\n",
      "Benchmarking Inference dla60_res2net \n",
      "dla60_res2net model average inference time : 11.277117729187012ms\n",
      "pass dla60_res2net.in1k\n",
      "Benchmarking Inference mixnet_l \n",
      "mixnet_l model average inference time : 11.011128425598145ms\n",
      "pass mixnet_l.ft_in1k\n",
      "Benchmarking Inference dla102x \n",
      "dla102x model average inference time : 13.253376483917236ms\n",
      "pass dla102x.in1k\n",
      "Benchmarking Inference hrnet_w18 \n",
      "hrnet_w18 model average inference time : 29.6038818359375ms\n",
      "pass hrnet_w18.ms_aug_in1k\n",
      "Benchmarking Inference pit_xs_224 \n",
      "pit_xs_224 model average inference time : 20.11497974395752ms\n",
      "pass pit_xs_224.in1k\n",
      "Benchmarking Inference regnetx_064 \n",
      "regnetx_064 model average inference time : 8.834257125854492ms\n",
      "pass regnetx_064.pycls_in1k\n",
      "Benchmarking Inference poolformerv2_s12 \n",
      "poolformerv2_s12 model average inference time : 5.599558353424072ms\n",
      "pass poolformerv2_s12.sail_in1k\n",
      "Benchmarking Inference hrnet_w40 \n",
      "hrnet_w40 model average inference time : 31.0919451713562ms\n",
      "pass hrnet_w40.ms_in1k\n",
      "Benchmarking Inference repvgg_b2 \n",
      "repvgg_b2 model average inference time : 8.993251323699951ms\n",
      "pass repvgg_b2.rvgg_in1k\n",
      "Benchmarking Inference res2net50_26w_6s \n",
      "res2net50_26w_6s model average inference time : 12.832269668579102ms\n",
      "pass res2net50_26w_6s.in1k\n",
      "Benchmarking Inference resmlp_12_224 \n",
      "resmlp_12_224 model average inference time : 3.2414817810058594ms\n",
      "pass resmlp_12_224.fb_distilled_in1k\n",
      "Benchmarking Inference legacy_seresnet152 \n",
      "legacy_seresnet152 model average inference time : 23.118774890899658ms\n",
      "pass legacy_seresnet152.in1k\n",
      "Benchmarking Inference SelecSls60b \n",
      "SelecSls60b model average inference time : 6.771907806396484ms\n",
      "pass SelecSls60b.in1k\n",
      "Benchmarking Inference hrnet_w32 \n",
      "hrnet_w32 model average inference time : 31.35765314102173ms\n",
      "pass hrnet_w32.ms_in1k\n",
      "Benchmarking Inference tf_efficientnetv2_b0 \n",
      "tf_efficientnetv2_b0 model average inference time : 7.897045612335205ms\n",
      "pass tf_efficientnetv2_b0.in1k\n",
      "Benchmarking Inference regnetx_040 \n",
      "regnetx_040 model average inference time : 10.728070735931396ms\n",
      "pass regnetx_040.pycls_in1k\n",
      "Benchmarking Inference regnety_032 \n",
      "regnety_032 model average inference time : 12.424120903015137ms\n",
      "pass regnety_032.pycls_in1k\n",
      "Benchmarking Inference hrnet_w30 \n",
      "hrnet_w30 model average inference time : 31.71663761138916ms\n",
      "pass hrnet_w30.ms_in1k\n",
      "Benchmarking Inference efficientnet_es \n",
      "efficientnet_es model average inference time : 4.884510040283203ms\n",
      "pass efficientnet_es.ra_in1k\n",
      "Benchmarking Inference tf_mixnet_l \n",
      "tf_mixnet_l model average inference time : 11.649773120880127ms\n",
      "pass tf_mixnet_l.in1k\n",
      "Benchmarking Inference wide_resnet101_2 \n",
      "wide_resnet101_2 model average inference time : 12.772037982940674ms\n",
      "pass wide_resnet101_2.tv_in1k\n",
      "Benchmarking Inference vit_small_patch16_224 \n",
      "vit_small_patch16_224 model average inference time : 5.057015419006348ms\n",
      "pass vit_small_patch16_224.augreg_in1k\n",
      "Benchmarking Inference dla60x \n",
      "dla60x model average inference time : 7.978470325469971ms\n",
      "pass dla60x.in1k\n",
      "Benchmarking Inference legacy_seresnet101 \n",
      "legacy_seresnet101 model average inference time : 15.83892822265625ms\n",
      "pass legacy_seresnet101.in1k\n",
      "Benchmarking Inference resnet50 \n",
      "resnet50 model average inference time : 6.068897247314453ms\n",
      "pass resnet50.a3_in1k\n",
      "Benchmarking Inference coat_lite_tiny \n",
      "coat_lite_tiny model average inference time : 12.639656066894531ms\n",
      "pass coat_lite_tiny.in1k\n",
      "Benchmarking Inference wide_resnet50_2 \n",
      "wide_resnet50_2 model average inference time : 7.459447383880615ms\n",
      "pass wide_resnet50_2.tv_in1k\n",
      "Benchmarking Inference repvgg_b1 \n",
      "repvgg_b1 model average inference time : 7.667744159698486ms\n",
      "pass repvgg_b1.rvgg_in1k\n",
      "Benchmarking Inference vit_base_patch16_224 \n",
      "vit_base_patch16_224 model average inference time : 7.210698127746582ms\n",
      "pass vit_base_patch16_224.augreg_in1k\n",
      "Benchmarking Inference res2net50_26w_4s \n",
      "res2net50_26w_4s model average inference time : 9.922966957092285ms\n",
      "pass res2net50_26w_4s.in1k\n",
      "Benchmarking Inference hardcorenas_f \n",
      "hardcorenas_f model average inference time : 7.523589134216309ms\n",
      "pass hardcorenas_f.miil_green_in1k\n",
      "Benchmarking Inference res2net50_14w_8s \n",
      "res2net50_14w_8s model average inference time : 15.014255046844482ms\n",
      "pass res2net50_14w_8s.in1k\n",
      "Benchmarking Inference SelecSls60 \n",
      "SelecSls60 model average inference time : 6.399691104888916ms\n",
      "pass SelecSls60.in1k\n",
      "Benchmarking Inference regnetx_032 \n",
      "regnetx_032 model average inference time : 10.57344913482666ms\n",
      "pass regnetx_032.pycls_in1k\n",
      "Benchmarking Inference resnet50c \n",
      "resnet50c model average inference time : 6.4023542404174805ms\n",
      "pass resnet50c.gluon_in1k\n",
      "Benchmarking Inference res2next50 \n",
      "res2next50 model average inference time : 10.14350414276123ms\n",
      "pass res2next50.in1k\n",
      "Benchmarking Inference dla102 \n",
      "dla102 model average inference time : 10.897886753082275ms\n",
      "pass dla102.in1k\n",
      "Benchmarking Inference rexnet_100 \n",
      "rexnet_100 model average inference time : 8.225939273834229ms\n",
      "pass rexnet_100.nav_in1k\n",
      "Benchmarking Inference res2net50_48w_2s \n",
      "res2net50_48w_2s model average inference time : 6.51188850402832ms\n",
      "pass res2net50_48w_2s.in1k\n",
      "Benchmarking Inference xcit_tiny_12_p16_224 \n",
      "xcit_tiny_12_p16_224 model average inference time : 10.335273742675781ms\n",
      "pass xcit_tiny_12_p16_224.fb_in1k\n",
      "Benchmarking Inference poolformer_s12 \n",
      "poolformer_s12 model average inference time : 4.93119478225708ms\n",
      "pass poolformer_s12.sail_in1k\n",
      "Benchmarking Inference efficientnet_b0 \n",
      "efficientnet_b0 model average inference time : 6.829485893249512ms\n",
      "pass efficientnet_b0.ra_in1k\n",
      "Benchmarking Inference tf_efficientnet_cc_b0_8e \n",
      "tf_efficientnet_cc_b0_8e model average inference time : 8.865745067596436ms\n",
      "pass tf_efficientnet_cc_b0_8e.in1k\n",
      "Benchmarking Inference hardcorenas_e \n",
      "hardcorenas_e model average inference time : 7.280395030975342ms\n",
      "pass hardcorenas_e.miil_green_in1k\n",
      "Benchmarking Inference gmixer_24_224 \n",
      "gmixer_24_224 model average inference time : 7.653989791870117ms\n",
      "pass gmixer_24_224.ra3_in1k\n",
      "Benchmarking Inference regnety_016 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regnety_016 model average inference time : 18.524765968322754ms\n",
      "pass regnety_016.pycls_in1k\n",
      "Benchmarking Inference resnext50_32x4d \n",
      "resnext50_32x4d model average inference time : 6.988542079925537ms\n",
      "pass resnext50_32x4d.tv_in1k\n",
      "Benchmarking Inference resnet50 \n",
      "resnet50 model average inference time : 6.067063808441162ms\n",
      "pass resnet50.gluon_in1k\n",
      "Benchmarking Inference densenet161 \n",
      "densenet161 model average inference time : 18.100674152374268ms\n",
      "pass densenet161.tv_in1k\n",
      "Benchmarking Inference mobilenetv2_120d \n",
      "mobilenetv2_120d model average inference time : 6.5221476554870605ms\n",
      "pass mobilenetv2_120d.ra_in1k\n",
      "Benchmarking Inference resnet101 \n",
      "resnet101 model average inference time : 10.733718872070312ms\n",
      "pass resnet101.tv_in1k\n",
      "Benchmarking Inference dpn68b \n",
      "dpn68b model average inference time : 10.912940502166748ms\n",
      "pass dpn68b.mx_in1k\n",
      "Benchmarking Inference hardcorenas_d \n",
      "hardcorenas_d model average inference time : 7.697744369506836ms\n",
      "pass hardcorenas_d.miil_green_in1k\n",
      "Benchmarking Inference xcit_nano_12_p8_224 \n",
      "xcit_nano_12_p8_224 model average inference time : 10.58086633682251ms\n",
      "pass xcit_nano_12_p8_224.fb_dist_in1k\n",
      "Benchmarking Inference dla60 \n",
      "dla60 model average inference time : 6.775472164154053ms\n",
      "pass dla60.in1k\n",
      "Benchmarking Inference repvgg_b1g4 \n",
      "repvgg_b1g4 model average inference time : 7.35137939453125ms\n",
      "pass repvgg_b1g4.rvgg_in1k\n",
      "Benchmarking Inference convmixer_1024_20_ks9_p14 \n",
      "convmixer_1024_20_ks9_p14 model average inference time : 6.918425559997559ms\n",
      "pass convmixer_1024_20_ks9_p14.in1k\n",
      "Benchmarking Inference legacy_seresnet50 \n",
      "legacy_seresnet50 model average inference time : 8.517074584960938ms\n",
      "pass legacy_seresnet50.in1k\n",
      "Benchmarking Inference regnetx_008 \n",
      "regnetx_008 model average inference time : 6.463606357574463ms\n",
      "pass regnetx_008.tv2_in1k\n",
      "Benchmarking Inference tf_efficientnet_b0 \n",
      "tf_efficientnet_b0 model average inference time : 7.17585563659668ms\n",
      "pass tf_efficientnet_b0.ap_in1k\n",
      "Benchmarking Inference skresnet34 \n",
      "skresnet34 model average inference time : 10.308465957641602ms\n",
      "pass skresnet34.ra_in1k\n",
      "Benchmarking Inference tf_efficientnet_cc_b0_4e \n",
      "tf_efficientnet_cc_b0_4e model average inference time : 8.857619762420654ms\n",
      "pass tf_efficientnet_cc_b0_4e.in1k\n",
      "Benchmarking Inference seresnet50 \n",
      "seresnet50 model average inference time : 8.503890037536621ms\n",
      "pass seresnet50.a3_in1k\n",
      "Benchmarking Inference resmlp_12_224 \n",
      "resmlp_12_224 model average inference time : 3.413872718811035ms\n",
      "pass resmlp_12_224.fb_in1k\n",
      "Benchmarking Inference densenet201 \n",
      "densenet201 model average inference time : 22.258176803588867ms\n",
      "pass densenet201.tv_in1k\n",
      "Benchmarking Inference mobilenetv3_large_100 \n",
      "mobilenetv3_large_100 model average inference time : 5.553140640258789ms\n",
      "pass mobilenetv3_large_100.miil_in21k_ft_in1k\n",
      "Benchmarking Inference mixnet_m \n",
      "mixnet_m model average inference time : 10.886139869689941ms\n",
      "pass mixnet_m.ft_in1k\n",
      "Benchmarking Inference legacy_seresnext26_32x4d \n",
      "legacy_seresnext26_32x4d model average inference time : 5.17878532409668ms\n",
      "pass legacy_seresnext26_32x4d.in1k\n",
      "Benchmarking Inference gernet_s \n",
      "gernet_s model average inference time : 5.079464912414551ms\n",
      "pass gernet_s.idstcv_in1k\n",
      "Benchmarking Inference tf_efficientnet_b0 \n",
      "tf_efficientnet_b0 model average inference time : 7.169198989868164ms\n",
      "pass tf_efficientnet_b0.aa_in1k\n",
      "Benchmarking Inference hrnet_w18 \n",
      "hrnet_w18 model average inference time : 29.806976318359375ms\n",
      "pass hrnet_w18.ms_in1k\n",
      "Benchmarking Inference SelecSls42b \n",
      "SelecSls42b model average inference time : 4.956605434417725ms\n",
      "pass SelecSls42b.in1k\n",
      "Benchmarking Inference efficientformerv2_s0 \n",
      "efficientformerv2_s0 model average inference time : 8.923966884613037ms\n",
      "pass efficientformerv2_s0.snap_dist_in1k\n",
      "Benchmarking Inference hardcorenas_c \n",
      "hardcorenas_c model average inference time : 5.953586101531982ms\n",
      "pass hardcorenas_c.miil_green_in1k\n",
      "Benchmarking Inference dpn68 \n",
      "dpn68 model average inference time : 10.389389991760254ms\n",
      "pass dpn68.mx_in1k\n",
      "Benchmarking Inference tf_mixnet_m \n",
      "tf_mixnet_m model average inference time : 11.845738887786865ms\n",
      "pass tf_mixnet_m.in1k\n",
      "Benchmarking Inference regnetx_016 \n",
      "regnetx_016 model average inference time : 6.8291521072387695ms\n",
      "pass regnetx_016.pycls_in1k\n",
      "Benchmarking Inference tf_efficientnet_es \n",
      "tf_efficientnet_es model average inference time : 5.208547115325928ms\n",
      "pass tf_efficientnet_es.in1k\n",
      "Benchmarking Inference mobilenetv2_140 \n",
      "mobilenetv2_140 model average inference time : 4.76656436920166ms\n",
      "pass mobilenetv2_140.ra_in1k\n",
      "Benchmarking Inference levit_128s \n",
      "levit_128s model average inference time : 6.505489349365234ms\n",
      "pass levit_128s.fb_dist_in1k\n",
      "Benchmarking Inference levit_conv_128s \n",
      "levit_conv_128s model average inference time : 6.243619918823242ms\n",
      "pass levit_conv_128s.fb_dist_in1k\n",
      "Benchmarking Inference repvgg_a2 \n",
      "repvgg_a2 model average inference time : 5.808000564575195ms\n",
      "pass repvgg_a2.rvgg_in1k\n",
      "Benchmarking Inference resnet50 \n",
      "resnet50 model average inference time : 6.153419017791748ms\n",
      "pass resnet50.tv_in1k\n",
      "Benchmarking Inference hardcorenas_b \n",
      "hardcorenas_b model average inference time : 5.740077495574951ms\n",
      "pass hardcorenas_b.miil_green_in1k\n",
      "Benchmarking Inference densenet169 \n",
      "densenet169 model average inference time : 18.620362281799316ms\n",
      "pass densenet169.tv_in1k\n",
      "Benchmarking Inference regnety_004 \n",
      "regnety_004 model average inference time : 8.961923122406006ms\n",
      "pass regnety_004.tv2_in1k\n",
      "Benchmarking Inference tf_efficientnet_b0 \n",
      "tf_efficientnet_b0 model average inference time : 7.33182430267334ms\n",
      "pass tf_efficientnet_b0.in1k\n",
      "Benchmarking Inference mixnet_s \n",
      "mixnet_s model average inference time : 8.854031562805176ms\n",
      "pass mixnet_s.ft_in1k\n",
      "Benchmarking Inference vit_small_patch32_224 \n",
      "vit_small_patch32_224 model average inference time : 5.090837478637695ms\n",
      "pass vit_small_patch32_224.augreg_in21k_ft_in1k\n",
      "Benchmarking Inference regnety_008 \n",
      "regnety_008 model average inference time : 8.072283267974854ms\n",
      "pass regnety_008.pycls_in1k\n",
      "Benchmarking Inference efficientnet_lite0 \n",
      "efficientnet_lite0 model average inference time : 4.460325241088867ms\n",
      "pass efficientnet_lite0.ra_in1k\n",
      "Benchmarking Inference resnest14d \n",
      "resnest14d model average inference time : 3.69107723236084ms\n",
      "pass resnest14d.gluon_in1k\n",
      "Benchmarking Inference hardcorenas_a \n",
      "hardcorenas_a model average inference time : 4.650020599365234ms\n",
      "pass hardcorenas_a.miil_green_in1k\n",
      "Benchmarking Inference efficientnet_es_pruned \n",
      "efficientnet_es_pruned model average inference time : 4.987585544586182ms\n",
      "pass efficientnet_es_pruned.in1k\n",
      "Benchmarking Inference mobilenetv3_rw \n",
      "mobilenetv3_rw model average inference time : 6.035475730895996ms\n",
      "pass mobilenetv3_rw.rmsp_in1k\n",
      "Benchmarking Inference semnasnet_100 \n",
      "semnasnet_100 model average inference time : 5.511901378631592ms\n",
      "pass semnasnet_100.rmsp_in1k\n",
      "Benchmarking Inference mobilenetv3_large_100 \n",
      "mobilenetv3_large_100 model average inference time : 5.578374862670898ms\n",
      "pass mobilenetv3_large_100.ra_in1k\n",
      "Benchmarking Inference vit_tiny_patch16_224 \n",
      "vit_tiny_patch16_224 model average inference time : 5.240387916564941ms\n",
      "pass vit_tiny_patch16_224.augreg_in21k_ft_in1k\n",
      "Benchmarking Inference mobilenetv2_110d \n",
      "mobilenetv2_110d model average inference time : 6.018481254577637ms\n",
      "pass mobilenetv2_110d.ra_in1k\n",
      "Benchmarking Inference tf_mixnet_s \n",
      "tf_mixnet_s model average inference time : 9.788095951080322ms\n",
      "pass tf_mixnet_s.in1k\n",
      "Benchmarking Inference repvgg_b0 \n",
      "repvgg_b0 model average inference time : 7.016191482543945ms\n",
      "pass repvgg_b0.rvgg_in1k\n",
      "Benchmarking Inference deit_tiny_distilled_patch16_224 \n",
      "deit_tiny_distilled_patch16_224 model average inference time : 5.268878936767578ms\n",
      "pass deit_tiny_distilled_patch16_224.fb_in1k\n",
      "Benchmarking Inference mixer_b16_224 \n",
      "mixer_b16_224 model average inference time : 5.204610824584961ms\n",
      "pass mixer_b16_224.goog_in21k_ft_in1k\n",
      "Benchmarking Inference hrnet_w18_small_v2 \n",
      "hrnet_w18_small_v2 model average inference time : 15.807106494903564ms\n",
      "pass hrnet_w18_small_v2.ms_in1k\n",
      "Benchmarking Inference tf_efficientnet_lite0 \n",
      "tf_efficientnet_lite0 model average inference time : 4.882314205169678ms\n",
      "pass tf_efficientnet_lite0.in1k\n",
      "Benchmarking Inference tf_mobilenetv3_large_100 \n",
      "tf_mobilenetv3_large_100 model average inference time : 5.615265369415283ms\n",
      "pass tf_mobilenetv3_large_100.in1k\n",
      "Benchmarking Inference pit_ti_distilled_224 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pit_ti_distilled_224 model average inference time : 14.24842357635498ms\n",
      "pass pit_ti_distilled_224.in1k\n",
      "Benchmarking Inference densenet121 \n",
      "densenet121 model average inference time : 13.54733943939209ms\n",
      "pass densenet121.tv_in1k\n",
      "Benchmarking Inference regnety_006 \n",
      "regnety_006 model average inference time : 8.220255374908447ms\n",
      "pass regnety_006.pycls_in1k\n",
      "Benchmarking Inference regnetx_004_tv \n",
      "regnetx_004_tv model average inference time : 8.409786224365234ms\n",
      "pass regnetx_004_tv.tv2_in1k\n",
      "Benchmarking Inference dla34 \n",
      "dla34 model average inference time : 4.670431613922119ms\n",
      "pass dla34.in1k\n",
      "Benchmarking Inference xcit_nano_12_p8_224 \n",
      "xcit_nano_12_p8_224 model average inference time : 10.20376443862915ms\n",
      "pass xcit_nano_12_p8_224.fb_in1k\n",
      "Benchmarking Inference fbnetc_100 \n",
      "fbnetc_100 model average inference time : 5.674831867218018ms\n",
      "pass fbnetc_100.rmsp_in1k\n",
      "Benchmarking Inference legacy_seresnet34 \n",
      "legacy_seresnet34 model average inference time : 6.83682918548584ms\n",
      "pass legacy_seresnet34.in1k\n",
      "Benchmarking Inference regnetx_008 \n",
      "regnetx_008 model average inference time : 6.68095588684082ms\n",
      "pass regnetx_008.pycls_in1k\n",
      "Benchmarking Inference resnet34 \n",
      "resnet34 model average inference time : 4.236695766448975ms\n",
      "pass resnet34.gluon_in1k\n",
      "Benchmarking Inference mnasnet_100 \n",
      "mnasnet_100 model average inference time : 4.876444339752197ms\n",
      "pass mnasnet_100.rmsp_in1k\n",
      "Benchmarking Inference vgg19_bn \n",
      "vgg19_bn model average inference time : 9.868698120117188ms\n",
      "pass vgg19_bn.tv_in1k\n",
      "Benchmarking Inference vit_base_patch32_224 \n",
      "vit_base_patch32_224 model average inference time : 5.367248058319092ms\n",
      "pass vit_base_patch32_224.augreg_in1k\n",
      "Benchmarking Inference convit_tiny \n",
      "convit_tiny model average inference time : 6.816277503967285ms\n",
      "pass convit_tiny.fb_in1k\n",
      "Benchmarking Inference spnasnet_100 \n",
      "spnasnet_100 model average inference time : 5.3023529052734375ms\n",
      "pass spnasnet_100.rmsp_in1k\n",
      "Benchmarking Inference resnet34 \n",
      "resnet34 model average inference time : 4.374721050262451ms\n",
      "pass resnet34.a3_in1k\n",
      "Benchmarking Inference ghostnet_100 \n",
      "ghostnet_100 model average inference time : 7.532303333282471ms\n",
      "pass ghostnet_100.in1k\n",
      "Benchmarking Inference regnety_004 \n",
      "regnety_004 model average inference time : 8.569250106811523ms\n",
      "pass regnety_004.pycls_in1k\n",
      "Benchmarking Inference skresnet18 \n",
      "skresnet18 model average inference time : 6.053972244262695ms\n",
      "pass skresnet18.ra_in1k\n",
      "Benchmarking Inference regnetx_006 \n",
      "regnetx_006 model average inference time : 7.090659141540527ms\n",
      "pass regnetx_006.pycls_in1k\n",
      "Benchmarking Inference pit_ti_224 \n",
      "pit_ti_224 model average inference time : 14.105398654937744ms\n",
      "pass pit_ti_224.in1k\n",
      "Benchmarking Inference resnet18 \n",
      "resnet18 model average inference time : 3.0046439170837402ms\n",
      "pass resnet18.fb_swsl_ig1b_ft_in1k\n",
      "Benchmarking Inference vgg16_bn \n",
      "vgg16_bn model average inference time : 8.778541088104248ms\n",
      "pass vgg16_bn.tv_in1k\n",
      "Benchmarking Inference semnasnet_075 \n",
      "semnasnet_075 model average inference time : 5.673954486846924ms\n",
      "pass semnasnet_075.rmsp_in1k\n",
      "Benchmarking Inference resnet34 \n",
      "resnet34 model average inference time : 4.503929615020752ms\n",
      "pass resnet34.tv_in1k\n",
      "Benchmarking Inference mobilenetv2_100 \n",
      "mobilenetv2_100 model average inference time : 4.476947784423828ms\n",
      "pass mobilenetv2_100.ra_in1k\n",
      "Benchmarking Inference xcit_nano_12_p16_224 \n",
      "xcit_nano_12_p16_224 model average inference time : 10.163145065307617ms\n",
      "pass xcit_nano_12_p16_224.fb_dist_in1k\n",
      "Benchmarking Inference vit_base_patch32_224 \n",
      "vit_base_patch32_224 model average inference time : 5.458745956420898ms\n",
      "pass vit_base_patch32_224.sam_in1k\n",
      "Benchmarking Inference resnet18 \n",
      "resnet18 model average inference time : 2.7906179428100586ms\n",
      "pass resnet18.fb_ssl_yfcc100m_ft_in1k\n",
      "Benchmarking Inference tf_mobilenetv3_large_075 \n",
      "tf_mobilenetv3_large_075 model average inference time : 5.436294078826904ms\n",
      "pass tf_mobilenetv3_large_075.in1k\n",
      "Benchmarking Inference deit_tiny_patch16_224 \n",
      "deit_tiny_patch16_224 model average inference time : 5.188148021697998ms\n",
      "pass deit_tiny_patch16_224.fb_in1k\n",
      "Benchmarking Inference hrnet_w18_small \n",
      "hrnet_w18_small model average inference time : 9.229059219360352ms\n",
      "pass hrnet_w18_small.ms_in1k\n",
      "Benchmarking Inference vgg19 \n",
      "vgg19 model average inference time : 9.008002281188965ms\n",
      "pass vgg19.tv_in1k\n",
      "Benchmarking Inference regnetx_004 \n",
      "regnetx_004 model average inference time : 8.247525691986084ms\n",
      "pass regnetx_004.pycls_in1k\n",
      "Benchmarking Inference tf_mobilenetv3_large_minimal_100 \n",
      "tf_mobilenetv3_large_minimal_100 model average inference time : 4.757728576660156ms\n",
      "pass tf_mobilenetv3_large_minimal_100.in1k\n",
      "Benchmarking Inference legacy_seresnet18 \n",
      "legacy_seresnet18 model average inference time : 4.124011993408203ms\n",
      "pass legacy_seresnet18.in1k\n",
      "Benchmarking Inference resnet14t \n",
      "resnet14t model average inference time : 3.212552070617676ms\n",
      "pass resnet14t.c3_in1k\n",
      "Benchmarking Inference vgg16 \n",
      "vgg16 model average inference time : 8.064508438110352ms\n",
      "pass vgg16.tv_in1k\n",
      "Benchmarking Inference vgg13_bn \n",
      "vgg13_bn model average inference time : 7.81282901763916ms\n",
      "pass vgg13_bn.tv_in1k\n",
      "Benchmarking Inference vit_tiny_r_s16_p8_224 \n",
      "vit_tiny_r_s16_p8_224 model average inference time : 5.451843738555908ms\n",
      "pass vit_tiny_r_s16_p8_224.augreg_in21k_ft_in1k\n",
      "Benchmarking Inference lcnet_100 \n",
      "lcnet_100 model average inference time : 2.9926443099975586ms\n",
      "pass lcnet_100.ra2_in1k\n",
      "Benchmarking Inference pvt_v2_b0 \n",
      "pvt_v2_b0 model average inference time : 5.48532247543335ms\n",
      "pass pvt_v2_b0.in1k\n",
      "Benchmarking Inference resnet18 \n",
      "resnet18 model average inference time : 2.977614402770996ms\n",
      "pass resnet18.gluon_in1k\n",
      "Benchmarking Inference vgg11_bn \n",
      "vgg11_bn model average inference time : 6.24985933303833ms\n",
      "pass vgg11_bn.tv_in1k\n",
      "Benchmarking Inference xcit_nano_12_p16_224 \n",
      "xcit_nano_12_p16_224 model average inference time : 10.203125476837158ms\n",
      "pass xcit_nano_12_p16_224.fb_in1k\n",
      "Benchmarking Inference regnety_002 \n",
      "regnety_002 model average inference time : 7.542617321014404ms\n",
      "pass regnety_002.pycls_in1k\n",
      "Benchmarking Inference resnet18 \n",
      "resnet18 model average inference time : 3.270549774169922ms\n",
      "pass resnet18.tv_in1k\n",
      "Benchmarking Inference mixer_l16_224 \n",
      "mixer_l16_224 model average inference time : 16.17499828338623ms\n",
      "pass mixer_l16_224.goog_in21k_ft_in1k\n",
      "Benchmarking Inference vgg13 \n",
      "vgg13 model average inference time : 7.136237621307373ms\n",
      "pass vgg13.tv_in1k\n",
      "Benchmarking Inference resnet18 \n",
      "resnet18 model average inference time : 2.834446430206299ms\n",
      "pass resnet18.a3_in1k\n",
      "Benchmarking Inference vgg11 \n",
      "vgg11 model average inference time : 5.823888778686523ms\n",
      "pass vgg11.tv_in1k\n",
      "Benchmarking Inference resnet10t \n",
      "resnet10t model average inference time : 2.4022579193115234ms\n",
      "pass resnet10t.c3_in1k\n",
      "Benchmarking Inference regnetx_002 \n",
      "regnetx_002 model average inference time : 5.551736354827881ms\n",
      "pass regnetx_002.pycls_in1k\n",
      "Benchmarking Inference lcnet_075 \n",
      "lcnet_075 model average inference time : 3.0254340171813965ms\n",
      "pass lcnet_075.ra2_in1k\n",
      "Benchmarking Inference dla60x_c \n",
      "dla60x_c model average inference time : 5.526912212371826ms\n",
      "pass dla60x_c.in1k\n",
      "Benchmarking Inference mobilenetv3_small_100 \n",
      "mobilenetv3_small_100 model average inference time : 4.501297473907471ms\n",
      "pass mobilenetv3_small_100.lamb_in1k\n",
      "Benchmarking Inference tf_mobilenetv3_small_100 \n",
      "tf_mobilenetv3_small_100 model average inference time : 4.895179271697998ms\n",
      "pass tf_mobilenetv3_small_100.in1k\n",
      "Benchmarking Inference mnasnet_small \n",
      "mnasnet_small model average inference time : 5.86989164352417ms\n",
      "pass mnasnet_small.lamb_in1k\n",
      "Benchmarking Inference dla46x_c \n",
      "dla46x_c model average inference time : 4.946575164794922ms\n",
      "pass dla46x_c.in1k\n",
      "Benchmarking Inference mobilenetv2_050 \n",
      "mobilenetv2_050 model average inference time : 4.6329665184021ms\n",
      "pass mobilenetv2_050.lamb_in1k\n",
      "Benchmarking Inference tf_mobilenetv3_small_075 \n",
      "tf_mobilenetv3_small_075 model average inference time : 4.592947959899902ms\n",
      "pass tf_mobilenetv3_small_075.in1k\n",
      "Benchmarking Inference dla46_c \n",
      "dla46_c model average inference time : 4.71571683883667ms\n",
      "pass dla46_c.in1k\n",
      "Benchmarking Inference mobilenetv3_small_075 \n",
      "mobilenetv3_small_075 model average inference time : 4.642612934112549ms\n",
      "pass mobilenetv3_small_075.lamb_in1k\n",
      "Benchmarking Inference lcnet_050 \n",
      "lcnet_050 model average inference time : 3.030221462249756ms\n",
      "pass lcnet_050.ra2_in1k\n",
      "Benchmarking Inference tf_mobilenetv3_small_minimal_100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_mobilenetv3_small_minimal_100 model average inference time : 3.430161476135254ms\n",
      "pass tf_mobilenetv3_small_minimal_100.in1k\n",
      "Benchmarking Inference mobilenetv3_small_050 \n",
      "mobilenetv3_small_050 model average inference time : 4.711909294128418ms\n",
      "pass mobilenetv3_small_050.lamb_in1k\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tnt_s_patch16_224': {'fp32': 11.72980785369873, 'top1': 86.914}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark = {}\n",
    "\n",
    "# inference float precision\n",
    "for i,modelname in tqdm(enumerate((modellist))):\n",
    "    try:\n",
    "        benchmark = inference(modelname.split(\".\")[0], benchmark)\n",
    "    except:\n",
    "        print(\"pass {}\".format(modelname))\n",
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fp32</th>\n",
       "      <th>top1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tnt_s_patch16_224</th>\n",
       "      <td>11.729808</td>\n",
       "      <td>86.914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        fp32    top1\n",
       "tnt_s_patch16_224  11.729808  86.914"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(benchmark).T\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"results_fp32_224.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7fa3873d8f70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.lmplot(y='top1', x='fp32',  \n",
    "           data=df_results, logx=True,\n",
    "           fit_reg=False, scatter_kws={'alpha':0.8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For various image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ryujaehun/pytorch-gpu-benchmark/blob/master/benchmark_models.py\n",
    "def inference_imsize(modelname, benchmark, imsize):\n",
    "    with torch.no_grad():\n",
    "        model = timm.create_model(modelname.split(\".\")[0])\n",
    "        model=model.to('cuda')\n",
    "        model.eval()\n",
    "        precision = \"float\"\n",
    "        durations = []\n",
    "        rand_loader = DataLoader(dataset=RandomDataset(BATCH_SIZE*(WARM_UP + NUM_TEST), imsize),\n",
    "                         batch_size=BATCH_SIZE, shuffle=False,num_workers=8)\n",
    "        print(f'Benchmarking Inference {modelname} ')\n",
    "        for step,img in enumerate(rand_loader):\n",
    "            img=getattr(img,precision)()\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "            model(img.to('cuda'))\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            if step >= WARM_UP:\n",
    "                durations.append((end - start)*1000)\n",
    "        print(f'{modelname} model average inference time : {sum(durations)/len(durations)}ms')\n",
    "        \n",
    "        benchmark[modelname] = {\"fp32\": np.mean(durations), \"top1\": float(df_models[df_models[\"model\"]==modelname][\"top1\"]), \"imsize\": imsize}\n",
    "    return benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5657284e9f364ec08fa2def09f3a0d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Inference eva02_large_patch14_448.mim_m38m_ft_in22k_in1k \n",
      "eva02_large_patch14_448.mim_m38m_ft_in22k_in1k model average inference time : 171.2168550491333ms\n",
      "Benchmarking Inference eva_giant_patch14_336.clip_ft_in1k \n",
      "eva_giant_patch14_336.clip_ft_in1k model average inference time : 166.4306116104126ms\n",
      "Benchmarking Inference eva02_large_patch14_448.mim_in22k_ft_in22k_in1k \n",
      "eva02_large_patch14_448.mim_in22k_ft_in22k_in1k model average inference time : 171.71310424804688ms\n",
      "Benchmarking Inference eva_giant_patch14_560.m30m_ft_in22k_in1k \n",
      "eva_giant_patch14_560.m30m_ft_in22k_in1k model average inference time : 581.2683391571045ms\n",
      "Benchmarking Inference eva02_large_patch14_448.mim_in22k_ft_in1k \n",
      "eva02_large_patch14_448.mim_in22k_ft_in1k model average inference time : 172.71728515625ms\n",
      "Benchmarking Inference eva_giant_patch14_336.m30m_ft_in22k_in1k \n",
      "eva_giant_patch14_336.m30m_ft_in22k_in1k model average inference time : 168.33984851837158ms\n",
      "Benchmarking Inference eva_large_patch14_336.in22k_ft_in1k \n",
      "eva_large_patch14_336.in22k_ft_in1k model average inference time : 63.14021587371826ms\n",
      "Benchmarking Inference eva_giant_patch14_224.clip_ft_in1k \n",
      "eva_giant_patch14_224.clip_ft_in1k model average inference time : 77.15846538543701ms\n",
      "Benchmarking Inference eva02_base_patch14_448.mim_in22k_ft_in22k_in1k \n",
      "eva02_base_patch14_448.mim_in22k_ft_in22k_in1k model average inference time : 64.20849800109863ms\n",
      "Benchmarking Inference eva02_large_patch14_448.mim_m38m_ft_in1k \n",
      "eva02_large_patch14_448.mim_m38m_ft_in1k model average inference time : 171.81569576263428ms\n",
      "Benchmarking Inference eva_large_patch14_336.in22k_ft_in22k_in1k \n",
      "eva_large_patch14_336.in22k_ft_in22k_in1k model average inference time : 62.99717903137207ms\n",
      "Benchmarking Inference eva02_base_patch14_448.mim_in22k_ft_in1k \n",
      "eva02_base_patch14_448.mim_in22k_ft_in1k model average inference time : 64.43453788757324ms\n",
      "Benchmarking Inference caformer_b36.sail_in22k_ft_in1k_384 \n",
      "caformer_b36.sail_in22k_ft_in1k_384 model average inference time : 69.94579792022705ms\n",
      "Benchmarking Inference beit_large_patch16_512.in22k_ft_in22k_in1k \n",
      "beit_large_patch16_512.in22k_ft_in22k_in1k model average inference time : 191.15906715393066ms\n",
      "Benchmarking Inference convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384 \n",
      "convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384 model average inference time : 42.12807655334473ms\n",
      "Benchmarking Inference regnety_1280.swag_ft_in1k \n",
      "regnety_1280.swag_ft_in1k model average inference time : 168.71695041656494ms\n",
      "Benchmarking Inference convnext_xxlarge.clip_laion2b_soup_ft_in1k \n",
      "convnext_xxlarge.clip_laion2b_soup_ft_in1k model average inference time : 54.968552589416504ms\n",
      "Benchmarking Inference convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 \n",
      "convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 model average inference time : 41.94615840911865ms\n",
      "Benchmarking Inference volo_d5_512.sail_in1k \n",
      "volo_d5_512.sail_in1k model average inference time : 252.26200103759766ms\n",
      "Benchmarking Inference beit_large_patch16_384.in22k_ft_in22k_in1k \n",
      "beit_large_patch16_384.in22k_ft_in22k_in1k model average inference time : 80.67306995391846ms\n",
      "Benchmarking Inference maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k \n",
      "maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k model average inference time : 80.75275897979736ms\n",
      "Benchmarking Inference volo_d5_448.sail_in1k \n",
      "volo_d5_448.sail_in1k model average inference time : 181.4624261856079ms\n",
      "Benchmarking Inference tf_efficientnet_l2.ns_jft_in1k \n",
      "tf_efficientnet_l2.ns_jft_in1k model average inference time : 576.2402296066284ms\n",
      "Benchmarking Inference maxvit_base_tf_512.in21k_ft_in1k \n",
      "pass maxvit_base_tf_512.in21k_ft_in1k\n",
      "Benchmarking Inference eva_large_patch14_196.in22k_ft_in22k_in1k \n",
      "eva_large_patch14_196.in22k_ft_in22k_in1k model average inference time : 20.766191482543945ms\n",
      "Benchmarking Inference vit_large_patch14_clip_336.openai_ft_in12k_in1k \n",
      "vit_large_patch14_clip_336.openai_ft_in12k_in1k model average inference time : 63.34489822387695ms\n",
      "Benchmarking Inference convnextv2_huge.fcmae_ft_in22k_in1k_512 \n",
      "convnextv2_huge.fcmae_ft_in22k_in1k_512 model average inference time : 208.26961517333984ms\n",
      "Benchmarking Inference convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320 \n",
      "convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320 model average inference time : 28.03785800933838ms\n",
      "Benchmarking Inference tf_efficientnet_l2.ns_jft_in1k_475 \n",
      "tf_efficientnet_l2.ns_jft_in1k_475 model average inference time : 230.43800830841064ms\n",
      "Benchmarking Inference eva_large_patch14_196.in22k_ft_in1k \n",
      "eva_large_patch14_196.in22k_ft_in1k model average inference time : 20.69573402404785ms\n",
      "Benchmarking Inference volo_d4_448.sail_in1k \n",
      "volo_d4_448.sail_in1k model average inference time : 126.32205486297607ms\n",
      "Benchmarking Inference vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k \n",
      "vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k model average inference time : 117.63307094573975ms\n",
      "Benchmarking Inference maxvit_xlarge_tf_512.in21k_ft_in1k \n",
      "pass maxvit_xlarge_tf_512.in21k_ft_in1k\n",
      "Benchmarking Inference convnextv2_huge.fcmae_ft_in22k_in1k_384 \n",
      "convnextv2_huge.fcmae_ft_in22k_in1k_384 model average inference time : 119.21083927154541ms\n",
      "Benchmarking Inference convnext_xlarge.fb_in22k_ft_in1k_384 \n",
      "convnext_xlarge.fb_in22k_ft_in1k_384 model average inference time : 56.667842864990234ms\n",
      "Benchmarking Inference caformer_m36.sail_in22k_ft_in1k_384 \n",
      "caformer_m36.sail_in22k_ft_in1k_384 model average inference time : 52.81733512878418ms\n",
      "Benchmarking Inference convformer_b36.sail_in22k_ft_in1k_384 \n",
      "convformer_b36.sail_in22k_ft_in1k_384 model average inference time : 63.89447212219238ms\n",
      "Benchmarking Inference maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k \n",
      "maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k model average inference time : 120.13701438903809ms\n",
      "Benchmarking Inference caformer_b36.sail_in22k_ft_in1k \n",
      "caformer_b36.sail_in22k_ft_in1k model average inference time : 23.295392990112305ms\n",
      "Benchmarking Inference vit_large_patch14_clip_336.laion2b_ft_in12k_in1k \n",
      "vit_large_patch14_clip_336.laion2b_ft_in12k_in1k model average inference time : 63.05959224700928ms\n",
      "Benchmarking Inference vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k \n",
      "vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k model average inference time : 53.60339164733887ms\n",
      "Benchmarking Inference swinv2_base_window12to24_192to384.ms_in22k_ft_in1k \n",
      "swinv2_base_window12to24_192to384.ms_in22k_ft_in1k model average inference time : 75.2923583984375ms\n",
      "Benchmarking Inference vit_large_patch14_clip_224.openai_ft_in12k_in1k \n",
      "vit_large_patch14_clip_224.openai_ft_in12k_in1k model average inference time : 26.922550201416016ms\n",
      "Benchmarking Inference maxvit_xlarge_tf_384.in21k_ft_in1k \n",
      "pass maxvit_xlarge_tf_384.in21k_ft_in1k\n",
      "Benchmarking Inference coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k \n",
      "coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k model average inference time : 54.370665550231934ms\n",
      "Benchmarking Inference beit_base_patch16_384.in22k_ft_in22k_in1k \n",
      "beit_base_patch16_384.in22k_ft_in22k_in1k model average inference time : 31.253252029418945ms\n",
      "Benchmarking Inference convnextv2_large.fcmae_ft_in22k_in1k_384 \n",
      "convnextv2_large.fcmae_ft_in22k_in1k_384 model average inference time : 56.44268989562988ms\n",
      "Benchmarking Inference maxvit_large_tf_512.in21k_ft_in1k \n",
      "pass maxvit_large_tf_512.in21k_ft_in1k\n",
      "Benchmarking Inference maxvit_base_tf_384.in21k_ft_in1k \n",
      "pass maxvit_base_tf_384.in21k_ft_in1k\n",
      "Benchmarking Inference convnextv2_base.fcmae_ft_in22k_in1k_384 \n",
      "convnextv2_base.fcmae_ft_in22k_in1k_384 model average inference time : 35.88428974151611ms\n",
      "Benchmarking Inference beitv2_large_patch16_224.in1k_ft_in22k_in1k \n",
      "beitv2_large_patch16_224.in1k_ft_in22k_in1k model average inference time : 22.39323616027832ms\n",
      "Benchmarking Inference vit_large_patch14_clip_336.laion2b_ft_in1k \n",
      "vit_large_patch14_clip_336.laion2b_ft_in1k model average inference time : 62.644171714782715ms\n",
      "Benchmarking Inference convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384 \n",
      "convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384 model average inference time : 25.72718620300293ms\n",
      "Benchmarking Inference maxvit_large_tf_384.in21k_ft_in1k \n",
      "pass maxvit_large_tf_384.in21k_ft_in1k\n",
      "Benchmarking Inference vit_large_patch14_clip_224.laion2b_ft_in12k_in1k \n",
      "vit_large_patch14_clip_224.laion2b_ft_in12k_in1k model average inference time : 26.761474609375ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Inference convnext_large_mlp.clip_laion2b_augreg_ft_in1k \n",
      "convnext_large_mlp.clip_laion2b_augreg_ft_in1k model average inference time : 18.87547016143799ms\n",
      "Benchmarking Inference vit_large_patch14_clip_224.openai_ft_in1k \n",
      "vit_large_patch14_clip_224.openai_ft_in1k model average inference time : 26.78530216217041ms\n",
      "Benchmarking Inference caformer_s36.sail_in22k_ft_in1k_384 \n",
      "caformer_s36.sail_in22k_ft_in1k_384 model average inference time : 41.20553493499756ms\n",
      "Benchmarking Inference convnext_base.fb_in22k_ft_in1k_384 \n",
      "convnext_base.fb_in22k_ft_in1k_384 model average inference time : 25.742578506469727ms\n",
      "Benchmarking Inference convnext_large.fb_in22k_ft_in1k_384 \n",
      "convnext_large.fb_in22k_ft_in1k_384 model average inference time : 41.74063682556152ms\n",
      "Benchmarking Inference deit3_large_patch16_384.fb_in22k_ft_in1k \n",
      "deit3_large_patch16_384.fb_in22k_ft_in1k model average inference time : 64.4304370880127ms\n",
      "Benchmarking Inference dm_nfnet_f6.dm_in1k \n",
      "dm_nfnet_f6.dm_in1k model average inference time : 261.2537479400635ms\n",
      "Benchmarking Inference seresnextaa101d_32x8d.sw_in12k_ft_in1k_288 \n",
      "seresnextaa101d_32x8d.sw_in12k_ft_in1k_288 model average inference time : 33.421077728271484ms\n",
      "Benchmarking Inference deit3_huge_patch14_224.fb_in22k_ft_in1k \n",
      "deit3_huge_patch14_224.fb_in22k_ft_in1k model average inference time : 54.67087268829346ms\n",
      "Benchmarking Inference regnety_320.swag_ft_in1k \n",
      "regnety_320.swag_ft_in1k model average inference time : 51.15824222564697ms\n",
      "Benchmarking Inference vit_base_patch16_clip_384.laion2b_ft_in1k \n",
      "vit_base_patch16_clip_384.laion2b_ft_in1k model average inference time : 24.795684814453125ms\n",
      "Benchmarking Inference vit_large_patch16_384.augreg_in21k_ft_in1k \n",
      "vit_large_patch16_384.augreg_in21k_ft_in1k model average inference time : 63.250885009765625ms\n",
      "Benchmarking Inference convformer_m36.sail_in22k_ft_in1k_384 \n",
      "convformer_m36.sail_in22k_ft_in1k_384 model average inference time : 48.76349925994873ms\n",
      "Benchmarking Inference convnextv2_huge.fcmae_ft_in1k \n",
      "convnextv2_huge.fcmae_ft_in1k model average inference time : 71.82510375976562ms\n",
      "Benchmarking Inference tf_efficientnetv2_l.in21k_ft_in1k \n",
      "tf_efficientnetv2_l.in21k_ft_in1k model average inference time : 54.7554349899292ms\n",
      "Benchmarking Inference vit_base_patch16_clip_384.openai_ft_in12k_in1k \n",
      "vit_base_patch16_clip_384.openai_ft_in12k_in1k model average inference time : 24.799633026123047ms\n",
      "Benchmarking Inference convformer_b36.sail_in22k_ft_in1k \n",
      "convformer_b36.sail_in22k_ft_in1k model average inference time : 27.095818519592285ms\n",
      "Benchmarking Inference vit_base_patch16_clip_384.laion2b_ft_in12k_in1k \n",
      "vit_base_patch16_clip_384.laion2b_ft_in12k_in1k model average inference time : 24.79379653930664ms\n",
      "Benchmarking Inference cait_m48_448.fb_dist_in1k \n",
      "cait_m48_448.fb_dist_in1k model average inference time : 485.3704833984375ms\n",
      "Benchmarking Inference vit_huge_patch14_clip_224.laion2b_ft_in1k \n",
      "vit_huge_patch14_clip_224.laion2b_ft_in1k model average inference time : 54.03954029083252ms\n",
      "Benchmarking Inference volo_d3_448.sail_in1k \n",
      "volo_d3_448.sail_in1k model average inference time : 94.85130310058594ms\n",
      "Benchmarking Inference swinv2_large_window12to24_192to384.ms_in22k_ft_in1k \n",
      "swinv2_large_window12to24_192to384.ms_in22k_ft_in1k model average inference time : 114.09129619598389ms\n",
      "Benchmarking Inference beit_large_patch16_224.in22k_ft_in22k_in1k \n",
      "beit_large_patch16_224.in22k_ft_in22k_in1k model average inference time : 22.73066520690918ms\n",
      "Benchmarking Inference beitv2_large_patch16_224.in1k_ft_in1k \n",
      "beitv2_large_patch16_224.in1k_ft_in1k model average inference time : 22.65066146850586ms\n",
      "Benchmarking Inference tf_efficientnet_b7.ns_jft_in1k \n",
      "tf_efficientnet_b7.ns_jft_in1k model average inference time : 81.2997817993164ms\n",
      "Benchmarking Inference vit_large_patch14_clip_224.laion2b_ft_in1k \n",
      "vit_large_patch14_clip_224.laion2b_ft_in1k model average inference time : 27.08686351776123ms\n",
      "Benchmarking Inference convnextv2_base.fcmae_ft_in22k_in1k \n",
      "convnextv2_base.fcmae_ft_in22k_in1k model average inference time : 21.859440803527832ms\n",
      "Benchmarking Inference convnext_xlarge.fb_in22k_ft_in1k \n",
      "convnext_xlarge.fb_in22k_ft_in1k model average inference time : 36.38938903808594ms\n",
      "Benchmarking Inference caformer_b36.sail_in1k_384 \n",
      "caformer_b36.sail_in1k_384 model average inference time : 69.72106456756592ms\n",
      "Benchmarking Inference cait_m36_384.fb_dist_in1k \n",
      "cait_m36_384.fb_dist_in1k model average inference time : 209.58524227142334ms\n",
      "Benchmarking Inference convnextv2_large.fcmae_ft_in22k_in1k \n",
      "convnextv2_large.fcmae_ft_in22k_in1k model average inference time : 33.177433013916016ms\n",
      "Benchmarking Inference seresnextaa101d_32x8d.sw_in12k_ft_in1k \n",
      "seresnextaa101d_32x8d.sw_in12k_ft_in1k model average inference time : 29.057912826538086ms\n",
      "Benchmarking Inference tf_efficientnetv2_m.in21k_ft_in1k \n",
      "tf_efficientnetv2_m.in21k_ft_in1k model average inference time : 30.637574195861816ms\n",
      "Benchmarking Inference convformer_s36.sail_in22k_ft_in1k_384 \n",
      "convformer_s36.sail_in22k_ft_in1k_384 model average inference time : 37.4086332321167ms\n",
      "Benchmarking Inference swin_large_patch4_window12_384.ms_in22k_ft_in1k \n",
      "swin_large_patch4_window12_384.ms_in22k_ft_in1k model average inference time : 57.90896415710449ms\n",
      "Benchmarking Inference deit3_large_patch16_224.fb_in22k_ft_in1k \n",
      "deit3_large_patch16_224.fb_in22k_ft_in1k model average inference time : 21.118860244750977ms\n",
      "Benchmarking Inference convnext_base.clip_laiona_augreg_ft_in1k_384 \n",
      "convnext_base.clip_laiona_augreg_ft_in1k_384 model average inference time : 25.838193893432617ms\n",
      "Benchmarking Inference swin_base_patch4_window12_384.ms_in22k_ft_in1k \n",
      "swin_base_patch4_window12_384.ms_in22k_ft_in1k model average inference time : 37.124857902526855ms\n",
      "Benchmarking Inference vit_base_patch16_384.augreg_in21k_ft_in1k \n",
      "vit_base_patch16_384.augreg_in21k_ft_in1k model average inference time : 24.75289821624756ms\n",
      "Benchmarking Inference maxvit_base_tf_512.in1k \n",
      "pass maxvit_base_tf_512.in1k\n",
      "Benchmarking Inference caformer_m36.sail_in1k_384 \n",
      "caformer_m36.sail_in1k_384 model average inference time : 52.869019508361816ms\n",
      "Benchmarking Inference convnextv2_large.fcmae_ft_in1k \n",
      "convnextv2_large.fcmae_ft_in1k model average inference time : 33.02581787109375ms\n",
      "Benchmarking Inference maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k \n",
      "maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k model average inference time : 38.51691246032715ms\n",
      "Benchmarking Inference swinv2_large_window12to16_192to256.ms_in22k_ft_in1k \n",
      "swinv2_large_window12to16_192to256.ms_in22k_ft_in1k model average inference time : 34.15823459625244ms\n",
      "Benchmarking Inference regnety_160.swag_ft_in1k \n",
      "regnety_160.swag_ft_in1k model average inference time : 30.883865356445312ms\n",
      "Benchmarking Inference convnext_small.fb_in22k_ft_in1k_384 \n",
      "convnext_small.fb_in22k_ft_in1k_384 model average inference time : 18.99101734161377ms\n",
      "Benchmarking Inference efficientnet_b5.sw_in12k_ft_in1k \n",
      "efficientnet_b5.sw_in12k_ft_in1k model average inference time : 26.90467357635498ms\n",
      "Benchmarking Inference deit3_base_patch16_384.fb_in22k_ft_in1k \n",
      "deit3_base_patch16_384.fb_in22k_ft_in1k model average inference time : 25.240039825439453ms\n",
      "Benchmarking Inference xcit_large_24_p8_384.fb_dist_in1k \n",
      "xcit_large_24_p8_384.fb_dist_in1k model average inference time : 173.2826852798462ms\n",
      "Benchmarking Inference volo_d5_224.sail_in1k \n",
      "volo_d5_224.sail_in1k model average inference time : 36.49049758911133ms\n",
      "Benchmarking Inference convnext_large.fb_in22k_ft_in1k \n",
      "convnext_large.fb_in22k_ft_in1k model average inference time : 24.340882301330566ms\n",
      "Benchmarking Inference swinv2_base_window12to16_192to256.ms_in22k_ft_in1k \n",
      "swinv2_base_window12to16_192to256.ms_in22k_ft_in1k model average inference time : 22.624244689941406ms\n",
      "Benchmarking Inference convnext_base.fb_in22k_ft_in1k \n",
      "convnext_base.fb_in22k_ft_in1k model average inference time : 15.66577434539795ms\n",
      "Benchmarking Inference caformer_m36.sail_in22k_ft_in1k \n",
      "caformer_m36.sail_in22k_ft_in1k model average inference time : 17.474775314331055ms\n",
      "Benchmarking Inference cait_s36_384.fb_dist_in1k \n",
      "cait_s36_384.fb_dist_in1k model average inference time : 101.8310022354126ms\n",
      "Benchmarking Inference coatnet_2_rw_224.sw_in12k_ft_in1k \n",
      "coatnet_2_rw_224.sw_in12k_ft_in1k model average inference time : 18.44301700592041ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Inference convformer_m36.sail_in22k_ft_in1k \n",
      "convformer_m36.sail_in22k_ft_in1k model average inference time : 19.892358779907227ms\n",
      "Benchmarking Inference xcit_medium_24_p8_384.fb_dist_in1k \n",
      "xcit_medium_24_p8_384.fb_dist_in1k model average inference time : 108.35433959960938ms\n",
      "Benchmarking Inference convnext_base.clip_laion2b_augreg_ft_in12k_in1k \n",
      "convnext_base.clip_laion2b_augreg_ft_in12k_in1k model average inference time : 11.771078109741211ms\n",
      "Benchmarking Inference volo_d4_224.sail_in1k \n",
      "volo_d4_224.sail_in1k model average inference time : 24.9357271194458ms\n",
      "Benchmarking Inference maxvit_large_tf_512.in1k \n",
      "pass maxvit_large_tf_512.in1k\n",
      "Benchmarking Inference vit_large_r50_s32_384.augreg_in21k_ft_in1k \n",
      "vit_large_r50_s32_384.augreg_in21k_ft_in1k model average inference time : 29.021129608154297ms\n",
      "Benchmarking Inference volo_d2_384.sail_in1k \n",
      "volo_d2_384.sail_in1k model average inference time : 42.049713134765625ms\n",
      "Benchmarking Inference swin_large_patch4_window7_224.ms_in22k_ft_in1k \n",
      "swin_large_patch4_window7_224.ms_in22k_ft_in1k model average inference time : 18.594460487365723ms\n",
      "Benchmarking Inference tf_efficientnet_b6.ns_jft_in1k \n",
      "tf_efficientnet_b6.ns_jft_in1k model average inference time : 48.62365245819092ms\n",
      "Benchmarking Inference coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k \n",
      "coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k model average inference time : 18.89118194580078ms\n",
      "Benchmarking Inference tf_efficientnetv2_xl.in21k_ft_in1k \n",
      "tf_efficientnetv2_xl.in21k_ft_in1k model average inference time : 84.6269702911377ms\n",
      "Benchmarking Inference maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k \n",
      "maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k model average inference time : 79.76803302764893ms\n",
      "Benchmarking Inference beitv2_base_patch16_224.in1k_ft_in22k_in1k \n",
      "beitv2_base_patch16_224.in1k_ft_in22k_in1k model average inference time : 7.904720306396484ms\n",
      "Benchmarking Inference dm_nfnet_f5.dm_in1k \n",
      "dm_nfnet_f5.dm_in1k model average inference time : 211.79538249969482ms\n",
      "Benchmarking Inference dm_nfnet_f4.dm_in1k \n",
      "dm_nfnet_f4.dm_in1k model average inference time : 161.23919010162354ms\n",
      "Benchmarking Inference xcit_small_24_p8_384.fb_dist_in1k \n",
      "xcit_small_24_p8_384.fb_dist_in1k model average inference time : 80.67288875579834ms\n",
      "Benchmarking Inference regnety_160.lion_in12k_ft_in1k \n",
      "regnety_160.lion_in12k_ft_in1k model average inference time : 21.250972747802734ms\n",
      "Benchmarking Inference caformer_s18.sail_in22k_ft_in1k_384 \n",
      "caformer_s18.sail_in22k_ft_in1k_384 model average inference time : 22.870726585388184ms\n",
      "Benchmarking Inference vit_base_patch8_224.augreg2_in21k_ft_in1k \n",
      "vit_base_patch8_224.augreg2_in21k_ft_in1k model average inference time : 35.424509048461914ms\n",
      "Benchmarking Inference convformer_m36.sail_in1k_384 \n",
      "convformer_m36.sail_in1k_384 model average inference time : 48.889498710632324ms\n",
      "Benchmarking Inference vit_base_patch16_clip_384.openai_ft_in1k \n",
      "vit_base_patch16_clip_384.openai_ft_in1k model average inference time : 24.94328022003174ms\n",
      "Benchmarking Inference caformer_s36.sail_in22k_ft_in1k \n",
      "caformer_s36.sail_in22k_ft_in1k model average inference time : 16.176657676696777ms\n",
      "Benchmarking Inference volo_d1_384.sail_in1k \n",
      "volo_d1_384.sail_in1k model average inference time : 24.760408401489258ms\n",
      "Benchmarking Inference deit3_large_patch16_384.fb_in1k \n",
      "deit3_large_patch16_384.fb_in1k model average inference time : 64.88831520080566ms\n",
      "Benchmarking Inference caformer_s36.sail_in1k_384 \n",
      "caformer_s36.sail_in1k_384 model average inference time : 41.4231538772583ms\n",
      "Benchmarking Inference xcit_large_24_p16_384.fb_dist_in1k \n",
      "xcit_large_24_p16_384.fb_dist_in1k model average inference time : 50.99280834197998ms\n",
      "Benchmarking Inference convformer_b36.sail_in1k_384 \n",
      "convformer_b36.sail_in1k_384 model average inference time : 64.12924766540527ms\n",
      "Benchmarking Inference tf_efficientnet_b5.ns_jft_in1k \n",
      "tf_efficientnet_b5.ns_jft_in1k model average inference time : 29.551067352294922ms\n",
      "Benchmarking Inference dm_nfnet_f3.dm_in1k \n",
      "dm_nfnet_f3.dm_in1k model average inference time : 97.38987445831299ms\n",
      "Benchmarking Inference convnext_base.clip_laion2b_augreg_ft_in1k \n",
      "convnext_base.clip_laion2b_augreg_ft_in1k model average inference time : 11.782488822937012ms\n",
      "Benchmarking Inference regnety_2560.seer_ft_in1k \n",
      "regnety_2560.seer_ft_in1k model average inference time : 272.9004383087158ms\n",
      "Benchmarking Inference convnext_small.in12k_ft_in1k_384 \n",
      "convnext_small.in12k_ft_in1k_384 model average inference time : 19.088926315307617ms\n",
      "Benchmarking Inference maxvit_base_tf_384.in1k \n",
      "pass maxvit_base_tf_384.in1k\n",
      "Benchmarking Inference tf_efficientnet_b8.ap_in1k \n",
      "tf_efficientnet_b8.ap_in1k model average inference time : 122.60839939117432ms\n",
      "Benchmarking Inference regnety_160.sw_in12k_ft_in1k \n",
      "regnety_160.sw_in12k_ft_in1k model average inference time : 21.192007064819336ms\n",
      "Benchmarking Inference vit_base_patch16_clip_224.laion2b_ft_in12k_in1k \n",
      "vit_base_patch16_clip_224.laion2b_ft_in12k_in1k model average inference time : 7.2327375411987305ms\n",
      "Benchmarking Inference maxvit_tiny_tf_512.in1k \n",
      "pass maxvit_tiny_tf_512.in1k\n",
      "Benchmarking Inference convformer_s18.sail_in22k_ft_in1k_384 \n",
      "convformer_s18.sail_in22k_ft_in1k_384 model average inference time : 21.111130714416504ms\n",
      "Benchmarking Inference volo_d3_224.sail_in1k \n",
      "volo_d3_224.sail_in1k model average inference time : 17.56282329559326ms\n",
      "Benchmarking Inference maxvit_large_tf_384.in1k \n",
      "pass maxvit_large_tf_384.in1k\n",
      "Benchmarking Inference tf_efficientnetv2_l.in1k \n",
      "tf_efficientnetv2_l.in1k model average inference time : 54.75166320800781ms\n",
      "Benchmarking Inference flexivit_large.1200ep_in1k \n",
      "flexivit_large.1200ep_in1k model average inference time : 24.442949295043945ms\n",
      "Benchmarking Inference xcit_large_24_p8_224.fb_dist_in1k \n",
      "xcit_large_24_p8_224.fb_dist_in1k model average inference time : 69.5625114440918ms\n",
      "Benchmarking Inference flexivit_large.600ep_in1k \n",
      "flexivit_large.600ep_in1k model average inference time : 24.433903694152832ms\n",
      "Benchmarking Inference xcit_small_12_p8_384.fb_dist_in1k \n",
      "xcit_small_12_p8_384.fb_dist_in1k model average inference time : 42.785401344299316ms\n",
      "Benchmarking Inference cait_s24_384.fb_dist_in1k \n",
      "cait_s24_384.fb_dist_in1k model average inference time : 68.5398530960083ms\n",
      "Benchmarking Inference convformer_s36.sail_in22k_ft_in1k \n",
      "convformer_s36.sail_in22k_ft_in1k model average inference time : 18.201823234558105ms\n",
      "Benchmarking Inference convnextv2_tiny.fcmae_ft_in22k_in1k_384 \n",
      "convnextv2_tiny.fcmae_ft_in22k_in1k_384 model average inference time : 17.601213455200195ms\n",
      "Benchmarking Inference convformer_s36.sail_in1k_384 \n",
      "convformer_s36.sail_in1k_384 model average inference time : 37.34050750732422ms\n",
      "Benchmarking Inference xcit_medium_24_p16_384.fb_dist_in1k \n",
      "xcit_medium_24_p16_384.fb_dist_in1k model average inference time : 32.19667911529541ms\n",
      "Benchmarking Inference convnext_tiny.in12k_ft_in1k_384 \n",
      "convnext_tiny.in12k_ft_in1k_384 model average inference time : 12.764992713928223ms\n",
      "Benchmarking Inference deit3_base_patch16_224.fb_in22k_ft_in1k \n",
      "deit3_base_patch16_224.fb_in22k_ft_in1k model average inference time : 7.408466339111328ms\n",
      "Benchmarking Inference vit_base_patch16_224.augreg2_in21k_ft_in1k \n",
      "vit_base_patch16_224.augreg2_in21k_ft_in1k model average inference time : 7.2542619705200195ms\n",
      "Benchmarking Inference maxvit_small_tf_512.in1k \n",
      "pass maxvit_small_tf_512.in1k\n",
      "Benchmarking Inference vit_base_patch32_clip_448.laion2b_ft_in12k_in1k \n",
      "vit_base_patch32_clip_448.laion2b_ft_in12k_in1k model average inference time : 8.94580364227295ms\n",
      "Benchmarking Inference regnety_120.sw_in12k_ft_in1k \n",
      "regnety_120.sw_in12k_ft_in1k model average inference time : 17.249088287353516ms\n",
      "Benchmarking Inference vit_base_patch8_224.augreg_in21k_ft_in1k \n",
      "vit_base_patch8_224.augreg_in21k_ft_in1k model average inference time : 35.202903747558594ms\n",
      "Benchmarking Inference deit_base_distilled_patch16_384.fb_in1k \n",
      "deit_base_distilled_patch16_384.fb_in1k model average inference time : 24.72419261932373ms\n",
      "Benchmarking Inference tf_efficientnet_b7.ap_in1k \n",
      "tf_efficientnet_b7.ap_in1k model average inference time : 81.04320049285889ms\n",
      "Benchmarking Inference vit_base_patch16_clip_224.laion2b_ft_in1k \n",
      "vit_base_patch16_clip_224.laion2b_ft_in1k model average inference time : 7.197971343994141ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Inference convnextv2_base.fcmae_ft_in1k \n",
      "convnextv2_base.fcmae_ft_in1k model average inference time : 21.80408477783203ms\n",
      "Benchmarking Inference caformer_b36.sail_in1k \n",
      "caformer_b36.sail_in1k model average inference time : 23.217487335205078ms\n",
      "Benchmarking Inference vit_base_patch16_clip_224.openai_ft_in12k_in1k \n",
      "vit_base_patch16_clip_224.openai_ft_in12k_in1k model average inference time : 7.255816459655762ms\n",
      "Benchmarking Inference hrnet_w48_ssld.paddle_in1k \n",
      "hrnet_w48_ssld.paddle_in1k model average inference time : 32.19280242919922ms\n",
      "Benchmarking Inference beit_base_patch16_224.in22k_ft_in22k_in1k \n",
      "beit_base_patch16_224.in22k_ft_in22k_in1k model average inference time : 7.8772735595703125ms\n",
      "Benchmarking Inference regnetz_e8.ra3_in1k \n",
      "regnetz_e8.ra3_in1k model average inference time : 29.124879837036133ms\n",
      "Benchmarking Inference deit3_small_patch16_384.fb_in22k_ft_in1k \n",
      "deit3_small_patch16_384.fb_in22k_ft_in1k model average inference time : 11.160078048706055ms\n",
      "Benchmarking Inference vit_medium_patch16_gap_384.sw_in12k_ft_in1k \n",
      "vit_medium_patch16_gap_384.sw_in12k_ft_in1k model average inference time : 15.053777694702148ms\n",
      "Benchmarking Inference tf_efficientnetv2_m.in1k \n",
      "tf_efficientnetv2_m.in1k model average inference time : 30.609455108642578ms\n",
      "Benchmarking Inference tf_efficientnet_b8.ra_in1k \n",
      "tf_efficientnet_b8.ra_in1k model average inference time : 122.40338802337646ms\n",
      "Benchmarking Inference tf_efficientnet_b6.ap_in1k \n",
      "tf_efficientnet_b6.ap_in1k model average inference time : 48.698787689208984ms\n",
      "Benchmarking Inference volo_d2_224.sail_in1k \n",
      "volo_d2_224.sail_in1k model average inference time : 12.503161430358887ms\n",
      "Benchmarking Inference caformer_s18.sail_in1k_384 \n",
      "caformer_s18.sail_in1k_384 model average inference time : 22.83780574798584ms\n",
      "Benchmarking Inference eva02_small_patch14_336.mim_in22k_ft_in1k \n",
      "eva02_small_patch14_336.mim_in22k_ft_in1k model average inference time : 13.184804916381836ms\n",
      "Benchmarking Inference vit_large_patch16_224.augreg_in21k_ft_in1k \n",
      "vit_large_patch16_224.augreg_in21k_ft_in1k model average inference time : 20.69068431854248ms\n",
      "Benchmarking Inference flexivit_large.300ep_in1k \n",
      "flexivit_large.300ep_in1k model average inference time : 24.44446563720703ms\n",
      "Benchmarking Inference tf_efficientnet_b4.ns_jft_in1k \n",
      "tf_efficientnet_b4.ns_jft_in1k model average inference time : 16.13293170928955ms\n",
      "Benchmarking Inference convnext_small.fb_in22k_ft_in1k \n",
      "convnext_small.fb_in22k_ft_in1k model average inference time : 11.204524040222168ms\n",
      "Benchmarking Inference xcit_small_24_p16_384.fb_dist_in1k \n",
      "xcit_small_24_p16_384.fb_dist_in1k model average inference time : 23.186001777648926ms\n",
      "Benchmarking Inference xcit_medium_24_p8_224.fb_dist_in1k \n",
      "xcit_medium_24_p8_224.fb_dist_in1k model average inference time : 43.39779853820801ms\n",
      "Benchmarking Inference beitv2_base_patch16_224.in1k_ft_in1k \n",
      "beitv2_base_patch16_224.in1k_ft_in1k model average inference time : 7.904620170593262ms\n",
      "Benchmarking Inference deit3_huge_patch14_224.fb_in1k \n",
      "deit3_huge_patch14_224.fb_in1k model average inference time : 54.60422992706299ms\n",
      "Benchmarking Inference coat_lite_medium_384.in1k \n",
      "coat_lite_medium_384.in1k model average inference time : 36.5740442276001ms\n",
      "Benchmarking Inference xcit_small_24_p8_224.fb_dist_in1k \n",
      "xcit_small_24_p8_224.fb_dist_in1k model average inference time : 29.130005836486816ms\n",
      "Benchmarking Inference xcit_small_12_p16_384.fb_dist_in1k \n",
      "xcit_small_12_p16_384.fb_dist_in1k model average inference time : 13.527212142944336ms\n",
      "Benchmarking Inference vit_base_patch32_clip_384.laion2b_ft_in12k_in1k \n",
      "vit_base_patch32_clip_384.laion2b_ft_in12k_in1k model average inference time : 7.342848777770996ms\n",
      "Benchmarking Inference dm_nfnet_f2.dm_in1k \n",
      "dm_nfnet_f2.dm_in1k model average inference time : 59.428324699401855ms\n",
      "Benchmarking Inference swin_base_patch4_window7_224.ms_in22k_ft_in1k \n",
      "swin_base_patch4_window7_224.ms_in22k_ft_in1k model average inference time : 12.82036304473877ms\n",
      "Benchmarking Inference vit_base_patch16_clip_224.openai_ft_in1k \n",
      "vit_base_patch16_clip_224.openai_ft_in1k model average inference time : 7.216582298278809ms\n",
      "Benchmarking Inference eca_nfnet_l2.ra3_in1k \n",
      "eca_nfnet_l2.ra3_in1k model average inference time : 37.229371070861816ms\n",
      "Benchmarking Inference cait_xs24_384.fb_dist_in1k \n",
      "cait_xs24_384.fb_dist_in1k model average inference time : 49.75600242614746ms\n",
      "Benchmarking Inference convformer_s18.sail_in1k_384 \n",
      "convformer_s18.sail_in1k_384 model average inference time : 21.09790325164795ms\n",
      "Benchmarking Inference maxvit_tiny_tf_384.in1k \n",
      "pass maxvit_tiny_tf_384.in1k\n",
      "Benchmarking Inference maxvit_small_tf_384.in1k \n",
      "pass maxvit_small_tf_384.in1k\n",
      "Benchmarking Inference resnext101_32x32d.fb_wsl_ig1b_ft_in1k \n",
      "resnext101_32x32d.fb_wsl_ig1b_ft_in1k model average inference time : 67.65288352966309ms\n",
      "Benchmarking Inference tf_efficientnet_b7.ra_in1k \n",
      "tf_efficientnet_b7.ra_in1k model average inference time : 81.0150957107544ms\n",
      "Benchmarking Inference ecaresnet269d.ra2_in1k \n",
      "ecaresnet269d.ra2_in1k model average inference time : 40.4368257522583ms\n",
      "Benchmarking Inference regnety_1280.seer_ft_in1k \n",
      "regnety_1280.seer_ft_in1k model average inference time : 168.409686088562ms\n",
      "Benchmarking Inference vit_base_patch32_clip_384.openai_ft_in12k_in1k \n",
      "vit_base_patch32_clip_384.openai_ft_in12k_in1k model average inference time : 7.444438934326172ms\n",
      "Benchmarking Inference xcit_large_24_p16_224.fb_dist_in1k \n",
      "xcit_large_24_p16_224.fb_dist_in1k model average inference time : 18.431344032287598ms\n",
      "Benchmarking Inference convnext_small.in12k_ft_in1k \n",
      "convnext_small.in12k_ft_in1k model average inference time : 11.193723678588867ms\n",
      "Benchmarking Inference dm_nfnet_f1.dm_in1k \n",
      "dm_nfnet_f1.dm_in1k model average inference time : 35.44049263000488ms\n",
      "Benchmarking Inference resmlp_big_24_224.fb_in22k_ft_in1k \n",
      "resmlp_big_24_224.fb_in22k_ft_in1k model average inference time : 34.073710441589355ms\n",
      "Benchmarking Inference xcit_small_12_p8_224.fb_dist_in1k \n",
      "xcit_small_12_p8_224.fb_dist_in1k model average inference time : 15.74880599975586ms\n",
      "Benchmarking Inference convnext_large.fb_in1k \n",
      "convnext_large.fb_in1k model average inference time : 24.32772159576416ms\n",
      "Benchmarking Inference caformer_m36.sail_in1k \n",
      "caformer_m36.sail_in1k model average inference time : 17.432398796081543ms\n",
      "Benchmarking Inference efficientnetv2_rw_m.agc_in1k \n",
      "efficientnetv2_rw_m.agc_in1k model average inference time : 27.33175754547119ms\n",
      "Benchmarking Inference regnety_1280.swag_lc_in1k \n",
      "regnety_1280.swag_lc_in1k model average inference time : 70.75120449066162ms\n",
      "Benchmarking Inference regnetz_d8_evos.ch_in1k \n",
      "regnetz_d8_evos.ch_in1k model average inference time : 31.721558570861816ms\n",
      "Benchmarking Inference regnetz_040_h.ra3_in1k \n",
      "regnetz_040_h.ra3_in1k model average inference time : 16.975302696228027ms\n",
      "Benchmarking Inference edgenext_base.in21k_ft_in1k \n",
      "edgenext_base.in21k_ft_in1k model average inference time : 78.00333023071289ms\n",
      "Benchmarking Inference tf_efficientnet_b5.ap_in1k \n",
      "tf_efficientnet_b5.ap_in1k model average inference time : 29.445910453796387ms\n",
      "Benchmarking Inference mvitv2_large.fb_in1k \n",
      "mvitv2_large.fb_in1k model average inference time : 49.111666679382324ms\n",
      "Benchmarking Inference caformer_s18.sail_in22k_ft_in1k \n",
      "caformer_s18.sail_in22k_ft_in1k model average inference time : 9.181790351867676ms\n",
      "Benchmarking Inference deit3_base_patch16_384.fb_in1k \n",
      "deit3_base_patch16_384.fb_in1k model average inference time : 25.33609390258789ms\n",
      "Benchmarking Inference deit3_medium_patch16_224.fb_in22k_ft_in1k \n",
      "deit3_medium_patch16_224.fb_in22k_ft_in1k model average inference time : 5.5593109130859375ms\n",
      "Benchmarking Inference volo_d1_224.sail_in1k \n",
      "volo_d1_224.sail_in1k model average inference time : 9.429736137390137ms\n",
      "Benchmarking Inference convnext_tiny.in12k_ft_in1k \n",
      "convnext_tiny.in12k_ft_in1k model average inference time : 7.422780990600586ms\n",
      "Benchmarking Inference tf_efficientnetv2_s.in21k_ft_in1k \n",
      "tf_efficientnetv2_s.in21k_ft_in1k model average inference time : 16.528091430664062ms\n",
      "Benchmarking Inference vit_base_patch16_224.augreg_in21k_ft_in1k \n",
      "vit_base_patch16_224.augreg_in21k_ft_in1k model average inference time : 7.22592830657959ms\n",
      "Benchmarking Inference convnext_tiny.fb_in22k_ft_in1k_384 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convnext_tiny.fb_in22k_ft_in1k_384 model average inference time : 12.775044441223145ms\n",
      "Benchmarking Inference regnetz_d8.ra3_in1k \n",
      "regnetz_d8.ra3_in1k model average inference time : 15.848784446716309ms\n",
      "Benchmarking Inference convformer_b36.sail_in1k \n",
      "convformer_b36.sail_in1k model average inference time : 27.05242156982422ms\n",
      "Benchmarking Inference coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k \n",
      "coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k model average inference time : 15.46870231628418ms\n",
      "Benchmarking Inference resnetrs420.tf_in1k \n",
      "resnetrs420.tf_in1k model average inference time : 93.55855941772461ms\n",
      "Benchmarking Inference resnext101_32x16d.fb_wsl_ig1b_ft_in1k \n",
      "resnext101_32x16d.fb_wsl_ig1b_ft_in1k model average inference time : 30.859827995300293ms\n",
      "Benchmarking Inference resnetrs270.tf_in1k \n",
      "resnetrs270.tf_in1k model average inference time : 46.121697425842285ms\n",
      "Benchmarking Inference vit_small_r26_s32_384.augreg_in21k_ft_in1k \n",
      "vit_small_r26_s32_384.augreg_in21k_ft_in1k model average inference time : 11.600751876831055ms\n",
      "Benchmarking Inference swin_small_patch4_window7_224.ms_in22k_ft_in1k \n",
      "swin_small_patch4_window7_224.ms_in22k_ft_in1k model average inference time : 12.54335880279541ms\n",
      "Benchmarking Inference vit_base_r50_s16_384.orig_in21k_ft_in1k \n",
      "vit_base_r50_s16_384.orig_in21k_ft_in1k model average inference time : 36.59476280212402ms\n",
      "Benchmarking Inference xcit_medium_24_p16_224.fb_dist_in1k \n",
      "xcit_medium_24_p16_224.fb_dist_in1k model average inference time : 18.222661018371582ms\n",
      "Benchmarking Inference tf_efficientnet_b7.aa_in1k \n",
      "tf_efficientnet_b7.aa_in1k model average inference time : 81.05557918548584ms\n",
      "Benchmarking Inference maxxvit_rmlp_small_rw_256.sw_in1k \n",
      "maxxvit_rmlp_small_rw_256.sw_in1k model average inference time : 61.75087928771973ms\n",
      "Benchmarking Inference seresnet152d.ra2_in1k \n",
      "seresnet152d.ra2_in1k model average inference time : 24.485087394714355ms\n",
      "Benchmarking Inference xcit_tiny_24_p8_384.fb_dist_in1k \n",
      "xcit_tiny_24_p8_384.fb_dist_in1k model average inference time : 43.99637699127197ms\n",
      "Benchmarking Inference convformer_m36.sail_in1k \n",
      "convformer_m36.sail_in1k model average inference time : 19.98155117034912ms\n",
      "Benchmarking Inference resnext101_32x8d.fb_swsl_ig1b_ft_in1k \n",
      "resnext101_32x8d.fb_swsl_ig1b_ft_in1k model average inference time : 17.562475204467773ms\n",
      "Benchmarking Inference resnetrs200.tf_in1k \n",
      "resnetrs200.tf_in1k model average inference time : 32.33295917510986ms\n",
      "Benchmarking Inference convnext_base.fb_in1k \n",
      "convnext_base.fb_in1k model average inference time : 15.703659057617188ms\n",
      "Benchmarking Inference deit3_large_patch16_224.fb_in1k \n",
      "deit3_large_patch16_224.fb_in1k model average inference time : 21.117572784423828ms\n",
      "Benchmarking Inference tf_efficientnet_b6.aa_in1k \n",
      "tf_efficientnet_b6.aa_in1k model average inference time : 48.68879318237305ms\n",
      "Benchmarking Inference resnetrs350.tf_in1k \n",
      "resnetrs350.tf_in1k model average inference time : 65.29199123382568ms\n",
      "Benchmarking Inference rexnetr_300.sw_in12k_ft_in1k \n",
      "rexnetr_300.sw_in12k_ft_in1k model average inference time : 10.839242935180664ms\n",
      "Benchmarking Inference caformer_s36.sail_in1k \n",
      "caformer_s36.sail_in1k model average inference time : 17.016444206237793ms\n",
      "Benchmarking Inference convnextv2_tiny.fcmae_ft_in22k_in1k \n",
      "convnextv2_tiny.fcmae_ft_in22k_in1k model average inference time : 10.33353328704834ms\n",
      "Benchmarking Inference vit_base_patch16_224_miil.in21k_ft_in1k \n",
      "vit_base_patch16_224_miil.in21k_ft_in1k model average inference time : 7.078952789306641ms\n",
      "Benchmarking Inference edgenext_base.usi_in1k \n",
      "edgenext_base.usi_in1k model average inference time : 78.02728652954102ms\n",
      "Benchmarking Inference regnetz_040.ra3_in1k \n",
      "regnetz_040.ra3_in1k model average inference time : 16.674165725708008ms\n",
      "Benchmarking Inference convformer_s18.sail_in22k_ft_in1k \n",
      "convformer_s18.sail_in22k_ft_in1k model average inference time : 9.89673137664795ms\n",
      "Benchmarking Inference resnetv2_152x2_bit.goog_in21k_ft_in1k \n",
      "resnetv2_152x2_bit.goog_in21k_ft_in1k model average inference time : 93.25749397277832ms\n",
      "Benchmarking Inference regnety_160.deit_in1k \n",
      "regnety_160.deit_in1k model average inference time : 21.190643310546875ms\n",
      "Benchmarking Inference davit_base.msft_in1k \n",
      "davit_base.msft_in1k model average inference time : 13.799924850463867ms\n",
      "Benchmarking Inference regnety_640.seer_ft_in1k \n",
      "regnety_640.seer_ft_in1k model average inference time : 85.89212894439697ms\n",
      "Benchmarking Inference vit_small_patch16_384.augreg_in21k_ft_in1k \n",
      "vit_small_patch16_384.augreg_in21k_ft_in1k model average inference time : 11.35932445526123ms\n",
      "Benchmarking Inference regnetz_d32.ra3_in1k \n",
      "regnetz_d32.ra3_in1k model average inference time : 15.807938575744629ms\n",
      "Benchmarking Inference flexivit_base.1200ep_in1k \n",
      "flexivit_base.1200ep_in1k model average inference time : 8.583693504333496ms\n",
      "Benchmarking Inference mvitv2_base.fb_in1k \n",
      "mvitv2_base.fb_in1k model average inference time : 21.49949073791504ms\n",
      "Benchmarking Inference regnety_080.ra3_in1k \n",
      "regnety_080.ra3_in1k model average inference time : 14.379606246948242ms\n",
      "Benchmarking Inference vit_medium_patch16_gap_256.sw_in12k_ft_in1k \n",
      "vit_medium_patch16_gap_256.sw_in12k_ft_in1k model average inference time : 6.27321720123291ms\n",
      "Benchmarking Inference davit_small.msft_in1k \n",
      "davit_small.msft_in1k model average inference time : 14.422059059143066ms\n",
      "Benchmarking Inference eca_nfnet_l1.ra2_in1k \n",
      "eca_nfnet_l1.ra2_in1k model average inference time : 20.871601104736328ms\n",
      "Benchmarking Inference swinv2_base_window16_256.ms_in1k \n",
      "swinv2_base_window16_256.ms_in1k model average inference time : 22.609152793884277ms\n",
      "Benchmarking Inference maxvit_base_tf_224.in1k \n",
      "pass maxvit_base_tf_224.in1k\n",
      "Benchmarking Inference regnety_320.seer_ft_in1k \n",
      "regnety_320.seer_ft_in1k model average inference time : 51.200881004333496ms\n"
     ]
    }
   ],
   "source": [
    "modellist = df_models[\"model\"]\n",
    "benchmark = {}\n",
    "\n",
    "# inference float precision\n",
    "for i,modelname in tqdm(enumerate((modellist))):\n",
    "    imsize = int(df_models[df_models[\"model\"]==modelname][\"img_size\"])\n",
    "    try:\n",
    "        benchmark = inference_imsize(modelname, benchmark, imsize)\n",
    "    except:\n",
    "        print(\"pass {}\".format(modelname))\n",
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(benchmark).T\n",
    "df_results\n",
    "df_results.to_csv(\"results_fp32_imsizeall.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
