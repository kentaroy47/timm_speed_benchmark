{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "#import torch.utils.benchmark as benchmark\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WARM_UP = 10\n",
    "BATCH_SIZE = 4\n",
    "NUM_TEST = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 14:44:46 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.82.00    Driver Version: 470.82.00    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:09:00.0 Off |                  Off |\n",
      "| 30%   47C    P8    17W / 300W |     33MiB / 48682MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1262      G   /usr/lib/xorg/Xorg                 23MiB |\n",
      "|    0   N/A  N/A      1703      G   /usr/bin/gnome-shell                8MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>top1</th>\n",
       "      <th>top1_err</th>\n",
       "      <th>top5</th>\n",
       "      <th>top5_err</th>\n",
       "      <th>param_count</th>\n",
       "      <th>img_size</th>\n",
       "      <th>cropt_pct</th>\n",
       "      <th>interpolation</th>\n",
       "      <th>top1_diff</th>\n",
       "      <th>top5_diff</th>\n",
       "      <th>rank_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beit_large_patch16_512</td>\n",
       "      <td>90.695</td>\n",
       "      <td>9.305</td>\n",
       "      <td>98.770</td>\n",
       "      <td>1.230</td>\n",
       "      <td>305.67</td>\n",
       "      <td>512</td>\n",
       "      <td>1.000</td>\n",
       "      <td>bicubic</td>\n",
       "      <td>2.111</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beit_large_patch16_384</td>\n",
       "      <td>90.601</td>\n",
       "      <td>9.399</td>\n",
       "      <td>98.777</td>\n",
       "      <td>1.223</td>\n",
       "      <td>305.00</td>\n",
       "      <td>384</td>\n",
       "      <td>1.000</td>\n",
       "      <td>bicubic</td>\n",
       "      <td>2.219</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tf_efficientnet_l2_ns</td>\n",
       "      <td>90.572</td>\n",
       "      <td>9.428</td>\n",
       "      <td>98.779</td>\n",
       "      <td>1.221</td>\n",
       "      <td>480.31</td>\n",
       "      <td>800</td>\n",
       "      <td>0.960</td>\n",
       "      <td>bicubic</td>\n",
       "      <td>2.226</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tf_efficientnet_l2_ns_475</td>\n",
       "      <td>90.527</td>\n",
       "      <td>9.473</td>\n",
       "      <td>98.706</td>\n",
       "      <td>1.294</td>\n",
       "      <td>480.31</td>\n",
       "      <td>475</td>\n",
       "      <td>0.936</td>\n",
       "      <td>bicubic</td>\n",
       "      <td>2.289</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beit_base_patch16_384</td>\n",
       "      <td>90.388</td>\n",
       "      <td>9.612</td>\n",
       "      <td>98.730</td>\n",
       "      <td>1.270</td>\n",
       "      <td>86.74</td>\n",
       "      <td>384</td>\n",
       "      <td>1.000</td>\n",
       "      <td>bicubic</td>\n",
       "      <td>3.580</td>\n",
       "      <td>0.590</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model    top1  top1_err    top5  top5_err  param_count  \\\n",
       "0     beit_large_patch16_512  90.695     9.305  98.770     1.230       305.67   \n",
       "1     beit_large_patch16_384  90.601     9.399  98.777     1.223       305.00   \n",
       "2      tf_efficientnet_l2_ns  90.572     9.428  98.779     1.221       480.31   \n",
       "3  tf_efficientnet_l2_ns_475  90.527     9.473  98.706     1.294       480.31   \n",
       "4      beit_base_patch16_384  90.388     9.612  98.730     1.270        86.74   \n",
       "\n",
       "   img_size  cropt_pct interpolation  top1_diff  top5_diff  rank_diff  \n",
       "0       512      1.000       bicubic      2.111      0.110          0  \n",
       "1       384      1.000       bicubic      2.219      0.169          0  \n",
       "2       800      0.960       bicubic      2.226      0.125          0  \n",
       "3       475      0.936       bicubic      2.289      0.156          0  \n",
       "4       384      1.000       bicubic      3.580      0.590          4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_models = pd.read_csv(\"results-imagenet-real.csv\")\n",
    "# use models with img size 224\n",
    "modellist = df_models[df_models[\"img_size\"]==224][\"model\"]\n",
    "df_models.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "    def __init__(self,  length, imsize):\n",
    "        self.len = length\n",
    "        self.data = torch.randn( 3, imsize, imsize, length)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[:,:,:,index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "rand_loader = DataLoader(dataset=RandomDataset(BATCH_SIZE*(WARM_UP + NUM_TEST), 224),\n",
    "                         batch_size=BATCH_SIZE, shuffle=False,num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ryujaehun/pytorch-gpu-benchmark/blob/master/benchmark_models.py\n",
    "def inference(modelname, benchmark, half=False):\n",
    "    with torch.no_grad():\n",
    "        model = timm.create_model(modelname,)\n",
    "        model=model.to('cuda')\n",
    "        model.eval()\n",
    "        precision = \"float\"\n",
    "        durations = []\n",
    "        print(f'Benchmarking Inference {modelname} ')\n",
    "        for step,img in enumerate(rand_loader):\n",
    "            img=getattr(img,precision)()\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "            model(img.to('cuda'))\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            if step >= WARM_UP:\n",
    "                durations.append((end - start)*1000)\n",
    "        print(f'{modelname} model average inference time : {sum(durations)/len(durations)}ms')\n",
    "        \n",
    "        if half:\n",
    "            durations_half = []\n",
    "            print(f'Benchmarking Inference half precision type {modelname} ')\n",
    "            model.half()\n",
    "            precision = \"half\"\n",
    "            for step,img in enumerate(rand_loader):\n",
    "                img=getattr(img,precision)()\n",
    "                torch.cuda.synchronize()\n",
    "                start = time.time()\n",
    "                model(img.to('cuda'))\n",
    "                torch.cuda.synchronize()\n",
    "                end = time.time()\n",
    "                if step >= WARM_UP:\n",
    "                    durations_half.append((end - start)*1000)\n",
    "            print(f'{modelname} half model average inference time : {sum(durations_half)/len(durations_half)}ms')\n",
    "            \n",
    "        if half:\n",
    "            benchmark[modelname] = {\"fp32\": np.mean(durations), \"fp16\": np.mean(durations_half), \"top1\": df_models[df_models[\"model\"]==modelname][\"top1\"]}\n",
    "        else:\n",
    "            benchmark[modelname] = {\"fp32\": np.mean(durations), \"top1\": float(df_models[df_models[\"model\"]==modelname][\"top1\"])}\n",
    "    return benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b825893fd69e43408292533bbc84eec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Inference beit_large_patch16_224 \n",
      "beit_large_patch16_224 model average inference time : 21.222758293151855ms\n",
      "Benchmarking Inference swin_large_patch4_window7_224 \n",
      "swin_large_patch4_window7_224 model average inference time : 17.0994234085083ms\n",
      "Benchmarking Inference xcit_large_24_p8_224_dist \n",
      "xcit_large_24_p8_224_dist model average inference time : 66.53245449066162ms\n",
      "Benchmarking Inference beit_base_patch16_224 \n",
      "beit_base_patch16_224 model average inference time : 7.464489936828613ms\n",
      "Benchmarking Inference vit_large_patch16_224 \n",
      "vit_large_patch16_224 model average inference time : 20.04459857940674ms\n",
      "Benchmarking Inference xcit_medium_24_p8_224_dist \n",
      "xcit_medium_24_p8_224_dist model average inference time : 40.78274726867676ms\n",
      "Benchmarking Inference xcit_small_24_p8_224_dist \n",
      "xcit_small_24_p8_224_dist model average inference time : 29.373767375946045ms\n",
      "Benchmarking Inference swin_base_patch4_window7_224 \n",
      "swin_base_patch4_window7_224 model average inference time : 12.806141376495361ms\n",
      "Benchmarking Inference ig_resnext101_32x32d \n",
      "ig_resnext101_32x32d model average inference time : 132.56885290145874ms\n",
      "Benchmarking Inference ig_resnext101_32x48d \n",
      "ig_resnext101_32x48d model average inference time : 210.10546445846558ms\n",
      "Benchmarking Inference xcit_large_24_p16_224_dist \n",
      "xcit_large_24_p16_224_dist model average inference time : 19.736363887786865ms\n",
      "Benchmarking Inference resmlp_big_24_224_in22ft1k \n",
      "resmlp_big_24_224_in22ft1k model average inference time : 33.9638614654541ms\n",
      "Benchmarking Inference xcit_small_12_p8_224_dist \n",
      "xcit_small_12_p8_224_dist model average inference time : 16.07722043991089ms\n",
      "Benchmarking Inference vit_base_patch16_224 \n",
      "vit_base_patch16_224 model average inference time : 7.092115879058838ms\n",
      "Benchmarking Inference xcit_medium_24_p16_224_dist \n",
      "xcit_medium_24_p16_224_dist model average inference time : 19.059040546417236ms\n",
      "Benchmarking Inference ig_resnext101_32x16d \n",
      "ig_resnext101_32x16d model average inference time : 35.41270971298218ms\n",
      "Benchmarking Inference swsl_resnext101_32x8d \n",
      "swsl_resnext101_32x8d model average inference time : 19.28312063217163ms\n",
      "Benchmarking Inference vit_base_patch16_224_miil \n",
      "vit_base_patch16_224_miil model average inference time : 6.86307430267334ms\n",
      "Benchmarking Inference pit_b_distilled_224 \n",
      "pit_b_distilled_224 model average inference time : 8.121466636657715ms\n",
      "Benchmarking Inference xcit_small_24_p16_224_dist \n",
      "xcit_small_24_p16_224_dist model average inference time : 18.69269609451294ms\n",
      "Benchmarking Inference cait_s24_224 \n",
      "cait_s24_224 model average inference time : 14.812660217285156ms\n",
      "Benchmarking Inference resmlp_big_24_distilled_224 \n",
      "resmlp_big_24_distilled_224 model average inference time : 33.96995306015015ms\n",
      "Benchmarking Inference vit_large_r50_s32_224 \n",
      "vit_large_r50_s32_224 model average inference time : 16.82636022567749ms\n",
      "Benchmarking Inference xcit_small_12_p16_224_dist \n",
      "xcit_small_12_p16_224_dist model average inference time : 10.687386989593506ms\n",
      "Benchmarking Inference deit_base_distilled_patch16_224 \n",
      "deit_base_distilled_patch16_224 model average inference time : 7.080423831939697ms\n",
      "Benchmarking Inference ig_resnext101_32x8d \n",
      "ig_resnext101_32x8d model average inference time : 19.176130294799805ms\n",
      "Benchmarking Inference xcit_large_24_p8_224 \n",
      "xcit_large_24_p8_224 model average inference time : 68.2600736618042ms\n",
      "Benchmarking Inference swsl_resnext101_32x4d \n",
      "swsl_resnext101_32x4d model average inference time : 12.613909244537354ms\n",
      "Benchmarking Inference xcit_tiny_24_p8_224_dist \n",
      "xcit_tiny_24_p8_224_dist model average inference time : 18.30082416534424ms\n",
      "Benchmarking Inference xcit_small_24_p8_224 \n",
      "xcit_small_24_p8_224 model average inference time : 29.7322678565979ms\n",
      "Benchmarking Inference twins_svt_large \n",
      "twins_svt_large model average inference time : 14.635822772979736ms\n",
      "Benchmarking Inference twins_pcpvt_large \n",
      "twins_pcpvt_large model average inference time : 24.414920806884766ms\n",
      "Benchmarking Inference xcit_small_12_p8_224 \n",
      "xcit_small_12_p8_224 model average inference time : 16.07599973678589ms\n",
      "Benchmarking Inference resnetv2_50x1_bit_distilled \n",
      "resnetv2_50x1_bit_distilled model average inference time : 8.04631233215332ms\n",
      "Benchmarking Inference tresnet_m \n",
      "pass tresnet_m\n",
      "Benchmarking Inference twins_pcpvt_base \n",
      "twins_pcpvt_base model average inference time : 16.830954551696777ms\n",
      "Benchmarking Inference swin_small_patch4_window7_224 \n",
      "swin_small_patch4_window7_224 model average inference time : 13.139035701751709ms\n",
      "Benchmarking Inference twins_svt_base \n",
      "twins_svt_base model average inference time : 13.144330978393555ms\n",
      "Benchmarking Inference xcit_medium_24_p8_224 \n",
      "xcit_medium_24_p8_224 model average inference time : 41.48906946182251ms\n",
      "Benchmarking Inference jx_nest_base \n",
      "jx_nest_base model average inference time : 13.87157917022705ms\n",
      "Benchmarking Inference swsl_resnext101_32x16d \n",
      "swsl_resnext101_32x16d model average inference time : 35.50509452819824ms\n",
      "Benchmarking Inference swsl_resnext50_32x4d \n",
      "swsl_resnext50_32x4d model average inference time : 6.88831090927124ms\n",
      "Benchmarking Inference levit_384 \n",
      "levit_384 model average inference time : 7.732181549072266ms\n",
      "Benchmarking Inference jx_nest_small \n",
      "jx_nest_small model average inference time : 10.803375244140625ms\n",
      "Benchmarking Inference resnetv2_152x2_bit_teacher \n",
      "resnetv2_152x2_bit_teacher model average inference time : 46.50394678115845ms\n",
      "Benchmarking Inference xcit_tiny_24_p8_224 \n",
      "xcit_tiny_24_p8_224 model average inference time : 18.08338165283203ms\n",
      "Benchmarking Inference coat_lite_small \n",
      "coat_lite_small model average inference time : 13.0839204788208ms\n",
      "Benchmarking Inference resnetv2_101 \n",
      "resnetv2_101 model average inference time : 10.269548892974854ms\n",
      "Benchmarking Inference ecaresnet101d \n",
      "ecaresnet101d model average inference time : 13.389935493469238ms\n",
      "Benchmarking Inference pit_s_distilled_224 \n",
      "pit_s_distilled_224 model average inference time : 5.669441223144531ms\n",
      "Benchmarking Inference mixer_b16_224_miil \n",
      "mixer_b16_224_miil model average inference time : 5.0153279304504395ms\n",
      "Benchmarking Inference tresnet_xl \n",
      "pass tresnet_xl\n",
      "Benchmarking Inference xcit_tiny_12_p8_224_dist \n",
      "xcit_tiny_12_p8_224_dist model average inference time : 10.577220916748047ms\n",
      "Benchmarking Inference convit_base \n",
      "convit_base model average inference time : 10.248382091522217ms\n",
      "Benchmarking Inference visformer_small \n",
      "visformer_small model average inference time : 12.276256084442139ms\n",
      "Benchmarking Inference xcit_small_24_p16_224 \n",
      "xcit_small_24_p16_224 model average inference time : 18.592159748077393ms\n",
      "Benchmarking Inference convit_small \n",
      "convit_small model average inference time : 7.061395645141602ms\n",
      "Benchmarking Inference jx_nest_tiny \n",
      "jx_nest_tiny model average inference time : 7.174441814422607ms\n",
      "Benchmarking Inference xcit_small_12_p16_224 \n",
      "xcit_small_12_p16_224 model average inference time : 10.355837345123291ms\n",
      "Benchmarking Inference deit_small_distilled_patch16_224 \n",
      "deit_small_distilled_patch16_224 model average inference time : 5.395364761352539ms\n",
      "Benchmarking Inference resmlp_36_distilled_224 \n",
      "resmlp_36_distilled_224 model average inference time : 8.569638729095459ms\n",
      "Benchmarking Inference xcit_large_24_p16_224 \n",
      "xcit_large_24_p16_224 model average inference time : 19.858109951019287ms\n",
      "Benchmarking Inference xcit_medium_24_p16_224 \n",
      "xcit_medium_24_p16_224 model average inference time : 18.936972618103027ms\n",
      "Benchmarking Inference tnt_s_patch16_224 \n",
      "tnt_s_patch16_224 model average inference time : 11.9808030128479ms\n",
      "Benchmarking Inference convmixer_1536_20 \n",
      "pass convmixer_1536_20\n",
      "Benchmarking Inference rexnet_200 \n",
      "rexnet_200 model average inference time : 8.933818340301514ms\n",
      "Benchmarking Inference vit_small_patch16_224 \n",
      "vit_small_patch16_224 model average inference time : 5.296797752380371ms\n",
      "Benchmarking Inference ssl_resnext101_32x16d \n",
      "ssl_resnext101_32x16d model average inference time : 35.33586263656616ms\n",
      "Benchmarking Inference vit_small_r26_s32_224 \n",
      "vit_small_r26_s32_224 model average inference time : 9.251573085784912ms\n",
      "Benchmarking Inference deit_base_patch16_224 \n",
      "deit_base_patch16_224 model average inference time : 7.058782577514648ms\n",
      "Benchmarking Inference coat_mini \n",
      "coat_mini model average inference time : 22.302887439727783ms\n",
      "Benchmarking Inference swsl_resnet50 \n",
      "swsl_resnet50 model average inference time : 5.740089416503906ms\n",
      "Benchmarking Inference ssl_resnext101_32x8d \n",
      "ssl_resnext101_32x8d model average inference time : 19.183614253997803ms\n",
      "Benchmarking Inference tresnet_l \n",
      "pass tresnet_l\n",
      "Benchmarking Inference twins_svt_small \n",
      "twins_svt_small model average inference time : 10.310540199279785ms\n",
      "Benchmarking Inference levit_256 \n",
      "levit_256 model average inference time : 7.933638095855713ms\n",
      "Benchmarking Inference seresnext50_32x4d \n",
      "seresnext50_32x4d model average inference time : 10.537798404693604ms\n",
      "Benchmarking Inference pit_b_224 \n",
      "pit_b_224 model average inference time : 8.083999156951904ms\n",
      "Benchmarking Inference swin_tiny_patch4_window7_224 \n",
      "swin_tiny_patch4_window7_224 model average inference time : 7.519328594207764ms\n",
      "Benchmarking Inference wide_resnet50_2 \n",
      "wide_resnet50_2 model average inference time : 11.235213279724121ms\n",
      "Benchmarking Inference twins_pcpvt_small \n",
      "twins_pcpvt_small model average inference time : 10.131669044494629ms\n",
      "Benchmarking Inference resmlp_24_distilled_224 \n",
      "resmlp_24_distilled_224 model average inference time : 6.0874199867248535ms\n",
      "Benchmarking Inference resnest50d_4s2x40d \n",
      "resnest50d_4s2x40d model average inference time : 22.870047092437744ms\n",
      "Benchmarking Inference repvgg_b3 \n",
      "repvgg_b3 model average inference time : 19.00216579437256ms\n",
      "Benchmarking Inference xcit_tiny_24_p16_224_dist \n",
      "xcit_tiny_24_p16_224_dist model average inference time : 19.405126571655273ms\n",
      "Benchmarking Inference ecaresnet50d \n",
      "ecaresnet50d model average inference time : 7.205207347869873ms\n",
      "Benchmarking Inference ssl_resnext101_32x4d \n",
      "ssl_resnext101_32x4d model average inference time : 12.962768077850342ms\n",
      "Benchmarking Inference gluon_resnet152_v1s \n",
      "gluon_resnet152_v1s model average inference time : 14.70705509185791ms\n",
      "Benchmarking Inference haloregnetz_b \n",
      "haloregnetz_b model average inference time : 13.94322156906128ms\n",
      "Benchmarking Inference resnest50d_1s4x24d \n",
      "resnest50d_1s4x24d model average inference time : 14.099588394165039ms\n",
      "Benchmarking Inference repvgg_b3g4 \n",
      "repvgg_b3g4 model average inference time : 21.578655242919922ms\n",
      "Benchmarking Inference legacy_senet154 \n",
      "legacy_senet154 model average inference time : 35.050692558288574ms\n",
      "Benchmarking Inference gernet_m \n",
      "gernet_m model average inference time : 4.982576370239258ms\n",
      "Benchmarking Inference cait_xxs36_224 \n",
      "cait_xxs36_224 model average inference time : 20.291597843170166ms\n",
      "Benchmarking Inference pit_s_224 \n",
      "pit_s_224 model average inference time : 5.699214935302734ms\n",
      "Benchmarking Inference gluon_senet154 \n",
      "gluon_senet154 model average inference time : 35.667269229888916ms\n",
      "Benchmarking Inference resnest50d \n",
      "resnest50d model average inference time : 13.990790843963623ms\n",
      "Benchmarking Inference convmixer_768_32 \n",
      "pass convmixer_768_32\n",
      "Benchmarking Inference ecaresnet101d_pruned \n",
      "ecaresnet101d_pruned model average inference time : 11.902644634246826ms\n",
      "Benchmarking Inference rexnet_150 \n",
      "rexnet_150 model average inference time : 8.516361713409424ms\n",
      "Benchmarking Inference xcit_tiny_12_p8_224 \n",
      "xcit_tiny_12_p8_224 model average inference time : 10.543198585510254ms\n",
      "Benchmarking Inference ssl_resnext50_32x4d \n",
      "ssl_resnext50_32x4d model average inference time : 6.872310638427734ms\n",
      "Benchmarking Inference ecaresnetlight \n",
      "ecaresnetlight model average inference time : 7.006616592407227ms\n",
      "Benchmarking Inference gluon_resnet101_v1s \n",
      "gluon_resnet101_v1s model average inference time : 10.721702575683594ms\n",
      "Benchmarking Inference resnetv2_50 \n",
      "resnetv2_50 model average inference time : 5.681214332580566ms\n",
      "Benchmarking Inference gluon_seresnext101_32x4d \n",
      "gluon_seresnext101_32x4d model average inference time : 20.829873085021973ms\n",
      "Benchmarking Inference resnet50d \n",
      "resnet50d model average inference time : 6.009654998779297ms\n",
      "Benchmarking Inference vit_base_patch32_224 \n",
      "vit_base_patch32_224 model average inference time : 5.190515518188477ms\n",
      "Benchmarking Inference gluon_seresnext101_64x4d \n",
      "gluon_seresnext101_64x4d model average inference time : 23.46662998199463ms\n",
      "Benchmarking Inference gluon_resnet152_v1d \n",
      "gluon_resnet152_v1d model average inference time : 14.824655055999756ms\n",
      "Benchmarking Inference vit_base_patch16_sam_224 \n",
      "vit_base_patch16_sam_224 model average inference time : 7.050213813781738ms\n",
      "Benchmarking Inference repvgg_b2g4 \n",
      "repvgg_b2g4 model average inference time : 19.92868661880493ms\n",
      "Benchmarking Inference seresnet50 \n",
      "seresnet50 model average inference time : 9.327127933502197ms\n",
      "Benchmarking Inference gluon_resnet101_v1d \n",
      "gluon_resnet101_v1d model average inference time : 10.482888221740723ms\n",
      "Benchmarking Inference mixnet_xl \n",
      "mixnet_xl model average inference time : 15.067610740661621ms\n",
      "Benchmarking Inference cspresnext50 \n",
      "cspresnext50 model average inference time : 6.738874912261963ms\n",
      "Benchmarking Inference gluon_resnext101_32x4d \n",
      "gluon_resnext101_32x4d model average inference time : 12.563707828521729ms\n",
      "Benchmarking Inference ese_vovnet39b \n",
      "ese_vovnet39b model average inference time : 6.843414306640625ms\n",
      "Benchmarking Inference legacy_seresnext101_32x4d \n",
      "legacy_seresnext101_32x4d model average inference time : 20.458691120147705ms\n",
      "Benchmarking Inference xcit_tiny_24_p16_224 \n",
      "xcit_tiny_24_p16_224 model average inference time : 18.694677352905273ms\n",
      "Benchmarking Inference regnety_320 \n",
      "regnety_320 model average inference time : 36.93737506866455ms\n",
      "Benchmarking Inference resnet50 \n",
      "resnet50 model average inference time : 5.8392333984375ms\n",
      "Benchmarking Inference gluon_resnext101_64x4d \n",
      "gluon_resnext101_64x4d model average inference time : 19.55361843109131ms\n",
      "Benchmarking Inference resmlp_big_24_224 \n",
      "resmlp_big_24_224 model average inference time : 34.003565311431885ms\n",
      "Benchmarking Inference deit_small_patch16_224 \n",
      "deit_small_patch16_224 model average inference time : 5.30376672744751ms\n",
      "Benchmarking Inference dpn107 \n",
      "dpn107 model average inference time : 25.07070302963257ms\n",
      "Benchmarking Inference pit_xs_distilled_224 \n",
      "pit_xs_distilled_224 model average inference time : 5.718038082122803ms\n",
      "Benchmarking Inference resmlp_36_224 \n",
      "resmlp_36_224 model average inference time : 8.562207221984863ms\n",
      "Benchmarking Inference levit_192 \n",
      "levit_192 model average inference time : 7.820937633514404ms\n",
      "Benchmarking Inference gluon_resnet152_v1c \n",
      "gluon_resnet152_v1c model average inference time : 14.790723323822021ms\n",
      "Benchmarking Inference ecaresnet50d_pruned \n",
      "ecaresnet50d_pruned model average inference time : 6.682915687561035ms\n",
      "Benchmarking Inference resnext50d_32x4d \n",
      "resnext50d_32x4d model average inference time : 7.0255208015441895ms\n",
      "Benchmarking Inference regnety_120 \n",
      "regnety_120 model average inference time : 23.444221019744873ms\n",
      "Benchmarking Inference regnetx_320 \n",
      "regnetx_320 model average inference time : 36.329240798950195ms\n",
      "Benchmarking Inference dpn92 \n",
      "dpn92 model average inference time : 12.439408302307129ms\n",
      "Benchmarking Inference rexnet_130 \n",
      "rexnet_130 model average inference time : 8.482873439788818ms\n",
      "Benchmarking Inference gluon_resnet152_v1b \n",
      "gluon_resnet152_v1b model average inference time : 14.631612300872803ms\n",
      "Benchmarking Inference resnetrs50 \n",
      "resnetrs50 model average inference time : 9.213309288024902ms\n",
      "Benchmarking Inference dpn131 \n",
      "dpn131 model average inference time : 22.95206308364868ms\n",
      "Benchmarking Inference dla102x2 \n",
      "dla102x2 model average inference time : 15.781023502349854ms\n",
      "Benchmarking Inference regnetx_160 \n",
      "regnetx_160 model average inference time : 26.581060886383057ms\n",
      "Benchmarking Inference gmlp_s16_224 \n",
      "gmlp_s16_224 model average inference time : 7.8601789474487305ms\n",
      "Benchmarking Inference gluon_seresnext50_32x4d \n",
      "gluon_seresnext50_32x4d model average inference time : 10.296087265014648ms\n",
      "Benchmarking Inference skresnext50_32x4d \n",
      "skresnext50_32x4d model average inference time : 14.00026798248291ms\n",
      "Benchmarking Inference gluon_resnet101_v1c \n",
      "gluon_resnet101_v1c model average inference time : 10.544290542602539ms\n",
      "Benchmarking Inference dpn98 \n",
      "dpn98 model average inference time : 16.838343143463135ms\n",
      "Benchmarking Inference regnety_064 \n",
      "regnety_064 model average inference time : 15.869503021240234ms\n",
      "Benchmarking Inference dpn68b \n",
      "dpn68b model average inference time : 10.606613159179688ms\n",
      "Benchmarking Inference resnetblur50 \n",
      "resnetblur50 model average inference time : 5.8940863609313965ms\n",
      "Benchmarking Inference resmlp_24_224 \n",
      "resmlp_24_224 model average inference time : 6.166572570800781ms\n",
      "Benchmarking Inference coat_lite_mini \n",
      "coat_lite_mini model average inference time : 7.513597011566162ms\n",
      "Benchmarking Inference resnext50_32x4d \n",
      "resnext50_32x4d model average inference time : 6.959478855133057ms\n",
      "Benchmarking Inference regnety_080 \n",
      "regnety_080 model average inference time : 14.425852298736572ms\n",
      "Benchmarking Inference cait_xxs24_224 \n",
      "cait_xxs24_224 model average inference time : 14.630074501037598ms\n",
      "Benchmarking Inference xcit_tiny_12_p16_224_dist \n",
      "xcit_tiny_12_p16_224_dist model average inference time : 10.864458084106445ms\n",
      "Benchmarking Inference resnext101_32x8d \n",
      "resnext101_32x8d model average inference time : 19.254169464111328ms\n",
      "Benchmarking Inference hrnet_w48 \n",
      "hrnet_w48 model average inference time : 34.230711460113525ms\n",
      "Benchmarking Inference regnetx_120 \n",
      "regnetx_120 model average inference time : 21.44188404083252ms\n",
      "Benchmarking Inference gluon_resnet101_v1b \n",
      "gluon_resnet101_v1b model average inference time : 9.984734058380127ms\n",
      "Benchmarking Inference hrnet_w64 \n",
      "hrnet_w64 model average inference time : 33.04595232009888ms\n",
      "Benchmarking Inference ssl_resnet50 \n",
      "ssl_resnet50 model average inference time : 5.827453136444092ms\n",
      "Benchmarking Inference res2net101_26w_4s \n",
      "res2net101_26w_4s model average inference time : 17.982370853424072ms\n",
      "Benchmarking Inference gluon_resnext50_32x4d \n",
      "gluon_resnext50_32x4d model average inference time : 6.866660118103027ms\n",
      "Benchmarking Inference tf_efficientnet_b0_ns \n",
      "tf_efficientnet_b0_ns model average inference time : 7.480754852294922ms\n",
      "Benchmarking Inference resnest26d \n",
      "resnest26d model average inference time : 9.21252727508545ms\n",
      "Benchmarking Inference coat_tiny \n",
      "coat_tiny model average inference time : 22.003560066223145ms\n",
      "Benchmarking Inference regnety_040 \n",
      "regnety_040 model average inference time : 12.508735656738281ms\n",
      "Benchmarking Inference dla169 \n",
      "dla169 model average inference time : 16.491498947143555ms\n",
      "Benchmarking Inference legacy_seresnext50_32x4d \n",
      "legacy_seresnext50_32x4d model average inference time : 9.978797435760498ms\n",
      "Benchmarking Inference hrnet_w44 \n",
      "hrnet_w44 model average inference time : 34.55578088760376ms\n",
      "Benchmarking Inference regnetx_080 \n",
      "regnetx_080 model average inference time : 16.284196376800537ms\n",
      "Benchmarking Inference gluon_resnet50_v1s \n",
      "gluon_resnet50_v1s model average inference time : 6.026053428649902ms\n",
      "Benchmarking Inference res2net50_26w_8s \n",
      "res2net50_26w_8s model average inference time : 16.22917652130127ms\n",
      "Benchmarking Inference dla60_res2next \n",
      "dla60_res2next model average inference time : 16.423168182373047ms\n",
      "Benchmarking Inference mixnet_l \n",
      "mixnet_l model average inference time : 11.95857048034668ms\n",
      "Benchmarking Inference levit_128 \n",
      "levit_128 model average inference time : 7.7445220947265625ms\n",
      "Benchmarking Inference dla60_res2net \n",
      "dla60_res2net model average inference time : 10.492653846740723ms\n",
      "Benchmarking Inference tv_resnet152 \n",
      "tv_resnet152 model average inference time : 14.771018028259277ms\n",
      "Benchmarking Inference dla102x \n",
      "dla102x model average inference time : 12.74256944656372ms\n",
      "Benchmarking Inference gluon_resnet50_v1d \n",
      "gluon_resnet50_v1d model average inference time : 5.942518711090088ms\n",
      "Benchmarking Inference regnetx_064 \n",
      "regnetx_064 model average inference time : 10.907056331634521ms\n",
      "Benchmarking Inference pit_xs_224 \n",
      "pit_xs_224 model average inference time : 5.417320728302002ms\n",
      "Benchmarking Inference hrnet_w40 \n",
      "hrnet_w40 model average inference time : 31.760694980621338ms\n",
      "Benchmarking Inference repvgg_b2 \n",
      "repvgg_b2 model average inference time : 13.380820751190186ms\n",
      "Benchmarking Inference res2net50_26w_6s \n",
      "res2net50_26w_6s model average inference time : 13.04659128189087ms\n",
      "Benchmarking Inference resmlp_12_distilled_224 \n",
      "resmlp_12_distilled_224 model average inference time : 3.3237171173095703ms\n",
      "Benchmarking Inference legacy_seresnet152 \n",
      "legacy_seresnet152 model average inference time : 26.403536796569824ms\n",
      "Benchmarking Inference selecsls60b \n",
      "selecsls60b model average inference time : 6.648528575897217ms\n",
      "Benchmarking Inference hrnet_w32 \n",
      "hrnet_w32 model average inference time : 31.174631118774414ms\n",
      "Benchmarking Inference tf_efficientnetv2_b0 \n",
      "tf_efficientnetv2_b0 model average inference time : 9.105782508850098ms\n",
      "Benchmarking Inference regnetx_040 \n",
      "regnetx_040 model average inference time : 9.47070598602295ms\n",
      "Benchmarking Inference hrnet_w30 \n",
      "hrnet_w30 model average inference time : 32.16704845428467ms\n",
      "Benchmarking Inference efficientnet_es \n",
      "efficientnet_es model average inference time : 4.538414478302002ms\n",
      "Benchmarking Inference tf_mixnet_l \n",
      "tf_mixnet_l model average inference time : 12.606806755065918ms\n",
      "Benchmarking Inference wide_resnet101_2 \n",
      "wide_resnet101_2 model average inference time : 18.493614196777344ms\n",
      "Benchmarking Inference dla60x \n",
      "dla60x model average inference time : 7.526495456695557ms\n",
      "Benchmarking Inference legacy_seresnet101 \n",
      "legacy_seresnet101 model average inference time : 17.574009895324707ms\n",
      "Benchmarking Inference coat_lite_tiny \n",
      "coat_lite_tiny model average inference time : 7.392082214355469ms\n",
      "Benchmarking Inference repvgg_b1 \n",
      "repvgg_b1 model average inference time : 10.306687355041504ms\n",
      "Benchmarking Inference res2net50_26w_4s \n",
      "res2net50_26w_4s model average inference time : 9.69325304031372ms\n",
      "Benchmarking Inference hardcorenas_f \n",
      "hardcorenas_f model average inference time : 7.530560493469238ms\n",
      "Benchmarking Inference res2net50_14w_8s \n",
      "res2net50_14w_8s model average inference time : 15.195362567901611ms\n",
      "Benchmarking Inference selecsls60 \n",
      "selecsls60 model average inference time : 6.841988563537598ms\n",
      "Benchmarking Inference res2next50 \n",
      "res2next50 model average inference time : 16.007883548736572ms\n",
      "Benchmarking Inference regnetx_032 \n",
      "regnetx_032 model average inference time : 9.208292961120605ms\n",
      "Benchmarking Inference gluon_resnet50_v1c \n",
      "gluon_resnet50_v1c model average inference time : 6.052649021148682ms\n",
      "Benchmarking Inference dla102 \n",
      "dla102 model average inference time : 10.941462516784668ms\n",
      "Benchmarking Inference rexnet_100 \n",
      "rexnet_100 model average inference time : 8.449954986572266ms\n",
      "Benchmarking Inference res2net50_48w_2s \n",
      "res2net50_48w_2s model average inference time : 6.536848545074463ms\n",
      "Benchmarking Inference resnet34d \n",
      "resnet34d model average inference time : 4.470558166503906ms\n",
      "Benchmarking Inference xcit_tiny_12_p16_224 \n",
      "xcit_tiny_12_p16_224 model average inference time : 10.63784122467041ms\n",
      "Benchmarking Inference efficientnet_b0 \n",
      "efficientnet_b0 model average inference time : 7.568955421447754ms\n",
      "Benchmarking Inference hardcorenas_e \n",
      "hardcorenas_e model average inference time : 7.396128177642822ms\n",
      "Benchmarking Inference gmixer_24_224 \n",
      "gmixer_24_224 model average inference time : 8.387062549591064ms\n",
      "Benchmarking Inference tf_efficientnet_cc_b0_8e \n",
      "tf_efficientnet_cc_b0_8e model average inference time : 9.599294662475586ms\n",
      "Benchmarking Inference regnety_016 \n",
      "regnety_016 model average inference time : 13.486073017120361ms\n",
      "Benchmarking Inference tv_resnext50_32x4d \n",
      "tv_resnext50_32x4d model average inference time : 6.946098804473877ms\n",
      "Benchmarking Inference gluon_resnet50_v1b \n",
      "gluon_resnet50_v1b model average inference time : 5.7448625564575195ms\n",
      "Benchmarking Inference densenet161 \n",
      "densenet161 model average inference time : 18.091626167297363ms\n",
      "Benchmarking Inference seresnext26t_32x4d \n",
      "seresnext26t_32x4d model average inference time : 5.766298770904541ms\n",
      "Benchmarking Inference mobilenetv2_120d \n",
      "mobilenetv2_120d model average inference time : 6.469125747680664ms\n",
      "Benchmarking Inference tv_resnet101 \n",
      "tv_resnet101 model average inference time : 10.18514633178711ms\n",
      "Benchmarking Inference hardcorenas_d \n",
      "hardcorenas_d model average inference time : 7.6528215408325195ms\n",
      "Benchmarking Inference dla60 \n",
      "dla60 model average inference time : 6.767768859863281ms\n",
      "Benchmarking Inference xcit_nano_12_p8_224_dist \n",
      "xcit_nano_12_p8_224_dist model average inference time : 10.852313041687012ms\n",
      "Benchmarking Inference seresnext26d_32x4d \n",
      "seresnext26d_32x4d model average inference time : 5.781450271606445ms\n",
      "Benchmarking Inference repvgg_b1g4 \n",
      "repvgg_b1g4 model average inference time : 15.741524696350098ms\n",
      "Benchmarking Inference convmixer_1024_20_ks9_p14 \n",
      "pass convmixer_1024_20_ks9_p14\n",
      "Benchmarking Inference legacy_seresnet50 \n",
      "legacy_seresnet50 model average inference time : 9.04362678527832ms\n",
      "Benchmarking Inference tf_efficientnet_b0_ap \n",
      "tf_efficientnet_b0_ap model average inference time : 7.821083068847656ms\n",
      "Benchmarking Inference skresnet34 \n",
      "skresnet34 model average inference time : 11.947510242462158ms\n",
      "Benchmarking Inference tf_efficientnet_cc_b0_4e \n",
      "tf_efficientnet_cc_b0_4e model average inference time : 9.393877983093262ms\n",
      "Benchmarking Inference resmlp_12_224 \n",
      "resmlp_12_224 model average inference time : 3.5558176040649414ms\n",
      "Benchmarking Inference mobilenetv3_large_100_miil \n",
      "mobilenetv3_large_100_miil model average inference time : 5.717058181762695ms\n",
      "Benchmarking Inference densenet201 \n",
      "densenet201 model average inference time : 22.792253494262695ms\n",
      "Benchmarking Inference gernet_s \n",
      "gernet_s model average inference time : 4.975404739379883ms\n",
      "Benchmarking Inference legacy_seresnext26_32x4d \n",
      "legacy_seresnext26_32x4d model average inference time : 5.526425838470459ms\n",
      "Benchmarking Inference mixnet_m \n",
      "mixnet_m model average inference time : 12.208683490753174ms\n",
      "Benchmarking Inference tf_efficientnet_b0 \n",
      "tf_efficientnet_b0 model average inference time : 7.39795446395874ms\n",
      "Benchmarking Inference hrnet_w18 \n",
      "hrnet_w18 model average inference time : 30.66802978515625ms\n",
      "Benchmarking Inference densenetblur121d \n",
      "densenetblur121d model average inference time : 13.869426250457764ms\n",
      "Benchmarking Inference selecsls42b \n",
      "selecsls42b model average inference time : 5.463900566101074ms\n",
      "Benchmarking Inference hardcorenas_c \n",
      "hardcorenas_c model average inference time : 5.89510440826416ms\n",
      "Benchmarking Inference regnetx_016 \n",
      "regnetx_016 model average inference time : 7.057490348815918ms\n",
      "Benchmarking Inference mobilenetv2_140 \n",
      "mobilenetv2_140 model average inference time : 4.270687103271484ms\n",
      "Benchmarking Inference tf_mixnet_m \n",
      "tf_mixnet_m model average inference time : 12.038166522979736ms\n",
      "Benchmarking Inference dpn68 \n",
      "dpn68 model average inference time : 9.72515344619751ms\n",
      "Benchmarking Inference tf_efficientnet_es \n",
      "tf_efficientnet_es model average inference time : 4.743032455444336ms\n",
      "Benchmarking Inference ese_vovnet19b_dw \n",
      "ese_vovnet19b_dw model average inference time : 4.022977352142334ms\n",
      "Benchmarking Inference levit_128s \n",
      "levit_128s model average inference time : 6.251623630523682ms\n",
      "Benchmarking Inference resnet26d \n",
      "resnet26d model average inference time : 3.824455738067627ms\n",
      "Benchmarking Inference repvgg_a2 \n",
      "repvgg_a2 model average inference time : 5.800914764404297ms\n",
      "Benchmarking Inference tv_resnet50 \n",
      "tv_resnet50 model average inference time : 5.629220008850098ms\n",
      "Benchmarking Inference hardcorenas_b \n",
      "hardcorenas_b model average inference time : 5.557959079742432ms\n",
      "Benchmarking Inference densenet121 \n",
      "densenet121 model average inference time : 13.61299991607666ms\n",
      "Benchmarking Inference densenet169 \n",
      "densenet169 model average inference time : 18.831112384796143ms\n",
      "Benchmarking Inference mixnet_s \n",
      "mixnet_s model average inference time : 9.757490158081055ms\n",
      "Benchmarking Inference vit_small_patch32_224 \n",
      "vit_small_patch32_224 model average inference time : 5.096225738525391ms\n",
      "Benchmarking Inference regnety_008 \n",
      "regnety_008 model average inference time : 7.901732921600342ms\n",
      "Benchmarking Inference efficientnet_lite0 \n",
      "efficientnet_lite0 model average inference time : 4.191188812255859ms\n",
      "Benchmarking Inference resnest14d \n",
      "resnest14d model average inference time : 6.851811408996582ms\n",
      "Benchmarking Inference hardcorenas_a \n",
      "hardcorenas_a model average inference time : 4.8409199714660645ms\n",
      "Benchmarking Inference efficientnet_es_pruned \n",
      "efficientnet_es_pruned model average inference time : 4.648795127868652ms\n",
      "Benchmarking Inference mobilenetv3_rw \n",
      "mobilenetv3_rw model average inference time : 5.667610168457031ms\n",
      "Benchmarking Inference semnasnet_100 \n",
      "semnasnet_100 model average inference time : 5.82747220993042ms\n",
      "Benchmarking Inference mobilenetv3_large_100 \n",
      "mobilenetv3_large_100 model average inference time : 5.91533899307251ms\n",
      "Benchmarking Inference resnet34 \n",
      "resnet34 model average inference time : 4.090702533721924ms\n",
      "Benchmarking Inference mobilenetv2_110d \n",
      "mobilenetv2_110d model average inference time : 5.3720855712890625ms\n",
      "Benchmarking Inference vit_tiny_patch16_224 \n",
      "vit_tiny_patch16_224 model average inference time : 5.227022171020508ms\n",
      "Benchmarking Inference tf_mixnet_s \n",
      "tf_mixnet_s model average inference time : 10.010137557983398ms\n",
      "Benchmarking Inference repvgg_b0 \n",
      "repvgg_b0 model average inference time : 6.675727367401123ms\n",
      "Benchmarking Inference deit_tiny_distilled_patch16_224 \n",
      "deit_tiny_distilled_patch16_224 model average inference time : 5.460071563720703ms\n",
      "Benchmarking Inference mixer_b16_224 \n",
      "mixer_b16_224 model average inference time : 5.086991786956787ms\n",
      "Benchmarking Inference pit_ti_distilled_224 \n",
      "pit_ti_distilled_224 model average inference time : 5.3986382484436035ms\n",
      "Benchmarking Inference hrnet_w18_small_v2 \n",
      "hrnet_w18_small_v2 model average inference time : 16.57296895980835ms\n",
      "Benchmarking Inference tf_efficientnet_lite0 \n",
      "tf_efficientnet_lite0 model average inference time : 4.450747966766357ms\n",
      "Benchmarking Inference resnet26 \n",
      "resnet26 model average inference time : 3.575725555419922ms\n",
      "Benchmarking Inference tf_mobilenetv3_large_100 \n",
      "tf_mobilenetv3_large_100 model average inference time : 5.966911315917969ms\n",
      "Benchmarking Inference tv_densenet121 \n",
      "tv_densenet121 model average inference time : 13.520746231079102ms\n",
      "Benchmarking Inference regnety_006 \n",
      "regnety_006 model average inference time : 8.412299156188965ms\n",
      "Benchmarking Inference dla34 \n",
      "dla34 model average inference time : 4.329702854156494ms\n",
      "Benchmarking Inference xcit_nano_12_p8_224 \n",
      "xcit_nano_12_p8_224 model average inference time : 10.801050662994385ms\n",
      "Benchmarking Inference fbnetc_100 \n",
      "fbnetc_100 model average inference time : 5.238170623779297ms\n",
      "Benchmarking Inference legacy_seresnet34 \n",
      "legacy_seresnet34 model average inference time : 7.046689987182617ms\n",
      "Benchmarking Inference regnetx_008 \n",
      "regnetx_008 model average inference time : 5.828697681427002ms\n",
      "Benchmarking Inference gluon_resnet34_v1b \n",
      "gluon_resnet34_v1b model average inference time : 4.112353324890137ms\n",
      "Benchmarking Inference mnasnet_100 \n",
      "mnasnet_100 model average inference time : 4.512279033660889ms\n",
      "Benchmarking Inference vgg19_bn \n",
      "vgg19_bn model average inference time : 12.25665807723999ms\n",
      "Benchmarking Inference convit_tiny \n",
      "convit_tiny model average inference time : 6.963202953338623ms\n",
      "Benchmarking Inference spnasnet_100 \n",
      "spnasnet_100 model average inference time : 5.339231491088867ms\n",
      "Benchmarking Inference ghostnet_100 \n",
      "ghostnet_100 model average inference time : 8.784103393554688ms\n",
      "Benchmarking Inference regnety_004 \n",
      "regnety_004 model average inference time : 9.942929744720459ms\n",
      "Benchmarking Inference skresnet18 \n",
      "skresnet18 model average inference time : 6.443450450897217ms\n",
      "Benchmarking Inference regnetx_006 \n",
      "regnetx_006 model average inference time : 5.590357780456543ms\n",
      "Benchmarking Inference pit_ti_224 \n",
      "pit_ti_224 model average inference time : 5.664281845092773ms\n",
      "Benchmarking Inference swsl_resnet18 \n",
      "swsl_resnet18 model average inference time : 2.8646159172058105ms\n",
      "Benchmarking Inference vgg16_bn \n",
      "vgg16_bn model average inference time : 10.735476016998291ms\n",
      "Benchmarking Inference resnet18d \n",
      "resnet18d model average inference time : 3.0302071571350098ms\n",
      "Benchmarking Inference tv_resnet34 \n",
      "tv_resnet34 model average inference time : 4.167530536651611ms\n",
      "Benchmarking Inference mobilenetv2_100 \n",
      "mobilenetv2_100 model average inference time : 4.245607852935791ms\n",
      "Benchmarking Inference xcit_nano_12_p16_224_dist \n",
      "xcit_nano_12_p16_224_dist model average inference time : 10.799438953399658ms\n",
      "Benchmarking Inference vit_base_patch32_sam_224 \n",
      "vit_base_patch32_sam_224 model average inference time : 5.403764247894287ms\n",
      "Benchmarking Inference ssl_resnet18 \n",
      "ssl_resnet18 model average inference time : 2.6334023475646973ms\n",
      "Benchmarking Inference tf_mobilenetv3_large_075 \n",
      "tf_mobilenetv3_large_075 model average inference time : 5.989499092102051ms\n",
      "Benchmarking Inference deit_tiny_patch16_224 \n",
      "deit_tiny_patch16_224 model average inference time : 5.194070339202881ms\n",
      "Benchmarking Inference hrnet_w18_small \n",
      "hrnet_w18_small model average inference time : 9.595694541931152ms\n",
      "Benchmarking Inference vgg19 \n",
      "vgg19 model average inference time : 11.495828628540039ms\n",
      "Benchmarking Inference regnetx_004 \n",
      "regnetx_004 model average inference time : 8.82101058959961ms\n",
      "Benchmarking Inference tf_mobilenetv3_large_minimal_100 \n",
      "tf_mobilenetv3_large_minimal_100 model average inference time : 4.257955551147461ms\n",
      "Benchmarking Inference legacy_seresnet18 \n",
      "legacy_seresnet18 model average inference time : 4.220445156097412ms\n",
      "Benchmarking Inference vgg16 \n",
      "vgg16 model average inference time : 9.98368501663208ms\n",
      "Benchmarking Inference vit_tiny_r_s16_p8_224 \n",
      "vit_tiny_r_s16_p8_224 model average inference time : 5.392265319824219ms\n",
      "Benchmarking Inference vgg13_bn \n",
      "vgg13_bn model average inference time : 9.115087985992432ms\n",
      "Benchmarking Inference gluon_resnet18_v1b \n",
      "gluon_resnet18_v1b model average inference time : 2.794067859649658ms\n",
      "Benchmarking Inference vgg11_bn \n",
      "vgg11_bn model average inference time : 7.375471591949463ms\n",
      "Benchmarking Inference xcit_nano_12_p16_224 \n",
      "xcit_nano_12_p16_224 model average inference time : 10.941822528839111ms\n",
      "Benchmarking Inference regnety_002 \n",
      "regnety_002 model average inference time : 8.2291841506958ms\n",
      "Benchmarking Inference mixer_l16_224 \n",
      "mixer_l16_224 model average inference time : 15.571436882019043ms\n",
      "Benchmarking Inference resnet18 \n",
      "resnet18 model average inference time : 2.7054572105407715ms\n",
      "Benchmarking Inference vgg13 \n",
      "vgg13 model average inference time : 8.459031581878662ms\n",
      "Benchmarking Inference vgg11 \n",
      "vgg11 model average inference time : 7.024950981140137ms\n",
      "Benchmarking Inference regnetx_002 \n",
      "regnetx_002 model average inference time : 5.828824043273926ms\n",
      "Benchmarking Inference dla60x_c \n",
      "dla60x_c model average inference time : 5.9967803955078125ms\n",
      "Benchmarking Inference tf_mobilenetv3_small_100 \n",
      "tf_mobilenetv3_small_100 model average inference time : 5.031616687774658ms\n",
      "Benchmarking Inference dla46x_c \n",
      "dla46x_c model average inference time : 4.911093711853027ms\n",
      "Benchmarking Inference tf_mobilenetv3_small_075 \n",
      "tf_mobilenetv3_small_075 model average inference time : 5.292332172393799ms\n",
      "Benchmarking Inference dla46_c \n",
      "dla46_c model average inference time : 5.090696811676025ms\n",
      "Benchmarking Inference tf_mobilenetv3_small_minimal_100 \n",
      "tf_mobilenetv3_small_minimal_100 model average inference time : 3.612961769104004ms\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'beit_large_patch16_224': {'fp32': 21.222758293151855, 'top1': 90.157},\n",
       " 'swin_large_patch4_window7_224': {'fp32': 17.0994234085083, 'top1': 89.796},\n",
       " 'xcit_large_24_p8_224_dist': {'fp32': 66.53245449066162, 'top1': 89.519},\n",
       " 'beit_base_patch16_224': {'fp32': 7.464489936828613, 'top1': 89.438},\n",
       " 'vit_large_patch16_224': {'fp32': 20.04459857940674, 'top1': 89.308},\n",
       " 'xcit_medium_24_p8_224_dist': {'fp32': 40.78274726867676, 'top1': 89.286},\n",
       " 'xcit_small_24_p8_224_dist': {'fp32': 29.373767375946045, 'top1': 89.207},\n",
       " 'swin_base_patch4_window7_224': {'fp32': 12.806141376495361, 'top1': 89.19},\n",
       " 'ig_resnext101_32x32d': {'fp32': 132.56885290145874, 'top1': 89.109},\n",
       " 'ig_resnext101_32x48d': {'fp32': 210.10546445846558, 'top1': 89.107},\n",
       " 'xcit_large_24_p16_224_dist': {'fp32': 19.736363887786865, 'top1': 89.045},\n",
       " 'resmlp_big_24_224_in22ft1k': {'fp32': 33.9638614654541, 'top1': 89.019},\n",
       " 'xcit_small_12_p8_224_dist': {'fp32': 16.07722043991089, 'top1': 89.007},\n",
       " 'vit_base_patch16_224': {'fp32': 7.092115879058838, 'top1': 88.857},\n",
       " 'xcit_medium_24_p16_224_dist': {'fp32': 19.059040546417236, 'top1': 88.806},\n",
       " 'ig_resnext101_32x16d': {'fp32': 35.41270971298218, 'top1': 88.804},\n",
       " 'swsl_resnext101_32x8d': {'fp32': 19.28312063217163, 'top1': 88.757},\n",
       " 'vit_base_patch16_224_miil': {'fp32': 6.86307430267334, 'top1': 88.748},\n",
       " 'pit_b_distilled_224': {'fp32': 8.121466636657715, 'top1': 88.678},\n",
       " 'xcit_small_24_p16_224_dist': {'fp32': 18.69269609451294, 'top1': 88.535},\n",
       " 'cait_s24_224': {'fp32': 14.812660217285156, 'top1': 88.451},\n",
       " 'resmlp_big_24_distilled_224': {'fp32': 33.96995306015015, 'top1': 88.447},\n",
       " 'vit_large_r50_s32_224': {'fp32': 16.82636022567749, 'top1': 88.424},\n",
       " 'xcit_small_12_p16_224_dist': {'fp32': 10.687386989593506, 'top1': 88.251},\n",
       " 'deit_base_distilled_patch16_224': {'fp32': 7.080423831939697,\n",
       "  'top1': 88.217},\n",
       " 'ig_resnext101_32x8d': {'fp32': 19.176130294799805, 'top1': 88.161},\n",
       " 'xcit_large_24_p8_224': {'fp32': 68.2600736618042, 'top1': 88.161},\n",
       " 'swsl_resnext101_32x4d': {'fp32': 12.613909244537354, 'top1': 88.086},\n",
       " 'xcit_tiny_24_p8_224_dist': {'fp32': 18.30082416534424, 'top1': 88.048},\n",
       " 'xcit_small_24_p8_224': {'fp32': 29.7322678565979, 'top1': 87.975},\n",
       " 'twins_svt_large': {'fp32': 14.635822772979736, 'top1': 87.907},\n",
       " 'twins_pcpvt_large': {'fp32': 24.414920806884766, 'top1': 87.879},\n",
       " 'xcit_small_12_p8_224': {'fp32': 16.07599973678589, 'top1': 87.832},\n",
       " 'resnetv2_50x1_bit_distilled': {'fp32': 8.04631233215332, 'top1': 87.802},\n",
       " 'twins_pcpvt_base': {'fp32': 16.830954551696777, 'top1': 87.728},\n",
       " 'swin_small_patch4_window7_224': {'fp32': 13.139035701751709, 'top1': 87.678},\n",
       " 'twins_svt_base': {'fp32': 13.144330978393555, 'top1': 87.629},\n",
       " 'xcit_medium_24_p8_224': {'fp32': 41.48906946182251, 'top1': 87.619},\n",
       " 'jx_nest_base': {'fp32': 13.87157917022705, 'top1': 87.602},\n",
       " 'swsl_resnext101_32x16d': {'fp32': 35.50509452819824, 'top1': 87.595},\n",
       " 'swsl_resnext50_32x4d': {'fp32': 6.88831090927124, 'top1': 87.593},\n",
       " 'levit_384': {'fp32': 7.732181549072266, 'top1': 87.559},\n",
       " 'jx_nest_small': {'fp32': 10.803375244140625, 'top1': 87.495},\n",
       " 'resnetv2_152x2_bit_teacher': {'fp32': 46.50394678115845, 'top1': 87.491},\n",
       " 'xcit_tiny_24_p8_224': {'fp32': 18.08338165283203, 'top1': 87.375},\n",
       " 'coat_lite_small': {'fp32': 13.0839204788208, 'top1': 87.365},\n",
       " 'resnetv2_101': {'fp32': 10.269548892974854, 'top1': 87.296},\n",
       " 'ecaresnet101d': {'fp32': 13.389935493469238, 'top1': 87.281},\n",
       " 'pit_s_distilled_224': {'fp32': 5.669441223144531, 'top1': 87.264},\n",
       " 'mixer_b16_224_miil': {'fp32': 5.0153279304504395, 'top1': 87.23},\n",
       " 'xcit_tiny_12_p8_224_dist': {'fp32': 10.577220916748047, 'top1': 87.224},\n",
       " 'convit_base': {'fp32': 10.248382091522217, 'top1': 87.205},\n",
       " 'visformer_small': {'fp32': 12.276256084442139, 'top1': 87.185},\n",
       " 'xcit_small_24_p16_224': {'fp32': 18.592159748077393, 'top1': 87.136},\n",
       " 'convit_small': {'fp32': 7.061395645141602, 'top1': 87.044},\n",
       " 'jx_nest_tiny': {'fp32': 7.174441814422607, 'top1': 87.014},\n",
       " 'xcit_small_12_p16_224': {'fp32': 10.355837345123291, 'top1': 87.0},\n",
       " 'deit_small_distilled_patch16_224': {'fp32': 5.395364761352539,\n",
       "  'top1': 86.993},\n",
       " 'resmlp_36_distilled_224': {'fp32': 8.569638729095459, 'top1': 86.987},\n",
       " 'xcit_large_24_p16_224': {'fp32': 19.858109951019287, 'top1': 86.961},\n",
       " 'xcit_medium_24_p16_224': {'fp32': 18.936972618103027, 'top1': 86.94},\n",
       " 'tnt_s_patch16_224': {'fp32': 11.9808030128479, 'top1': 86.901},\n",
       " 'rexnet_200': {'fp32': 8.933818340301514, 'top1': 86.854},\n",
       " 'vit_small_patch16_224': {'fp32': 5.296797752380371, 'top1': 86.852},\n",
       " 'ssl_resnext101_32x16d': {'fp32': 35.33586263656616, 'top1': 86.85},\n",
       " 'vit_small_r26_s32_224': {'fp32': 9.251573085784912, 'top1': 86.84},\n",
       " 'deit_base_patch16_224': {'fp32': 7.058782577514648, 'top1': 86.827},\n",
       " 'coat_mini': {'fp32': 22.302887439727783, 'top1': 86.81},\n",
       " 'swsl_resnet50': {'fp32': 5.740089416503906, 'top1': 86.801},\n",
       " 'ssl_resnext101_32x8d': {'fp32': 19.183614253997803, 'top1': 86.797},\n",
       " 'twins_svt_small': {'fp32': 10.310540199279785, 'top1': 86.754},\n",
       " 'levit_256': {'fp32': 7.933638095855713, 'top1': 86.733},\n",
       " 'seresnext50_32x4d': {'fp32': 10.537798404693604, 'top1': 86.705},\n",
       " 'pit_b_224': {'fp32': 8.083999156951904, 'top1': 86.69},\n",
       " 'swin_tiny_patch4_window7_224': {'fp32': 7.519328594207764, 'top1': 86.671},\n",
       " 'wide_resnet50_2': {'fp32': 11.235213279724121, 'top1': 86.647},\n",
       " 'twins_pcpvt_small': {'fp32': 10.131669044494629, 'top1': 86.609},\n",
       " 'resmlp_24_distilled_224': {'fp32': 6.0874199867248535, 'top1': 86.607},\n",
       " 'resnest50d_4s2x40d': {'fp32': 22.870047092437744, 'top1': 86.581},\n",
       " 'repvgg_b3': {'fp32': 19.00216579437256, 'top1': 86.57},\n",
       " 'xcit_tiny_24_p16_224_dist': {'fp32': 19.405126571655273, 'top1': 86.543},\n",
       " 'ecaresnet50d': {'fp32': 7.205207347869873, 'top1': 86.479},\n",
       " 'ssl_resnext101_32x4d': {'fp32': 12.962768077850342, 'top1': 86.474},\n",
       " 'gluon_resnet152_v1s': {'fp32': 14.70705509185791, 'top1': 86.453},\n",
       " 'haloregnetz_b': {'fp32': 13.94322156906128, 'top1': 86.449},\n",
       " 'resnest50d_1s4x24d': {'fp32': 14.099588394165039, 'top1': 86.44},\n",
       " 'repvgg_b3g4': {'fp32': 21.578655242919922, 'top1': 86.359},\n",
       " 'legacy_senet154': {'fp32': 35.050692558288574, 'top1': 86.334},\n",
       " 'gernet_m': {'fp32': 4.982576370239258, 'top1': 86.325},\n",
       " 'cait_xxs36_224': {'fp32': 20.291597843170166, 'top1': 86.321},\n",
       " 'pit_s_224': {'fp32': 5.699214935302734, 'top1': 86.319},\n",
       " 'gluon_senet154': {'fp32': 35.667269229888916, 'top1': 86.274},\n",
       " 'resnest50d': {'fp32': 13.990790843963623, 'top1': 86.231},\n",
       " 'ecaresnet101d_pruned': {'fp32': 11.902644634246826, 'top1': 86.182},\n",
       " 'rexnet_150': {'fp32': 8.516361713409424, 'top1': 86.165},\n",
       " 'xcit_tiny_12_p8_224': {'fp32': 10.543198585510254, 'top1': 86.118},\n",
       " 'ssl_resnext50_32x4d': {'fp32': 6.872310638427734, 'top1': 86.075},\n",
       " 'ecaresnetlight': {'fp32': 7.006616592407227, 'top1': 86.045},\n",
       " 'gluon_resnet101_v1s': {'fp32': 10.721702575683594, 'top1': 86.037},\n",
       " 'resnetv2_50': {'fp32': 5.681214332580566, 'top1': 86.02},\n",
       " 'gluon_seresnext101_32x4d': {'fp32': 20.829873085021973, 'top1': 86.007},\n",
       " 'resnet50d': {'fp32': 6.009654998779297, 'top1': 85.994},\n",
       " 'vit_base_patch32_224': {'fp32': 5.190515518188477, 'top1': 85.96},\n",
       " 'gluon_seresnext101_64x4d': {'fp32': 23.46662998199463, 'top1': 85.951},\n",
       " 'gluon_resnet152_v1d': {'fp32': 14.824655055999756, 'top1': 85.906},\n",
       " 'vit_base_patch16_sam_224': {'fp32': 7.050213813781738, 'top1': 85.879},\n",
       " 'repvgg_b2g4': {'fp32': 19.92868661880493, 'top1': 85.857},\n",
       " 'seresnet50': {'fp32': 9.327127933502197, 'top1': 85.83},\n",
       " 'gluon_resnet101_v1d': {'fp32': 10.482888221740723, 'top1': 85.827},\n",
       " 'mixnet_xl': {'fp32': 15.067610740661621, 'top1': 85.785},\n",
       " 'cspresnext50': {'fp32': 6.738874912261963, 'top1': 85.763},\n",
       " 'gluon_resnext101_32x4d': {'fp32': 12.563707828521729, 'top1': 85.744},\n",
       " 'ese_vovnet39b': {'fp32': 6.843414306640625, 'top1': 85.742},\n",
       " 'legacy_seresnext101_32x4d': {'fp32': 20.458691120147705, 'top1': 85.738},\n",
       " 'xcit_tiny_24_p16_224': {'fp32': 18.694677352905273, 'top1': 85.736},\n",
       " 'regnety_320': {'fp32': 36.93737506866455, 'top1': 85.723},\n",
       " 'resnet50': {'fp32': 5.8392333984375, 'top1': 85.704},\n",
       " 'gluon_resnext101_64x4d': {'fp32': 19.55361843109131, 'top1': 85.704},\n",
       " 'resmlp_big_24_224': {'fp32': 34.003565311431885, 'top1': 85.697},\n",
       " 'deit_small_patch16_224': {'fp32': 5.30376672744751, 'top1': 85.663},\n",
       " 'dpn107': {'fp32': 25.07070302963257, 'top1': 85.65},\n",
       " 'pit_xs_distilled_224': {'fp32': 5.718038082122803, 'top1': 85.644},\n",
       " 'resmlp_36_224': {'fp32': 8.562207221984863, 'top1': 85.623},\n",
       " 'levit_192': {'fp32': 7.820937633514404, 'top1': 85.586},\n",
       " 'gluon_resnet152_v1c': {'fp32': 14.790723323822021, 'top1': 85.576},\n",
       " 'ecaresnet50d_pruned': {'fp32': 6.682915687561035, 'top1': 85.573},\n",
       " 'resnext50d_32x4d': {'fp32': 7.0255208015441895, 'top1': 85.561},\n",
       " 'regnety_120': {'fp32': 23.444221019744873, 'top1': 85.541},\n",
       " 'regnetx_320': {'fp32': 36.329240798950195, 'top1': 85.516},\n",
       " 'dpn92': {'fp32': 12.439408302307129, 'top1': 85.484},\n",
       " 'rexnet_130': {'fp32': 8.482873439788818, 'top1': 85.465},\n",
       " 'gluon_resnet152_v1b': {'fp32': 14.631612300872803, 'top1': 85.458},\n",
       " 'resnetrs50': {'fp32': 9.213309288024902, 'top1': 85.433},\n",
       " 'dpn131': {'fp32': 22.95206308364868, 'top1': 85.411},\n",
       " 'dla102x2': {'fp32': 15.781023502349854, 'top1': 85.383},\n",
       " 'regnetx_160': {'fp32': 26.581060886383057, 'top1': 85.364},\n",
       " 'gmlp_s16_224': {'fp32': 7.8601789474487305, 'top1': 85.349},\n",
       " 'gluon_seresnext50_32x4d': {'fp32': 10.296087265014648, 'top1': 85.343},\n",
       " 'skresnext50_32x4d': {'fp32': 14.00026798248291, 'top1': 85.315},\n",
       " 'gluon_resnet101_v1c': {'fp32': 10.544290542602539, 'top1': 85.311},\n",
       " 'dpn98': {'fp32': 16.838343143463135, 'top1': 85.309},\n",
       " 'regnety_064': {'fp32': 15.869503021240234, 'top1': 85.287},\n",
       " 'dpn68b': {'fp32': 10.606613159179688, 'top1': 85.285},\n",
       " 'resnetblur50': {'fp32': 5.8940863609313965, 'top1': 85.277},\n",
       " 'resmlp_24_224': {'fp32': 6.166572570800781, 'top1': 85.275},\n",
       " 'coat_lite_mini': {'fp32': 7.513597011566162, 'top1': 85.249},\n",
       " 'resnext50_32x4d': {'fp32': 6.959478855133057, 'top1': 85.232},\n",
       " 'regnety_080': {'fp32': 14.425852298736572, 'top1': 85.232},\n",
       " 'cait_xxs24_224': {'fp32': 14.630074501037598, 'top1': 85.221},\n",
       " 'xcit_tiny_12_p16_224_dist': {'fp32': 10.864458084106445, 'top1': 85.206},\n",
       " 'resnext101_32x8d': {'fp32': 19.254169464111328, 'top1': 85.187},\n",
       " 'hrnet_w48': {'fp32': 34.230711460113525, 'top1': 85.155},\n",
       " 'regnetx_120': {'fp32': 21.44188404083252, 'top1': 85.138},\n",
       " 'gluon_resnet101_v1b': {'fp32': 9.984734058380127, 'top1': 85.129},\n",
       " 'hrnet_w64': {'fp32': 33.04595232009888, 'top1': 85.117},\n",
       " 'ssl_resnet50': {'fp32': 5.827453136444092, 'top1': 85.104},\n",
       " 'res2net101_26w_4s': {'fp32': 17.982370853424072, 'top1': 85.089},\n",
       " 'gluon_resnext50_32x4d': {'fp32': 6.866660118103027, 'top1': 85.01},\n",
       " 'tf_efficientnet_b0_ns': {'fp32': 7.480754852294922, 'top1': 85.003},\n",
       " 'resnest26d': {'fp32': 9.21252727508545, 'top1': 84.997},\n",
       " 'coat_tiny': {'fp32': 22.003560066223145, 'top1': 84.969},\n",
       " 'regnety_040': {'fp32': 12.508735656738281, 'top1': 84.948},\n",
       " 'dla169': {'fp32': 16.491498947143555, 'top1': 84.914},\n",
       " 'legacy_seresnext50_32x4d': {'fp32': 9.978797435760498, 'top1': 84.909},\n",
       " 'hrnet_w44': {'fp32': 34.55578088760376, 'top1': 84.886},\n",
       " 'regnetx_080': {'fp32': 16.284196376800537, 'top1': 84.875},\n",
       " 'gluon_resnet50_v1s': {'fp32': 6.026053428649902, 'top1': 84.852},\n",
       " 'res2net50_26w_8s': {'fp32': 16.22917652130127, 'top1': 84.843},\n",
       " 'dla60_res2next': {'fp32': 16.423168182373047, 'top1': 84.828},\n",
       " 'mixnet_l': {'fp32': 11.95857048034668, 'top1': 84.826},\n",
       " 'levit_128': {'fp32': 7.7445220947265625, 'top1': 84.822},\n",
       " 'dla60_res2net': {'fp32': 10.492653846740723, 'top1': 84.82},\n",
       " 'tv_resnet152': {'fp32': 14.771018028259277, 'top1': 84.818},\n",
       " 'dla102x': {'fp32': 12.74256944656372, 'top1': 84.813},\n",
       " 'gluon_resnet50_v1d': {'fp32': 5.942518711090088, 'top1': 84.811},\n",
       " 'regnetx_064': {'fp32': 10.907056331634521, 'top1': 84.783},\n",
       " 'pit_xs_224': {'fp32': 5.417320728302002, 'top1': 84.783},\n",
       " 'hrnet_w40': {'fp32': 31.760694980621338, 'top1': 84.745},\n",
       " 'repvgg_b2': {'fp32': 13.380820751190186, 'top1': 84.726},\n",
       " 'res2net50_26w_6s': {'fp32': 13.04659128189087, 'top1': 84.724},\n",
       " 'resmlp_12_distilled_224': {'fp32': 3.3237171173095703, 'top1': 84.707},\n",
       " 'legacy_seresnet152': {'fp32': 26.403536796569824, 'top1': 84.696},\n",
       " 'selecsls60b': {'fp32': 6.648528575897217, 'top1': 84.655},\n",
       " 'hrnet_w32': {'fp32': 31.174631118774414, 'top1': 84.653},\n",
       " 'tf_efficientnetv2_b0': {'fp32': 9.105782508850098, 'top1': 84.625},\n",
       " 'regnetx_040': {'fp32': 9.47070598602295, 'top1': 84.6},\n",
       " 'hrnet_w30': {'fp32': 32.16704845428467, 'top1': 84.591},\n",
       " 'efficientnet_es': {'fp32': 4.538414478302002, 'top1': 84.591},\n",
       " 'tf_mixnet_l': {'fp32': 12.606806755065918, 'top1': 84.559},\n",
       " 'wide_resnet101_2': {'fp32': 18.493614196777344, 'top1': 84.555},\n",
       " 'dla60x': {'fp32': 7.526495456695557, 'top1': 84.529},\n",
       " 'legacy_seresnet101': {'fp32': 17.574009895324707, 'top1': 84.502},\n",
       " 'coat_lite_tiny': {'fp32': 7.392082214355469, 'top1': 84.457},\n",
       " 'repvgg_b1': {'fp32': 10.306687355041504, 'top1': 84.399},\n",
       " 'res2net50_26w_4s': {'fp32': 9.69325304031372, 'top1': 84.371},\n",
       " 'hardcorenas_f': {'fp32': 7.530560493469238, 'top1': 84.322},\n",
       " 'res2net50_14w_8s': {'fp32': 15.195362567901611, 'top1': 84.307},\n",
       " 'selecsls60': {'fp32': 6.841988563537598, 'top1': 84.282},\n",
       " 'res2next50': {'fp32': 16.007883548736572, 'top1': 84.23},\n",
       " 'regnetx_032': {'fp32': 9.208292961120605, 'top1': 84.23},\n",
       " 'gluon_resnet50_v1c': {'fp32': 6.052649021148682, 'top1': 84.209},\n",
       " 'dla102': {'fp32': 10.941462516784668, 'top1': 84.192},\n",
       " 'rexnet_100': {'fp32': 8.449954986572266, 'top1': 84.173},\n",
       " 'res2net50_48w_2s': {'fp32': 6.536848545074463, 'top1': 84.121},\n",
       " 'resnet34d': {'fp32': 4.470558166503906, 'top1': 84.104},\n",
       " 'xcit_tiny_12_p16_224': {'fp32': 10.63784122467041, 'top1': 84.083},\n",
       " 'efficientnet_b0': {'fp32': 7.568955421447754, 'top1': 84.034},\n",
       " 'hardcorenas_e': {'fp32': 7.396128177642822, 'top1': 83.972},\n",
       " 'gmixer_24_224': {'fp32': 8.387062549591064, 'top1': 83.97},\n",
       " 'tf_efficientnet_cc_b0_8e': {'fp32': 9.599294662475586, 'top1': 83.97},\n",
       " 'regnety_016': {'fp32': 13.486073017120361, 'top1': 83.955},\n",
       " 'tv_resnext50_32x4d': {'fp32': 6.946098804473877, 'top1': 83.955},\n",
       " 'gluon_resnet50_v1b': {'fp32': 5.7448625564575195, 'top1': 83.942},\n",
       " 'densenet161': {'fp32': 18.091626167297363, 'top1': 83.9},\n",
       " 'seresnext26t_32x4d': {'fp32': 5.766298770904541, 'top1': 83.887},\n",
       " 'mobilenetv2_120d': {'fp32': 6.469125747680664, 'top1': 83.887},\n",
       " 'tv_resnet101': {'fp32': 10.18514633178711, 'top1': 83.855},\n",
       " 'hardcorenas_d': {'fp32': 7.6528215408325195, 'top1': 83.763},\n",
       " 'dla60': {'fp32': 6.767768859863281, 'top1': 83.735},\n",
       " 'xcit_nano_12_p8_224_dist': {'fp32': 10.852313041687012, 'top1': 83.733},\n",
       " 'seresnext26d_32x4d': {'fp32': 5.781450271606445, 'top1': 83.724},\n",
       " 'repvgg_b1g4': {'fp32': 15.741524696350098, 'top1': 83.695},\n",
       " 'legacy_seresnet50': {'fp32': 9.04362678527832, 'top1': 83.671},\n",
       " 'tf_efficientnet_b0_ap': {'fp32': 7.821083068847656, 'top1': 83.66},\n",
       " 'skresnet34': {'fp32': 11.947510242462158, 'top1': 83.65},\n",
       " 'tf_efficientnet_cc_b0_4e': {'fp32': 9.393877983093262, 'top1': 83.639},\n",
       " 'resmlp_12_224': {'fp32': 3.5558176040649414, 'top1': 83.573},\n",
       " 'mobilenetv3_large_100_miil': {'fp32': 5.717058181762695, 'top1': 83.549},\n",
       " 'densenet201': {'fp32': 22.792253494262695, 'top1': 83.547},\n",
       " 'gernet_s': {'fp32': 4.975404739379883, 'top1': 83.522},\n",
       " 'legacy_seresnext26_32x4d': {'fp32': 5.526425838470459, 'top1': 83.515},\n",
       " 'mixnet_m': {'fp32': 12.208683490753174, 'top1': 83.513},\n",
       " 'tf_efficientnet_b0': {'fp32': 7.39795446395874, 'top1': 83.513},\n",
       " 'hrnet_w18': {'fp32': 30.66802978515625, 'top1': 83.5},\n",
       " 'densenetblur121d': {'fp32': 13.869426250457764, 'top1': 83.475},\n",
       " 'selecsls42b': {'fp32': 5.463900566101074, 'top1': 83.46},\n",
       " 'hardcorenas_c': {'fp32': 5.89510440826416, 'top1': 83.336},\n",
       " 'regnetx_016': {'fp32': 7.057490348815918, 'top1': 83.186},\n",
       " 'mobilenetv2_140': {'fp32': 4.270687103271484, 'top1': 83.176},\n",
       " 'tf_mixnet_m': {'fp32': 12.038166522979736, 'top1': 83.174},\n",
       " 'dpn68': {'fp32': 9.72515344619751, 'top1': 83.171},\n",
       " 'tf_efficientnet_es': {'fp32': 4.743032455444336, 'top1': 83.167},\n",
       " 'ese_vovnet19b_dw': {'fp32': 4.022977352142334, 'top1': 83.114},\n",
       " 'levit_128s': {'fp32': 6.251623630523682, 'top1': 83.065},\n",
       " 'resnet26d': {'fp32': 3.824455738067627, 'top1': 83.043},\n",
       " 'repvgg_a2': {'fp32': 5.800914764404297, 'top1': 82.999},\n",
       " 'tv_resnet50': {'fp32': 5.629220008850098, 'top1': 82.954},\n",
       " 'hardcorenas_b': {'fp32': 5.557959079742432, 'top1': 82.877},\n",
       " 'densenet121': {'fp32': 13.61299991607666, 'top1': 82.815},\n",
       " 'densenet169': {'fp32': 18.831112384796143, 'top1': 82.661},\n",
       " 'mixnet_s': {'fp32': 9.757490158081055, 'top1': 82.525},\n",
       " 'vit_small_patch32_224': {'fp32': 5.096225738525391, 'top1': 82.51},\n",
       " 'regnety_008': {'fp32': 7.901732921600342, 'top1': 82.478},\n",
       " 'efficientnet_lite0': {'fp32': 4.191188812255859, 'top1': 82.388},\n",
       " 'resnest14d': {'fp32': 6.851811408996582, 'top1': 82.362},\n",
       " 'hardcorenas_a': {'fp32': 4.8409199714660645, 'top1': 82.305},\n",
       " 'efficientnet_es_pruned': {'fp32': 4.648795127868652, 'top1': 82.279},\n",
       " 'mobilenetv3_rw': {'fp32': 5.667610168457031, 'top1': 82.268},\n",
       " 'semnasnet_100': {'fp32': 5.82747220993042, 'top1': 82.253},\n",
       " 'mobilenetv3_large_100': {'fp32': 5.91533899307251, 'top1': 82.177},\n",
       " 'resnet34': {'fp32': 4.090702533721924, 'top1': 82.125},\n",
       " 'mobilenetv2_110d': {'fp32': 5.3720855712890625, 'top1': 82.076},\n",
       " 'vit_tiny_patch16_224': {'fp32': 5.227022171020508, 'top1': 82.044},\n",
       " 'tf_mixnet_s': {'fp32': 10.010137557983398, 'top1': 82.04},\n",
       " 'repvgg_b0': {'fp32': 6.675727367401123, 'top1': 82.006},\n",
       " 'deit_tiny_distilled_patch16_224': {'fp32': 5.460071563720703,\n",
       "  'top1': 82.001},\n",
       " 'mixer_b16_224': {'fp32': 5.086991786956787, 'top1': 81.997},\n",
       " 'pit_ti_distilled_224': {'fp32': 5.3986382484436035, 'top1': 81.972},\n",
       " 'hrnet_w18_small_v2': {'fp32': 16.57296895980835, 'top1': 81.959},\n",
       " 'tf_efficientnet_lite0': {'fp32': 4.450747966766357, 'top1': 81.95},\n",
       " 'resnet26': {'fp32': 3.575725555419922, 'top1': 81.942},\n",
       " 'tf_mobilenetv3_large_100': {'fp32': 5.966911315917969, 'top1': 81.841},\n",
       " 'tv_densenet121': {'fp32': 13.520746231079102, 'top1': 81.728},\n",
       " 'regnety_006': {'fp32': 8.412299156188965, 'top1': 81.703},\n",
       " 'dla34': {'fp32': 4.329702854156494, 'top1': 81.643},\n",
       " 'xcit_nano_12_p8_224': {'fp32': 10.801050662994385, 'top1': 81.641},\n",
       " 'fbnetc_100': {'fp32': 5.238170623779297, 'top1': 81.555},\n",
       " 'legacy_seresnet34': {'fp32': 7.046689987182617, 'top1': 81.532},\n",
       " 'regnetx_008': {'fp32': 5.828697681427002, 'top1': 81.508},\n",
       " 'gluon_resnet34_v1b': {'fp32': 4.112353324890137, 'top1': 81.489},\n",
       " 'mnasnet_100': {'fp32': 4.512279033660889, 'top1': 81.472},\n",
       " 'vgg19_bn': {'fp32': 12.25665807723999, 'top1': 81.451},\n",
       " 'convit_tiny': {'fp32': 6.963202953338623, 'top1': 81.115},\n",
       " 'spnasnet_100': {'fp32': 5.339231491088867, 'top1': 80.866},\n",
       " 'ghostnet_100': {'fp32': 8.784103393554688, 'top1': 80.699},\n",
       " 'regnety_004': {'fp32': 9.942929744720459, 'top1': 80.648},\n",
       " 'skresnet18': {'fp32': 6.443450450897217, 'top1': 80.624},\n",
       " 'regnetx_006': {'fp32': 5.590357780456543, 'top1': 80.616},\n",
       " 'pit_ti_224': {'fp32': 5.664281845092773, 'top1': 80.597},\n",
       " 'swsl_resnet18': {'fp32': 2.8646159172058105, 'top1': 80.562},\n",
       " 'vgg16_bn': {'fp32': 10.735476016998291, 'top1': 80.535},\n",
       " 'resnet18d': {'fp32': 3.0302071571350098, 'top1': 80.387},\n",
       " 'tv_resnet34': {'fp32': 4.167530536651611, 'top1': 80.377},\n",
       " 'mobilenetv2_100': {'fp32': 4.245607852935791, 'top1': 80.257},\n",
       " 'xcit_nano_12_p16_224_dist': {'fp32': 10.799438953399658, 'top1': 80.225},\n",
       " 'vit_base_patch32_sam_224': {'fp32': 5.403764247894287, 'top1': 80.212},\n",
       " 'ssl_resnet18': {'fp32': 2.6334023475646973, 'top1': 80.114},\n",
       " 'tf_mobilenetv3_large_075': {'fp32': 5.989499092102051, 'top1': 80.088},\n",
       " 'deit_tiny_patch16_224': {'fp32': 5.194070339202881, 'top1': 80.009},\n",
       " 'hrnet_w18_small': {'fp32': 9.595694541931152, 'top1': 79.544},\n",
       " 'vgg19': {'fp32': 11.495828628540039, 'top1': 79.484},\n",
       " 'regnetx_004': {'fp32': 8.82101058959961, 'top1': 79.424},\n",
       " 'tf_mobilenetv3_large_minimal_100': {'fp32': 4.257955551147461,\n",
       "  'top1': 79.232},\n",
       " 'legacy_seresnet18': {'fp32': 4.220445156097412, 'top1': 79.151},\n",
       " 'vgg16': {'fp32': 9.98368501663208, 'top1': 79.031},\n",
       " 'vit_tiny_r_s16_p8_224': {'fp32': 5.392265319824219, 'top1': 78.997},\n",
       " 'vgg13_bn': {'fp32': 9.115087985992432, 'top1': 78.987},\n",
       " 'gluon_resnet18_v1b': {'fp32': 2.794067859649658, 'top1': 78.376},\n",
       " 'vgg11_bn': {'fp32': 7.375471591949463, 'top1': 77.926},\n",
       " 'xcit_nano_12_p16_224': {'fp32': 10.941822528839111, 'top1': 77.909},\n",
       " 'regnety_002': {'fp32': 8.2291841506958, 'top1': 77.417},\n",
       " 'mixer_l16_224': {'fp32': 15.571436882019043, 'top1': 77.294},\n",
       " 'resnet18': {'fp32': 2.7054572105407715, 'top1': 77.276},\n",
       " 'vgg13': {'fp32': 8.459031581878662, 'top1': 77.234},\n",
       " 'vgg11': {'fp32': 7.024950981140137, 'top1': 76.397},\n",
       " 'regnetx_002': {'fp32': 5.828824043273926, 'top1': 76.102},\n",
       " 'dla60x_c': {'fp32': 5.9967803955078125, 'top1': 75.656},\n",
       " 'tf_mobilenetv3_small_100': {'fp32': 5.031616687774658, 'top1': 74.736},\n",
       " 'dla46x_c': {'fp32': 4.911093711853027, 'top1': 73.645},\n",
       " 'tf_mobilenetv3_small_075': {'fp32': 5.292332172393799, 'top1': 72.816},\n",
       " 'dla46_c': {'fp32': 5.090696811676025, 'top1': 72.607},\n",
       " 'tf_mobilenetv3_small_minimal_100': {'fp32': 3.612961769104004,\n",
       "  'top1': 70.107}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark = {}\n",
    "\n",
    "# inference float precision\n",
    "for i,modelname in tqdm(enumerate((modellist))):\n",
    "    try:\n",
    "        benchmark = inference(modelname, benchmark)\n",
    "    except:\n",
    "        print(\"pass {}\".format(modelname))\n",
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fp32</th>\n",
       "      <th>top1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beit_large_patch16_224</th>\n",
       "      <td>21.222758</td>\n",
       "      <td>90.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swin_large_patch4_window7_224</th>\n",
       "      <td>17.099423</td>\n",
       "      <td>89.796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xcit_large_24_p8_224_dist</th>\n",
       "      <td>66.532454</td>\n",
       "      <td>89.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beit_base_patch16_224</th>\n",
       "      <td>7.464490</td>\n",
       "      <td>89.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vit_large_patch16_224</th>\n",
       "      <td>20.044599</td>\n",
       "      <td>89.308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tf_mobilenetv3_small_100</th>\n",
       "      <td>5.031617</td>\n",
       "      <td>74.736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dla46x_c</th>\n",
       "      <td>4.911094</td>\n",
       "      <td>73.645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tf_mobilenetv3_small_075</th>\n",
       "      <td>5.292332</td>\n",
       "      <td>72.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dla46_c</th>\n",
       "      <td>5.090697</td>\n",
       "      <td>72.607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tf_mobilenetv3_small_minimal_100</th>\n",
       "      <td>3.612962</td>\n",
       "      <td>70.107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>322 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       fp32    top1\n",
       "beit_large_patch16_224            21.222758  90.157\n",
       "swin_large_patch4_window7_224     17.099423  89.796\n",
       "xcit_large_24_p8_224_dist         66.532454  89.519\n",
       "beit_base_patch16_224              7.464490  89.438\n",
       "vit_large_patch16_224             20.044599  89.308\n",
       "...                                     ...     ...\n",
       "tf_mobilenetv3_small_100           5.031617  74.736\n",
       "dla46x_c                           4.911094  73.645\n",
       "tf_mobilenetv3_small_075           5.292332  72.816\n",
       "dla46_c                            5.090697  72.607\n",
       "tf_mobilenetv3_small_minimal_100   3.612962  70.107\n",
       "\n",
       "[322 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(benchmark).T\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"results_fp32_224.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7efec36ae978>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.lmplot(y='top1', x='fp32',  \n",
    "           data=df_results, logx=True,\n",
    "           fit_reg=False, scatter_kws={'alpha':0.8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For various image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ryujaehun/pytorch-gpu-benchmark/blob/master/benchmark_models.py\n",
    "def inference_imsize(modelname, benchmark, imsize):\n",
    "    with torch.no_grad():\n",
    "        model = timm.create_model(modelname,)\n",
    "        model=model.to('cuda')\n",
    "        model.eval()\n",
    "        precision = \"float\"\n",
    "        durations = []\n",
    "        rand_loader = DataLoader(dataset=RandomDataset(BATCH_SIZE*(WARM_UP + NUM_TEST), imsize),\n",
    "                         batch_size=BATCH_SIZE, shuffle=False,num_workers=8)\n",
    "        print(f'Benchmarking Inference {modelname} ')\n",
    "        for step,img in enumerate(rand_loader):\n",
    "            img=getattr(img,precision)()\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "            model(img.to('cuda'))\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            if step >= WARM_UP:\n",
    "                durations.append((end - start)*1000)\n",
    "        print(f'{modelname} model average inference time : {sum(durations)/len(durations)}ms')\n",
    "        \n",
    "        benchmark[modelname] = {\"fp32\": np.mean(durations), \"top1\": float(df_models[df_models[\"model\"]==modelname][\"top1\"]), \"imsize\": imsize}\n",
    "    return benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d39bd3bec7b48e48e7e59a25ad100a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Inference beit_large_patch16_512 \n",
      "beit_large_patch16_512 model average inference time : 187.82024145126343ms\n",
      "Benchmarking Inference beit_large_patch16_384 \n",
      "beit_large_patch16_384 model average inference time : 78.99953365325928ms\n",
      "Benchmarking Inference tf_efficientnet_l2_ns \n"
     ]
    }
   ],
   "source": [
    "modellist = df_models[\"model\"]\n",
    "benchmark = {}\n",
    "\n",
    "# inference float precision\n",
    "for i,modelname in tqdm(enumerate((modellist))):\n",
    "    imsize = int(df_models[df_models[\"model\"]==modelname][\"img_size\"])\n",
    "    try:\n",
    "        benchmark = inference_imsize(modelname, benchmark, imsize)\n",
    "    except:\n",
    "        print(\"pass {}\".format(modelname))\n",
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(benchmark).T\n",
    "df_results\n",
    "df_results.to_csv(\"results_fp32_imsizeall.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
